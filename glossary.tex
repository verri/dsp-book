\newabbreviation{erm}{ERM}{empirical risk minimization}
\newabbreviation{svm}{SVM}{support vector machine}
\newabbreviation{bnf}{BNF}{Backusâ€“Naur form}
\newabbreviation{sql}{SQL}{structured query language}
\newabbreviation{ibm}{IBM}{International Business Machines Corporation}
\newabbreviation{rdbms}{RDBMS}{relational database management system}
\newabbreviation{etl}{ETL}{extract, transform, load}
\newabbreviation{bi}{BI}{business intelligence}
\newabbreviation{hdfs}{HDFS}{Hadoop distributed file system}
\newabbreviation{lusi}{LUSI}{learning using statistical inference}
\newabbreviation{iot}{IoT}{internet of things}
\newabbreviation{lifo}{LIFO}{last-in-first-out}
\newabbreviation{fifo}{FIFO}{first-in-first-out}
\newabbreviation{pmf}{PMF}{probability mass function}
\newabbreviation{pdf}{PDF}{probability density function}
\newabbreviation{cdf}{CDF}{cumulative distribution function}
\newabbreviation{cicd}{CI/CD}{continuous integration/continuous deployment}
\newabbreviation{slt}{SLT}{statistical learning theory}
\newabbreviation{ai}{AI}{artificial intelligence}
\newabbreviation{ml}{ML}{machine learning}
\newabbreviation{vc}{VC}{Vapnik-Chervonenkis}
\newabbreviation{srm}{SRM}{structural risk minimization}
\newabbreviation{mlp}{MLP}{multilayer perceptron}
\newabbreviation{iqr}{IQR}{interquartile range}
\newabbreviation{cnn}{CNN}{convolutional neural network}
\newabbreviation{pca}{PCA}{principal component analysis}
\newabbreviation{llm}{LLM}{large language model}
\newabbreviation{eda}{EDA}{exploratory data analysis}
\newabbreviation{devops}{DevOps}{development operations}
\newabbreviation{mlops}{MLOps}{machine learning operations}
\newabbreviation{dataops}{DataOps}{data operations}

\newglossaryentry{structureddata}{%
  name=structured data,
  description={Data organized in a tabular format where each cell contains a value
    of a specific data type and all cells in the same column share the same type.}
}

\newglossaryentry{summarystatistics}{%
  name=summary statistics,
  description={A form of statistical analysis that describes data in terms of its
    central tendencies and variability.}
}

\newglossaryentry{transformer}{%
  name=transformer,
  description={A deep learning architecture based on attention mechanisms in contrast to
    recurrent or convolutional neural networks.}
}

\newglossaryentry{splitinvariance}{%
  name=split invariance,
  description={The property of a data handling operation being distributive over the
    bind operation.}
}

\newglossaryentry{inductivelearning}{%
  name=inductive learning,
  description={A learning approach that derives general rules from specific
    observations, enabling predictions on any new instances.}
}

\newglossaryentry{transductivelearning}{%
  name=transductive learning,
  description={A learning approach that obtains specific knowledge from specific
    observations, without deriving general rules.}
}

\newglossaryentry{learningmachine}{%
  name=learning machine,
  description={A system capable of generating a set of models for fixed
    set of parametrizations.}
}

\newglossaryentry{loss}{%
  name=loss,
  description={A function that measures the discrepancy between the true response and
    the prediction of a model. Also called discrepancy.}
}

\newglossaryentry{riskfunction}{%
  name=risk function,
  description={The expected value of the loss function over the joint distribution of
    inputs and outputs.}
}

\newglossaryentry{classifier}{%
  name=classifier,
  description={A model for the binary data classification task, where the output is
    an indicator function.}
}

\newglossaryentry{regressor}{%
  name=regressor,
  description={A model for the regression estimation task, where the output is a
    real-valued function.}
}

\newglossaryentry{bayeserrorrate}{%
  name=Bayes error rate,
  description={The lowest possible loss for any classifier in a given problem. Also
    called the irreducible error.}
}

\newglossaryentry{regressionfunction}{%
  name=regression function,
  description={The optimal solution for the regression estimation task, defined as
    the conditional expectation of the target variable given the input.}
}

\newglossaryentry{empiricalrisk}{%
  name=empirical risk,
  description={An approximation of the risk function computed from a finite training
    set by averaging the loss over all training samples. Also called the empirical
    risk functional.}
}

\newglossaryentry{growthfunction}{%
  name=growth function,
  description={A function characterizing the capacity of a hypothesis space. Its
    behavior determines whether uniform convergence of the empirical risk is
    guaranteed.}
}

\newglossaryentry{biasvariance}{%
  name=bias-variance trade-off,
  description={The relationship between bias error, from failure to capture relevant
    relationships, and variance error, from erroneously modeling noise in the
    training data.}
}

\newglossaryentry{regularization}{%
  name=regularization,
  description={A technique that modifies the loss function by adding a penalty term
    depending on model complexity, encouraging robust pattern learning over
    memorization.}
}

\newglossaryentry{admissiblestructure}{%
  name=admissible structure,
  description={A nested sequence of subsets of the hypothesis space with
    non-decreasing finite VC dimensions, used in the structural risk minimization
    principle.}
}

\newglossaryentry{supportvector}{%
  name=support vector,
  description={A training vector that lies exactly on the margin of a maximal margin
    classifier and determines the separating hyperplane.}
}

\newglossaryentry{datapreprocessing}{%
  name=data preprocessing,
  description={The process of adjusting the data to make it suitable for a particular
    learning machine.}
}

\newglossaryentry{confusionmatrix}{%
  name=confusion matrix,
  description={A table summarizing the predictions of a classifier against the true
    classes, serving as the basis for classification performance metrics.}
}

\newglossaryentry{crossvalidation}{%
  name=cross-validation,
  description={A sampling strategy that divides the dataset into $r$ folds, using
    each fold once as a test set and the remaining folds as the training set.}
}

\newglossaryentry{hyperparameter}{%
  name=hyperparameter,
  description={A parameter of a learning algorithm that is not learned from the data
    but is set by the user.}
}

\newglossaryentry{experimentalplanning}{%
  name=experimental planning,
  description={The design and organization of experiments to gather performance data
    systematically in order to reach specific goals or test hypotheses.}
}

\newglossaryentry{ontology}{%
  name=ontology,
  description={%
    Ontology is the study of being, existence, and reality. In computer science and
    information science, an ontology is a formal naming and definition of the types,
    properties, and interrelationships of the entities that really or fundamentally exist
    for a particular domain.}
}

\newglossaryentry{leakage}{%
  name=data leakage,
  description={%
    A situation where information from the test set is used to transform the training
    set in any way or to train the model.}
}

\newglossaryentry{model}{%
  name=model,
  description={%
    A general function that can be used to estimate the relationship between the
    input and output variables in a dataset.}
}

\newglossaryentry{preprocessor}{%
  name=preprocessor,
  description={%
    A chain of data handling operations that transforms the input data into a format that
    is suitable for the model.}
}

\makeglossaries

% vim: set spell spelllang=en:
