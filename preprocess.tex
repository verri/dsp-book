\chapter{Data preprocessing}
\label{chap:preprocess}
\glsresetall

\chapterprecishere{I find your lack of faith disturbing.
  \par\raggedleft--- \textup{Darth Vader}, Star Wars: Episode IV -- A New Hope (1977)}

\begin{mainbox}{Chapter remarks}

  \boxsubtitle{Contents}

  \startcontents[chapters]
  \printcontents[chapters]{}{1}{}
  \vspace{1em}

  \boxsubtitle{Context}

  \begin{itemize}
    \itemsep0em
    \item \dots
  \end{itemize}

  \boxsubtitle{Objectives}

  \begin{itemize}
    \itemsep0em
    \item \dots
  \end{itemize}

  \boxsubtitle{Takeways}

  \begin{itemize}
    \itemsep0em
    \item \dots
  \end{itemize}
\end{mainbox}

{}
\clearpage

\section{Data handling pipeline}

Before we study the data handling tasks, we need to understand that a data handling
pipeline is a sequence of operations that \emph{does} depend on the input data.
This might seem obvious, but the implications are not.

A common error in data handling is to perform operations ad hoc, usually leading to
\gls{leakage}.  For instance, one might impute missing values before splitting the data into
training and testing sets.  This is a mistake because the imputation is based on the
entire dataset, including the testing set.

To avoid this kind of error, one must declare\footnote{This is the declarative nature of
data handling operations.} the operations that will be performed on the data before
applying them.  This is usually done by creating the full data handling pipeline
beforehand.

The pipeline, like a model, must be ``fitted'' to the data.  This means that parameters of
the operations are not fixed until the first data is given as input.  Subsequent data fed
to the pipeline will be handled keeping the first ``learned'' parameters.

The fitted pipeline is called a \gls{preprocessor} in our context.  This is because the
pipeline is used to preprocess the data before the model is applied.

Consider the following example.  Suppose we have a dataset with missing values for
variable A.  We want to impute the missing values and then standardize
A.  The pipeline is created as follows:  \code{D -> impute\_zero(A) ->
standardize(A)}.

The operation \code{impute\_zero(A)} is parametrized by the value 0, which, in this case,
is fixed.  However, the operation \code{standardize(A)} is parametrized by the mean and
the standard deviation of the values in A.  These values are not fixed until the
first data is given as input.

\begin{hlbox}{A note about fixed parameters}
  Even if your data handling pipeline contains operations that have fixed parameters and
  can be safely applied to data before the model search, \emph{I strongly recommend} that
  you declare the pipeline as a whole.  This is because it is easier to maintain and
  reproduce the data handling process, especially in deployment.  Performing ad hoc
  handling in your data is a source of errors and important transformations can be
  forgotten when receiving new data.
\end{hlbox}

In a practical scenario, the source code of the \emph{model search} method includes not
only strategies for the model, but also the data handling pipeline.  Moreover, the deployment
of the model includes the data handling pipeline as well.  In other words, it does not
matter which model is used, in the example above, the mean and the standard deviation of
the values in A should be stored and used in deployed models.

In terms of reproducibility and validation, having a single consolidated pipeline is
crucial.

\begin{hlbox}{A note about ``filtering'' operations}
  Some operations may conditionally remove rows from the dataset.  For instance, after
  observing that there exists few missing values in an important column, one might decide
  to remove rows with missing values in it.  In production, this means that some new
  observations might be discarded before reaching the model itself.  However, the user
  still expects an answer from the model.  In this case, one must define either a default
  value for the answer or a default behavior to handle discarded examples.
\end{hlbox}

% \begin{slidebox}{Data handling pipeline}{}
%   \begin{itemize}
%     \item A data handling pipeline is a sequence of operations that depend on the input data;
%     \item The pipeline must be declared before applying the operations;
%     \item The pipeline is fitted to the data;
%     \item The selected pipeline is part of the model search and deployment.
%     \item Even operations that have fixed parameters or that can be safely applied to data
%       before the model search should be declared in the pipeline.
%   \end{itemize}
% \end{slidebox}

Just to be clear, before reaching the pipeline, data is already tidy.  Thus, simple
integration (not enhancement), pivoting and aggregating are kept outside the pipeline.
These operations must depend only on variable names and not variable values.

\section{Data transformation}

One important task in data handling is data transformation.  This is the process of adjusting
the format and the types of the data to make it suitable for analysis.

Before data transformation we must make sure that data is tidy, i.e., to have
each variable in a column and each observation in a row.  Remember that, depending on the
problem definition, we target a particular observational unit.  Having a clear picture of
the observational unit is important to define the columns and the rows of the dataset.

Then, when the data format is acceptable, we can perform a series of operations to make the
column's types and values suitable for modeling.  The reason for this is that most
machine learning methods require the input variables to follow some restrictions.  For
instance, some methods require the input variables to be real numbers, others require the
input variables are in a specific range, etc.

% \subsection{Reshaping}
%
% \textcolor{red}{TODO: pipeline exceptions: like pivoting and aggregating are kept outside
% the pipeline.}
%
% Reshaping is the process of changing the format of the data.  The most common reshaping
% operations are pivoting and unpivoting, which we have already discussed.  However, there
% are other reshaping operations that are useful in practice.
%
% For instance, one can reshape a dataset by splitting a column into multiple columns.  This
% is useful when a column contains multiple values that should be separated.  This can be
% done with mutation with appropriate expressions.  Some frameworks might provide special functions
% to do this, usually called splitting functions.
%
% We can also consider reshaping the operations of filtering, selecting, and aggregating.
% Filtering is usually done to reduce the scope of the data, given some conditions on the
% variables.  Selecting is usually done to remove irrelevant variables or highly correlated
% ones.  Aggregating in a reshaping task is usually applied together with pivoting to change the
% observational unit of the dataset.

% \begin{slidebox}{Reshaping}{}
%   \begin{itemize}
%     \item Reshaping is the process of changing the format of the data;
%     \item The most common reshaping operations are pivoting and unpivoting;
%     \item Other common operation include:
%       \begin{itemize}
%         \item Splitting a column into multiple columns;
%         \item Filtering to reduce the scope of the data;
%         \item Selecting to remove irrelevant variables or highly correlated ones;
%         \item Aggregating to change the observational unit of the dataset.
%       \end{itemize}
%   \end{itemize}
% \end{slidebox}

\subsection{Type conversion}

Type conversion is the process of changing the type of the values in the columns.  This
is usually done to make the data suitable for modeling.  For instance, some machine
learning methods require the input variables to be real numbers.

The most common type conversions are from categorical to numerical and from numerical to
categorical.  The former is usually done by creating dummy variables, i.e., a new column
for each possible value of the categorical variable.  This transformation is also known as
one-hot encoding.  The latter is usually done by binning (discretization and quantization
other concepts) the numerical variable, either by
frequency or by range.

% \begin{slidebox}{Type conversion}{}
%   \begin{itemize}
%     \item Type conversion is the process of changing the type of the values in the columns;
%     \item Use one-hot encoding to convert categorical variables to numerical;
%     \item Use binning to convert numerical variables to categorical.
%   \end{itemize}
% \end{slidebox}

\subsection{Normalization}

Normalization is the process of scaling the values in the columns.  This is usually done to
keep data in a specific range or to make the data comparable.  For instance, some machine
learning methods require the input variables to be in the range $[0, 1]$.

The most common normalization methods are standardization and rescaling.  The former is done
by subtracting the mean and dividing by the standard deviation of the values in the column.
The latter is performed so the values are in a specific range, usually $[0, 1]$ or $[-1, 1]$.

\begin{hlbox}{Clamping after rescaling}
  In production, it is common to clamp the values after rescaling.  This is done to avoid
  the model to make predictions that are out of the range of the training data.
\end{hlbox}

Related to normalization is the log transformation.  This is usually done to make the data
more symmetric or to reduce the effect of outliers.  The log transformation is the process
of taking the logarithm of the values in the column.

% \begin{slidebox}{Normalization}{}
%   \begin{itemize}
%     \item Normalization is the process of scaling the values in the columns;
%     \item Use standardization to make the values have mean 0 and standard deviation 1;
%     \item Use rescaling to make the values be in a specific range;
%     \item Use the log transformation to make the data more symmetric or to reduce the effect
%       of outliers.
%   \end{itemize}
% \end{slidebox}

\subsection{Sampling}

Sampling is the process of selecting a random subset of the data.  This is usually done to
reduce the size of the data or to create a balanced dataset.  For instance, some machine
learning methods are heavily affected by the number of observations in each class.
Also, some methods are computationally expensive and a smaller dataset might be enough to
solve the problem.

The most common sampling methods are random sampling and resampling\footnote{Resampling is
the process of sampling with replacement, sometimes called bootstrapping.}.  The former is
done by selecting a random subset of the data.  The latter is done by selecting a random
subset of the data with replacement.

While random sampling is useful to reduce the size of the data, resampling can be used to
increase the size of the data.  (Although this has some caveats.)  Moreover, resampling
can also create variations of the original dataset with the same distribution of the
values.

More advanced sampling methods are usually used to create balanced datasets.  For
instance, one can use the SMOTE algorithm\footfullcite{chawla2002smote} to create
synthetic observations of the minority class.

\textcolor{red}{In production, just ignore.}

\subsection{Dimensionality reduction}

Dimensionality reduction is the process of reducing the number of variables in the data.
This is usually done to reduce the complexity of the model or to identify irrelevant
variables.  The so-called \emph{curse of dimensionality} is a common problem in machine
learning, where the number of variables is much larger than the number of observations.

There are two main types of dimensionality reduction algorithms: feature selection and
feature extraction.  The former is done by selecting a subset of the variables that leads
to the best models.  The latter is done by creating new variables that are combinations
of the original ones.

Feature selection can be performed before modeling (filter), together with the model
search (wrapper), or as a part of the model itself (embedded).

Feature extraction is usually done by linear methods, such as principal component analysis
(PCA), or by non-linear methods, such as convolution layers and autoencoders.  These methods are able to
compress the information in the data into a smaller number of variables.

% \begin{slidebox}{Dimensionality reduction}{}
%   \begin{itemize}
%     \item Dimensionality reduction is the process of reducing the number of variables in the data;
%     \item Use feature selection to select a subset of the variables that leads to the best models;
%     \item Use feature extraction to create new variables that are combinations of the original ones.
%   \end{itemize}
% \end{slidebox}

% \begin{hlbox}{Practice!}
%   Can you identify which data transformation operations are used to make datasets
%   presented in \cref{chap:data} tidy?
% \end{hlbox}

\section{Data cleaning}

Data cleaning is the process of removing errors and inconsistencies from the data.  This is
usually done to make the data more reliable and to avoid bias in the analysis.

\subsection{Dealing with missing data}

Since most models do not cope with missing data, it is crucial to deal with it in the data
handling pipelines.

There are four main strategies to deal with missing data:
\begin{itemize}
  \item Remove the rows with missing data;
  \item Remove the columns with missing data;
  \item Impute the missing data;
  \item Use an indicator variable to mark the missing data.
\end{itemize}

Removing rows and columns are commonly used when the number of missing data is small
compared to the total number of rows or columns.  However, be aware that removing rows can
artificially change data distribution, especially when the missing data is not missing at
random.  Row removal suffers from the same problem as filtering operations in the
pipeline; the developer must specify a default behavior for the model when a row is
discarded in production.

Imputing the missing data is usually done by replacing the missing values with some
statistic of the available values in the column, such as the mean, the median, or the
mode.  This is a simple and effective strategy, but it can introduce bias in the data.
Also, it is not suitable when one is not sure whether the missing data is missing because
of a systematic error or phenomenon.

For this case, creating an indicator variable is a good strategy.  This is done by creating
a new column that contains a logical value indicating whether the data is missing or
not\footnote{Some kind of imputation is still needed, but we expect the model to deal
better with it since it can decide using both the indicator and the original variable.}.
By doing so, the model can learn the importance of the missing data.

% \footnote{\color{red}Sometimes the indicator variable is already present: pregnancy and sex
% example.}.

Many times the indicator variable is already present in the data.  For instance, in a
dataset that contains information about pregnancy, let us say the number of days since
the last pregnancy.  This information certainly be missing if sex and number of children \dots


\subsection{Dealing with invalid and inconsistent information}

Sometimes, during data collection, information is recorded using special codes.  For
instance, the value 9999 might be used to indicate that the data is missing.  Such codes
must be replaced with more appropriate values before modeling.  If a single variable
encodes more than one concept, new variables must be created to represent each concept.

Another common problem is inconsistent information.  For instance, the same category might
be represented by different names.  This is usually done by creating a dictionary that
maps the different names to a single one.

It is also useful to check whether all columns that store physical quantities have the
same unit of measurement.  If not, one must convert the values to the same unit.

If one knows that a variable has a specific range of values, it is useful to check
whether the values are within this range.  If not, one must replace the values with
missing data or with the closest valid value.

\subsection{Outliers}

Outliers are observations that are significantly different from the other observations.
They can be caused by errors in the data collection process or by the presence of a
different phenomenon.  In both cases, it is important to deal with outliers before
modeling.

There are many outliers detection methods, such as the Z-score, the IQR, and the DBSCAN.
% TODO

Like filtering operations in the pipeline, the developer must specify a default behavior
for the model when an outlier is detected in production.

% \begin{slidebox}{Data cleaning}{}
%   \begin{itemize}
%     \item Data cleaning is the process of removing errors and inconsistencies from the data;
%     \item Use the following strategies to deal with missing data:
%       \begin{itemize}
%         \item Remove the rows with missing data;
%         \item Remove the columns with missing data;
%         \item Impute the missing data;
%         \item Use an indicator variable to mark the missing data.
%       \end{itemize}
%     \item Replace special codes with more appropriate values;
%     \item Create a dictionary to map different names to a single one;
%     \item Check whether all columns that store physical quantities have the same unit of
%       measurement;
%     \item Check whether the values are within the expected range;
%     \item Use outlier detection methods to deal with outliers.
%   \end{itemize}
% \end{slidebox}

\section{Data integration}

Data integration is the process of combining data from different sources into a single
dataset.  This is usually done to create a more complete dataset or to create a dataset
with a different observational unit.

To perform integration, consider the discussions in \cref{sec:normalization,sub:bridge}.

Additionally, one must consider the following points:
\begin{itemize}
  \item Sometimes the same column may have different names in different datasets.  Redundant
    columns must be removed.
  \item Separate datasets that share the same variables usually happen because there is a
    hidden variable that is not present in the datasets.  During integration, the new
    variable must be created.
\end{itemize}

% \begin{slidebox}{Data integration}{}
%   \begin{itemize}
%     \item Data integration is the process of combining data from different sources into a single dataset;
%     \item Not every join is possible, consider the discussions in \cref{sec:normalization,sub:bridge};
%     \item Remove redundant columns;
%     \item Create new variables to represent the hidden variables.
%   \end{itemize}
% \end{slidebox}

In the data handling pipeline, data integration is useful for data enhancement.  This is
the process of adding new columns to the dataset or single instances.  For example,
imagine that in the tidy data we have a column with the zip code of the customers.  We can
use this information to join (in this case a left join) a dataset with social and economic
information about the region of the zip code.

% vim: spell spelllang=en
