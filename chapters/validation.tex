\chapter{Solution validation}
\label{chap:planning}
\glsresetall

\chapterprecishere{%
  All models are wrong, but some are useful.
  \par\raggedleft--- \textup{George E. P. Box}, Robustness in Statistics}

Once we have defined what an inductive problem is and the means to solve it, we need to
think about how to validate the solution.

In this chapter, we present the experimental planning that one can use in the data-driven
parts of a data science project.  \emph{Experimental planning}  in the context of data
science involves designing and organizing experiments to gather performance data
systematically in order to reach specific goals or test hypotheses.

The reason we need to plan experiments is that data science is experimental, i.e., we
usually lack a theoretical model that can predict the outcome of a given algorithm on a
given dataset.  This is why we need to run experiments to gather performance data and make
inferences from it.  The stochastic nature of data and of the learning process makes it
more difficult to predict the outcome of a given algorithm on a given dataset.  Robust
experimental planning is essential to ensure that the results of the experiments are
reliable and can be used to make decisions.

Moreover, we need to understand the main metrics that are used to evaluate the performance
of a solution --- i.e., the pair preprocessor and model.  Each learning task has different
metrics, and the goals of the project will determine which metrics are more important.

There is not a single way to plan experiments, but there are some common steps that can
be followed to design a good experimental plan.  In this chapter, we present a
framework for experimental planning that can be used in most data science projects
for inductive problems.

\begin{mainbox}{Chapter remarks}

  \boxsubtitle{Contents}

  \startcontents[chapters]
  \printcontents[chapters]{}{1}{}
  \vspace{1em}

  \boxsubtitle{Context}

  \begin{itemize}
    \itemsep0em
    \item Before putting a solution into production, we need to validate it.
    \item The validation process is experimental.
  \end{itemize}

  \boxsubtitle{Objectives}

  \begin{itemize}
    \itemsep0em
    \item Understand the importance of experimental planning.
    \item Learn the main evaluation metrics used in predictive tasks.
    \item Learn how to design an experimental plan to validate a solution.
  \end{itemize}

  \boxsubtitle{Takeaways}

  \begin{itemize}
    \itemsep0em
    \item Evaluation metrics should be chosen according to the goals of the project.
    \item The experimental plan should be designed to gather performance data
      systematically.
    \item A hypothesis test can be used to validate the results of the experiments.
  \end{itemize}
\end{mainbox}

{}
\clearpage

\section{Evaluation}
\label{sec:evaluation}

One fundamental step in the validation of a data-driven solution for a task is the
\emph{evaluation} of the pair preprocessor and model. This chapter presents strategies to
measure performance of
classifiers and regressors, and how to interpret the results.

We consider the following setup.  Let $T = (K, H, c)$ be a table that represents the data
in the desired observational unit --- as defined in \cref{sec:formal-structured-data}.
Without loss of generality --- as the keys are not used in the modeling process ---, we
can consider $K = \{1, 2, \dots\}$ such that $\rowcard[i] = 1$, if $i \in \{1, \dots,
n\}$, and $\rowcard[i] = 0$, otherwise.  That means that every row $r \in \{1, \dots, n\}$
is present in the table.

The table is split into two sets: a training set, given by indices (or keys)
$\mathcal{I}_\text{training} \in \{1, \dots, n\}$, and a test set, given by indices
$\mathcal{I}_\text{test} \in \{1, \dots, n\}$, such that $$\mathcal{I}_\text{training}
\cap \mathcal{I}_\text{test} = \emptyset$$ and $$\mathcal{I}_\text{training} \cup
\mathcal{I}_\text{test} = \{1,\dots,n\}\text{.}$$

The bridge between the table format (\cref{def:itable}) and the data format used in the
learning process (as described in \cref{sec:learning-problem}) is explained in the
following.  We say that the pair $(\vec{x}_i, y_i)$ contains the feature vector $\vec{x}_i$
and the target value $y_i$ of the sample with key $i$ in table $T$.  Mathematically,
given target variable $h \in H$, we have that $y_i = c(i, h)$ and $\vec{x}_i$ is the tuple
$$\big(c(i, h') : h' \in H \setminus \left\{ h \right\}\big)\text{.}$$

For evaluation, we consider a data preprocessing technique $F$ and a learning machine
$M$.  The following steps are taken.

\paragraph{Preprocessing}

Preprocessing technique $F$ is applied to the training set $T_\text{training} = (K, H,
c_\text{training})$ where \[
  c_\text{training}(i, h) = \begin{cases}
    c(i, h) & \text{if } i \in \mathcal{I}_\text{training}\text{,} \\
    () & \text{otherwise}\text{.}
  \end{cases}
\]  The result is an adjusted training set $T'_\text{training}$ and a fitted
preprocessor $f(\vec{x}; \phi) \equiv f_\phi(\vec{x})$, where $\vec{x} \in \mathcal{X}$
for some space $\mathcal{X}$ that does not include (or does not modify) the target
variable --- consult \cref{sub:formal-preprocessing}.  Note that, by definition, the size
of the adjusted training set can be different from the original due to sampling or
filtering.  The hard requirement is that the target variable $h$ is not changed.

\paragraph{Learning}

The learning machine $M$ is trained on the adjusted training set $D'_\text{training} =
\{(\vec{x}'_i, y'_i)\}$, where pairs $(\vec{x}'_i, y'_i)$ come from the table
$T'_\text{training}$.  The result is a model $f(\vec{x}'; \theta) \equiv
f_\theta(\vec{x}')$ --- consult \cref{chap:slt}.

\paragraph{Transformation}

The preprocessor $f_\phi$ is applied to the test set $T_\text{test} = (K, H,
c_\text{test})$ where \[
  c_\text{test}(i, h) = \begin{cases}
    c(i, h) & \text{if } i \in \mathcal{I}_\text{test}\text{,} \\
    () & \text{otherwise}\text{.}
  \end{cases}
\]  The result is a preprocessed test set $T'_\text{test}$ from which we can obtain the
set $D'_\text{test} = \{(\vec{x}'_i, y_i) : i \in \mathcal{I}_\text{test}\}$ such
that $\vec{x}'_i = f_\phi(\vec{x}_i)$.  Note that, to avoid \gls{leakage} and other
issues, the preprocessor has no access to the target values $y_i$ (even if the
adjusted training set uses the label somehow).

\paragraph{Prediction}

The model $f_\theta$ is used to make predictions on the preprocessed test set
$D'_\text{test}$ to obtain predicted values $\hat{y}_i = f_\theta(\vec{x}'_i)$ for all
$i \in \mathcal{I}_\text{test}$.

\paragraph{Evaluation}

By comparing $\hat{y}_i$ with $y_i$ for all $i \in \mathcal{I}_\text{test}$, we
evaluate how well the choice of $\phi$ (parameters of the preprocessor) and $\theta$
(parameters of the model) is.

\subsection{Binary classification evaluation}

To assess the quality of a solution for a binary classification task, we need to know which
samples in the test set were classified into which classes.  This information is
summarized in the \emph{confusion matrix}, which is the basis for performance metrics in
classification tasks.

\subsubsection{Confusion matrix}

The confusion matrix is a table where the rows represent the true classes and the columns
represent the predicted classes.  The diagonal of the matrix represents the correct
classifications, while the off-diagonal elements represent errors.  For binary
classification, the confusion matrix is given by
\begin{equation*}
  \begin{blockarray}{cccc}
    & & \multicolumn{2}{c}{\text{Predicted}} \\
    & & 1 & 0 \\
    \begin{block}{l c (c c)}
      \text{Expected} & 1 & \text{TP} & \text{FN} \\
      & 0 & \text{FP} & \text{TN} \\
    \end{block}
  \end{blockarray}
\end{equation*}
where TP is the number of true positives
$$|\{ i \in \mathcal{I}_\text{test} : y_i = 1 \land \hat{y}_i = 1 \}|\text{,}$$
TN is the number of true negatives
$$|\{ i \in \mathcal{I}_\text{test} : y_i = 0 \land \hat{y}_i = 0 \}|\text{,}$$
FN is the number of false negatives
$$|\{ i \in \mathcal{I}_\text{test} : y_i = 1 \land \hat{y}_i = 0 \}|\text{,}$$
and FP is the number of false positives
$$|\{ i \in \mathcal{I}_\text{test} : y_i = 0 \land \hat{y}_i = 1 \}|\text{.}$$

\subsubsection{Performance metrics}

From the confusion matrix, we can derive several performance metrics.  Each of them focuses
on different aspects of the classification task, and the choice of the metric depends on
the problem at hand.  Each metric prioritizes different types of errors and yields
a value between 0 and 1, where 1 is the best possible value.

\paragraph{Accuracy} is the proportion of correct predictions over the total number of
samples in the test set, given by
\begin{equation*}
  \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}\text{.}
\end{equation*}
This metric is simple and easy to interpret: a classifier with an accuracy of 1 is
perfect, while a classifier with an accuracy of 0.5 misses half of the predictions.
Accuracy assigns the same weight to any kind of error --- i.e., false positives and false
negatives.  As a result, if the proportion of positive and negative samples is imbalanced,
the value of accuracy may become misleading.  Let $\pi$ be the ratio of positive samples in
the test set --- consequently, $1-\pi$ is the ratio of negative samples ---, then a
classifier that correctly predicts all positive samples and none of the negative samples
will have an accuracy of $\pi$.  If $\pi$ is close to 1, the classifier will have a high value
of accuracy even if it is not good at predicting the negative class.

This issue is not impeditive for the usage of accuracy in imbalanced datasets, but one
needs to be aware that accuracy values lower than $\max(\pi, 1-\pi)$ are not better than
guessing.

\paragraph{Balanced accuracy} aims to solve this interpretation issue of the accuracy.  It
is the average of the true positive rate (TPR) and the true negative rate (TNR), given by
\begin{equation*}
  \text{Balanced Accuracy} = \frac{\text{TPR} + \text{TNR}}{2}\text{,}
\end{equation*}
where
\[
  \text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}\text{,}
\]
and
\[
  \text{TNR} = \frac{\text{TN}}{\text{TN} + \text{FP}}\text{.}
\]
Each term penalizes a different type of error independently: TPR penalizes false
negatives, while TNR penalizes false positives.  Balanced accuracy is useful when the cost
of errors on the minority class is higher than the cost of errors on the majority class.
This way, any value greater than 0.5 is better than random guessing.

A limitation of the balanced accuracy is that it ``automatically'' assigns the weight of
errors based on the class proportion, which may not be the best choice for the problem.
Other metrics focus only on one of the classes and are more flexible to adjust the weight of
errors.

\paragraph{Precision} is an asymmetrical metric that focuses on the positive class.  It is
the proportion of true positive predictions over the total number of samples predicted as
positive, given by
\begin{equation*}
  \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}\text{.}
\end{equation*}
This metric is useful when the cost of false alarms is high, as it quantifies the
ability of the classifier to avoid false positives.  For example, in a medical diagnosis
task, precision is important to avoid unnecessary treatments (false positive diagnoses).
Semantically, precision measures how confident we can be that a positive prediction is
actually positive.  Note that it measures nothing about the ability of the classifier in
terms of the negative predictions.

\paragraph{Recall} is another asymmetrical metric that also focuses on the positive class.
It is the proportion of true positive predictions over the total number of
samples that are actually positive, given by
\begin{equation*}
  \text{Recall} = \text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}\text{.}
\end{equation*}
This metric is useful when the cost of missing a positive sample is high, as it quantifies the
ability of the classifier to avoid false negatives.  It can also be interpreted as the
``completeness'' of the classifier: how many positive samples were correctly retrieved.
For example, in a medical diagnosis task, recall is important to avoid missing a
diagnosis.

\paragraph{F-score} is a way of balancing both kinds of errors, false positives and false
negatives, while maintaining the focus on the positive class. It is the weighted harmonic
mean of precision and recall given by
\begin{equation*}
  \text{F-score}(\beta) = \text{F}_\beta\text{-score} =
    \frac%
      {(1 + \beta^2) \cdot \text{Precision} \cdot \text{Recall}}
      {\beta^2 \cdot \text{Precision} + \text{Recall}}\text{,}
\end{equation*}
where $\beta > 0$ is a parameter that controls the weight of precision in the metric.
The most common value for $\beta$ is 1, which gives the F$_1$-score.  Higher values of
$\beta$ give more weight to precision ($\beta > 1$), while lower values give more weight
to recall ($0 < \beta < 1$).

\paragraph{Specificity} goes in the opposite direction of recall, focusing on the negative
class.  It is the proportion of true negative predictions over the total
number of samples that are actually negative, given by
\begin{equation*}
  \text{Specificity} = \text{TNR} = \frac{\text{TN}}{\text{TN} + \text{FP}}\text{.}
\end{equation*}
This metric is very common in the medical literature, but less common in other contexts.
The probable reason is that it is easier to interpret the metrics that focus on the
positive class, as the negative class is usually the majority class --- and, thus, less
interesting.

\subsubsection{Interpretation of metrics}

\Cref{tab:classification-metrics} summarizes the properties of the classification
performance metrics.  Accuracy and balanced accuracy are good metrics when no particular
class is more important than the other.  Remember, however, that balanced accuracy gives
more weight to errors on the minority class.  Precision and recall are useful to evaluate
the performance of the solution in terms of the positive class.  They are complementary metrics,
and looking at only one of them may give a biased view of the performance --- more on that
below.  The F-score is a way to balance precision and recall with a controllable parameter.

\begin{tablebox}[label=tab:classification-metrics]{Summary of the properties of
  data classification performance metrics.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{l c c}
    \toprule
    \textbf{Metric} & \textbf{Focus} & \textbf{Interpretation} \\
    \midrule
    Accuracy           & Symmetrical & Penalizes all \\
    Balanced Accuracy  & Symmetrical & Penalizes all (weighted) \\
    Recall (TPR)       & Positive & Penalizes FN \\
    Precision          & Positive & Penalizes FP \\
    F-score            & Positive & Penalizes all (weighted) \\
    Specificity (TNR)  & Negative & Penalizes FP \\
    % Fall-out (FPR)     & Negative & Not affected & Penalizes TN \\
    % FPR = 1 - TNR
    \bottomrule
  \end{tabular}
\end{tablebox}

A common misconception about the asymmetrical metrics (especially precision) is that they
are always robust to class imbalance.  Observe \cref{tab:classification-metrics-ex}, which shows
the behavior of the classification performance metrics for three (useless) classifiers: one
that always predicts the positive class (Guess 1), another that always predicts the
negative class (Guess 0), and a classifier that randomly guesses the class independently
of the class priors (Random).

\begin{tablebox}[label=tab:classification-metrics-ex]{Behavior of classification
  performance metrics for different classifiers.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{l c c c}
    \toprule
    \textbf{Metric} & \textbf{Guess 1} & \textbf{Guess 0} & \textbf{Random} \\
    \midrule
    Accuracy$^\dagger$ & $\pi$ & $1 - \pi$ & $0.5$ \\
    Balanced Accuracy & $0.5$ & $0.5$ & $0.5$ \\
    Recall (TPR) & $1$ & $0$ & $0.5$ \\
    Precision$^\dagger$ & $\pi$ & $0/0 = 0$ & $\pi$ \\
    F$_1$-score$^\dagger$ & $\frac{2 \pi}{1 + \pi}$ & 0 & $\frac{2 \pi}{1 + 2\pi}$ \\
    Specificity (TNR) & $0$ & $1$ & $0.5$ \\
    \bottomrule
  \end{tabular}
  \tcblower
  Performance of different classifiers in the example of a dataset with ratio $\pi$ of
  positive and $1-\pi$ of negative samples.  Metrics affected by class imbalance are
  marked with $^\dagger$.
\end{tablebox}

We can see that, as $\pi \to 1$, i.e. the positive class dominates the dataset, guessing
the positive class achieves maximum values for metrics like accuracy, precision, and
F$_1$-score.  Even for random guessing the class, precision (and F$_1$-score) is affected
by the class imbalance, yielding $1$ (and $2/3$) as $\pi \to 1$.  As a result, these
metrics should be preferred when the positive class is the minority class, so the results
are not erroneously inflated --- and, consequently, mistakenly interpreted as good.
\textcite{Williams2021}\footfullcite{Williams2021} provides an interesting discussion on
that.

Finally, besides accuracy, the other metrics do not behave well when the evaluation set is
too small.  In this case, the metrics may be too sensitive to the particular samples in
the test set or may not be able to be calculated at all.

\subsection{Regression estimation evaluation}

Performance metrics for regression tasks are usually calculated based on the error (also
called residual) $$\epsilon_i = \hat{y}_i - y_i$$ for all $i \in \mathcal{I}_\text{test}$ or
a scaled version $$\epsilon_i^{(f)} = f(\hat{y}_i) - f(y_i)\text{,}$$ for some scaling
function $f$.

\subsubsection{Performance metrics}

From the errors, we can calculate several performance metrics that give us useful
information about the behavior of the model.  Specifically, we are interested in
understanding what kind of errors the model is making and how large they are.  Unlike
classification, the higher the value of the metric, the worse the model is.

\paragraph{Mean absolute error} is probably the simplest performance metric for regression
estimation tasks.  It is the average of the absolute values of the errors,
given by
\begin{equation*}
  \text{MAE} = \frac{1}{n} \sum_{i=1}^n | \epsilon_i |\text{.}
\end{equation*}
This metric is easy to interpret, is in the same unit as the target variable, and gives an
idea of the average error of the model.  It ignores the direction of the errors, so it is
not useful to understand if the model is systematically overestimating or underestimating
the target variable.

\paragraph{Mean squared error} is the average of the squared residuals, given by
\begin{equation*}
  \text{MSE} = \frac{1}{n} \sum_{i=1}^n \epsilon_i^2\text{.}
\end{equation*}
This metric penalizes large errors more than the mean absolute error, as the squared
residuals are summed.

\paragraph{Root mean squared error} is the square root of the mean squared error, given by
\begin{equation*}
  \text{RMSE} = \sqrt{\text{MSE}}\text{.}
\end{equation*}
This metric is in the same unit as the target variable, which makes it easier to
interpret.  It keeps the same properties as the mean squared error, such as penalizing
large errors more than the mean absolute error.

Both MAE and RMSE (or MSE) work well for positive and negative values of the target
variable.  However, they might be misleading when the range of the target variable is
large.

\paragraph{Mean absolute percentage error} is an alternative when the target variable (and
the predictions) assume only strictly positive values, i.e., $y_i > 0$ and $\hat{y}_i > 0$.
It is the average of the relative errors, given by
\begin{equation*}
  \text{MAPE} = \frac{1}{n} \sum_{i=1}^n \frac{|\epsilon_i|}{y_i}\text{.}
\end{equation*}
This metric is useful when the range of the target variable is large, as it gives an idea
of the relative error of the model, not the absolute error.

\paragraph{Mean absolute logarithmic error} is an alternative for the MAPE under the same
premises of the target values.  It aims to reduce the influence of outliers in the error
calculation, especially when the target variable prior follows a long-tail distribution
--- many small values and few large values.  Distributions like that are common in
practice, e.g., in sales, income, and population data.  It is given by
\begin{equation*}
  \text{MALE} = \frac{1}{n} \sum_{i=1}^n | \epsilon_i^{(\ln)} | =
    \frac{1}{n} \sum_{i=1}^n | \ln\hat{y}_i - \ln y_i |\text{.}
\end{equation*}

\subsubsection{Interpretation of metrics}

Note that, unlike the classification performance metrics, the scale of the regression
performance metrics is not bounded between 0 and 1.  This makes it potentially harder to interpret
the results, as the values depend on the scale of the target variable.

Absolute error metrics, like MAE and RMSE, are useful for understanding the central tendency
of the magnitude of the errors.  They are easy to interpret because they are in the same
unit as the target variable.  However, they tend to be less informative when the target
variable has a large range or when the errors are not normally distributed.

In those situations, relative error metrics, like MAPE and MALE, are more useful.  For
instance, imagine we are predicting house prices.  The error of \$20,000
for a house that costs \$100,000 is more significant than the same error for a house that
costs \$1,000,000.  The absolute error is the same in both cases, but the relative error
is different.

In that example, the MAPE would be 20\% for the first house and 2\% for the second house.
Note, however, that MAPE punishes overestimating more than underestimating in
multiplicative terms.  Consider the example in \cref{tab:MAPEvsMALE}.  In the first row,
the prediction is ten times larger than the actual value, which results in a MAPE of
900\%.  In the second row, the prediction is one tenth of the actual value, which results
in a MAPE of 90\%.

\begin{tablebox}[label=tab:MAPEvsMALE]{Comparison of relative error metrics.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{r r r r r}
    \toprule
    $\hat{y}$ & $y$ & $\epsilon$ & MAPE & $\exp(\text{MALE})$ \\
    \midrule
    100 & 10 & 90 & 9.0 & 10 \\
      1 & 10 &  9 & 0.9 & 10 \\
    \bottomrule
  \end{tabular}
  \tcblower
  MAPE and MALE for two predictions.  The MAPE punishes overestimating more than
  underestimating.
\end{tablebox}

If multiplicative factors of the error are important, one should consider using MALE.
Observe that $\ln(\hat{y}) - \ln(y) = \ln(\hat{y}/y)$, which is the logarithm of the
ratio of the prediction to the actual value.  In the case of the absolute value, we have
another interesting property: \[
  |\ln\hat{y} - \ln y| =
    |\ln\frac{\hat{y}}{y}| =
    |\ln\frac{y}{\hat{y}}| =
    \ln\max\left(\frac{\hat{y}}{y}, \frac{y}{\hat{y}}\right)\text{.}
\]
\textcite{Tofallis2015}\footfullcite{Tofallis2015} discuss some of these advantages.  To
interpret MALE, we can use the exponential function, which gives us a multiplicative
factor of the error.  In the example in \cref{tab:MAPEvsMALE}, we have that \[
  \exp\ln\max\left(\frac{100}{10}, \frac{10}{100}\right) =
    \max\left(\frac{100}{10}, \frac{10}{100}\right) = 10\text{.}
\]

Finally, for the experimental plan we propose in this book, we should avoid metrics like
coefficient of determination, $R^2$, as we do not make assumptions about the model --- in this
case, we do not assume that the model is linear.  Similarly to data classification, we
should prefer metrics that work well with small test sets.

\subsection{Probabilistic classification evaluation}

A particular case of the regression estimation is when we want to estimate the
probability\footnote{Although the term probability is used, the output of the regressor
does not need to be a probability in the strict sense.  It is a confidence level in the
interval $[0, 1]$ that can be interpreted as a probability.} of a sample belonging to the
positive class --- i.e. $y = 1$.  In this case, the output of the model should be a
value in the interval $[0, 1]$.  We can use a threshold $\tau$ to convert the
probabilities into binary predictions.  The default threshold is usually $\tau = 0.5$ ---
a sample is positive if the probability is greater than or equal to 0.5, and it is negative,
otherwise.

However, the threshold can be adjusted to change the trade-off between recall and
specificity. A low threshold, $\tau \approx 0$, will increase recall at the expense of
specificity, while a high threshold, $\tau \approx 1$, will increase specificity at the
expense of recall.

Thus, any regressor $f_R : \mathcal{X} \rightarrow [0, 1]$ can be converted into a binary
classifier $f_C : \mathcal{X} \rightarrow \{0, 1\}$ by comparing the output with the
threshold $\tau$:
\begin{equation*}
  f_C(\vec{x}; \tau) = \begin{cases}
    1 & \text{if } f_R(\vec{x}) \geq \tau\text{,} \\
    0 & \text{otherwise}\text{.}
  \end{cases}
\end{equation*}

Since the task is still a classification task, one should not use regression performance
metrics.  On the other hand, instead of choosing a particular threshold and measuring the
resulting classifier performance, we can summarize the performance of all possible
variations of the classifiers using appropriate metrics.

Before diving into the metrics, consider the following error metric.  Let false positive
rate (FPR) be the proportion of false positive predictions over the total number of
samples that are actually negative,
\begin{equation*}
  \text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}\text{.}
\end{equation*}
It is the complement of the specificity, i.e. $\text{FPR} = 1 - \text{Specificity}$.

Consider the example in \cref{tab:prob-reg-out} of a given test set and the predictions of
a regressor. We can see that a threshold of 0.5 would yield a classifier that errors
in 3 out of 9 samples.  We can adjust the threshold to understand the behavior of the
other possible classifiers.

\begin{tablebox}[label=tab:prob-reg-out]{Illustrative example of probability regressor
  output.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{rr}
    \toprule
    \textbf{Expected} & \textbf{Predicted} \\
    \midrule
    0 & 0.1  \\
    0 & 0.5  \\
    0 & 0.2  \\
    0 & 0.6  \\
    1 & 0.4  \\
    1 & 0.9  \\
    1 & 0.7  \\
    1 & 0.8  \\
    1 & 0.9  \\
    \bottomrule
  \end{tabular}
\end{tablebox}

We first sort the samples by the predicted probabilities and then calculate the TPR
(recall) and FPR for each threshold.  We need to consider only thresholds equal to the
predicted values to understand the variations.  In this case, TPR values become the
cumulative sum of the expected outputs divided by the total number of positive samples,
and FPR values become the cumulative sum of the complement of the expected outputs
divided by the total number of negative samples.

\begin{tablebox}[label=tab:prob-reg-example]{Illustrative example of classifiers derived
  from different thresholds.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{rrrrr}
    \toprule
    \textbf{Expected} & \textbf{Threshold} & \textbf{TPR} & \textbf{FPR}  \\
    \midrule
    - & - / $\infty$ & $0/5$ & $0/4$ \\
    1 & $0.9$        & $1/5$ & $0/4$ \\
    1 & $0.9$        & $2/5$ & $0/4$ \\
    1 & $0.8$        & $3/5$ & $0/4$ \\
    1 & $0.7$        & $4/5$ & $0/4$ \\
    0 & $0.6$        & $4/5$ & $1/4$ \\
    0 & $0.5$        & $4/5$ & $2/4$ \\
    1 & $0.4$        & $5/5$ & $2/4$ \\
    0 & $0.2$        & $5/5$ & $3/4$ \\
    0 & $0.1$        & $5/5$ & $4/4$ \\
    \bottomrule
  \end{tabular}
  \tcblower
  Performance of different classifiers derived from the regressor output in
  \cref{tab:prob-reg-out}.  The thresholds are equal to the predicted values.
\end{tablebox}

Note that, from the ordered list of predictions, we can easily see that a threshold of 0.7
would yield a classifier that commits only one error.  A way to summarize the performance
of all possible classifiers is presented in the following.

\subsubsection{Receiver operating characteristic}

The receiver operating characteristic (ROC) curve is a graphical representation of the
trade-off between TPR and FPR as the threshold
$\tau$ is varied.  The ROC curve is obtained by plotting the TPR against the FPR for all
possible thresholds.  \Cref{fig:roc-example} is the ROC curve for the example in
\cref{tab:prob-reg-example}.

\begin{figurebox}[label=fig:roc-example]{Illustrative example of ROC curve.}
  \centering
  \begin{tikzpicture}
    \datavisualization [
      scientific axes=clean,
      visualize as line/.list={curve, diagonal},
      visualize as scatter/.list={points},
      x axis={label={FPR}, include value=0.0, include value=1.0, length=5cm},
      y axis={label={TPR}, include value=0.0, include value=1.0, length=5cm},
      all axes={grid},
      style sheet={vary dashing},
    ] data [set=curve] {
      x, y
      0.0, 0.0
      0.0, 0.2
      0.0, 0.4
      0.0, 0.6
      0.0, 0.8
      0.25, 0.8
      0.50, 0.8
      0.50, 1.0
      0.75, 1.0
      1.0, 1.0
    } data [set=diagonal] {
      x, y
      0.0, 0.0
      1.0, 1.0
    } data [set=points] {
      x, y
      0.0, 0.0
      0.0, 0.2
      0.0, 0.4
      0.0, 0.6
      0.0, 0.8
      0.25, 0.8
      0.50, 0.8
      0.50, 1.0
      0.75, 1.0
      1.0, 1.0
    };
  \end{tikzpicture}
  \tcblower
  ROC curve for the example in \cref{tab:prob-reg-example}.  The diagonal line represents
  a random classifier, and points above the diagonal are better than random.
\end{figurebox}

The ROC curve is useful to explore the trade-off between recall and specificity.  The
diagonal line represents a random classifier, and points above the diagonal are better
than random.

The area under the ROC curve (AUC) is an interesting metric of the performance of the
family of classifiers.  It ranges between 0 and 1, where 1 is the best possible value.
The AUC is scale invariant, which means that it measures how well
predictions are ranked, rather than their absolute values.  It is also robust to
class imbalance, once both recall and specificity are considered.
In our example, the AUC is $0.9$.

% TODO: error visualization or summary of the DET curve
% \subsubsection{Detection error trade-off}
%
% The detection error trade-off (DET) curve is a graphical representation of the trade-off
% between the false positive rate and the false negative rate (FNR),
% \begin{equation*}
%   \text{FNR} = \frac{\text{FN}}{\text{TP} + \text{FN}} = 1 - \text{TPR}\text{.}
% \end{equation*}
% The DET curve is similar to the ROC curve, but by plotting only the FPR and FNR, it gives
% a better view of the ``cost'' (errors) of different thresholds.  The DET curve is
% especially useful when the cost of false positives and false negatives is different.
% The DET curve of our example is shown in \cref{fig:det-example}.
%
% \begin{figurebox}[label=fig:det-example]{Illustrative example of DET curve.}
%   \centering
%   \begin{tikzpicture}
%     \datavisualization [
%       scientific axes=clean,
%       visualize as line,
%       x axis={label={FPR}, include value=0.0, include value=1.0, length=5cm},
%       y axis={label={FNR}, include value=0.0, include value=1.0, length=5cm},
%       all axes={grid},
%     ] data {
%       % based on the table above
%       x, y
%       0.0, 1.0
%       0.0, 0.8,
%       0.0, 0.6,
%       0.2, 0.6,
%       0.2, 0.4,
%       0.2, 0.2,
%       0.4, 0.2,
%       0.6, 0.2,
%       0.6, 0.0,
%       0.8, 0.0,
%       1.0, 0.0,
%     };
%   \end{tikzpicture}
%   \tcblower
%   DET curve for the example in \cref{tab:prob-reg-example}.  The diagonal line represents
%   a random classifier, and points below the diagonal are better than random.
% \end{figurebox}
%
% Usually, the DET curve is plotted in a normal deviate scale~\parencite{Martin1997}.  In
% this scale, the axes are transformed to show the error rates in a more linear way.
%
% \begin{figurebox}[label=fig:det-example-normal]{Illustrative example of DET curve (normal deviate scale).}
%   \centering
%   \begin{tikzpicture}
%     \datavisualization [
%       scientific axes=clean,
%       visualize as line,
%       x axis={%
%         label={FPR},
%         include value=0.001, include value=0.999,
%         scaling=-3 at 0cm and 3 at 5cm,
%         ticks={%
%         %   major={at={0.001, 0.005, 0.02, 0.05, 0.1, 0.2, 0.5, 0.8, 0.9, 0.95, 0.98, 0.995, 0.999}},
%           tick typesetter/.code={%
%             \pgfmathprintnumber{##1}$\sigma$
%           },%
%         },%
%       }%
%     ] data {
%       x,y
%       -3.090232306167813,3.090232306167813
%       -2.5758293035489,2.5758293035489
%       -2.0537489106318225,2.053748910631822
%       -1.6448536269514726,1.6448536269514715
%       -1.2815515655446008,1.2815515655446008
%       -0.8416212335729142,0.8416212335729144
%       0,0
%       0.8416212335729144,-0.8416212335729142
%       1.2815515655446008,-1.2815515655446008
%       1.6448536269514715,-1.6448536269514726
%       2.053748910631822,-2.0537489106318225
%       2.5758293035489,-2.5758293035489
%       3.090232306167813,-3.090232306167813
%     };
%   \end{tikzpicture}
% \end{figurebox}

% vim: spell spelllang=en

\section{An experimental plan for data science}

Like any other experimental science, data science requires a robust experimental
plan to ensure that evaluation results are reliable and can be used to make decisions.
Failure to use the resources we have at hand --- i.e., the limited amount of data ---
can lead to incorrect conclusions about the performance of a solution.

There are important elements that should be considered when designing an experimental
plan.  These elements are:
\begin{itemize}
  \item \textbf{Hypothesis}: The main question that the experiment aims to validate.
    In this chapter, we address common questions in data science projects and how to
    validate them.
  \item \textbf{Data}: The dataset that will be used in the experiment.  In
    \cref{chap:fundamental,chap:data}, we address topics about collecting and organizing data.
    In \cref{chap:handling}, we address topics about preparing the data for the
    experiments.
  \item \textbf{Solution search algorithm}: Techniques that find a solution for the task.
    We use the term ``search'' because the chosen algorithm aims at optimizing both the
    parameters of the preprocessing chain and those of the model. The theoretical
    basis for these techniques is in \cref{chap:preprocess,chap:slt}.
  \item \textbf{Performance measuring}: The metric that will be used to evaluate the
    performance of the model.  Refer to \cref{sec:evaluation} for the main metrics used in
    binary classification and regression estimation tasks.
\end{itemize}

A general example of a description of an experimental plan is ``What is the probability that
the technique $A$ will find a model that reaches a performance $X$ in terms of metric $Y$ in
the real-world given dataset $Z$ as training set (assuming $Z$ is a representative
dataset)?''

Another example is ``Is technique $A$ better than technique $B$ for finding a model that
predicts the output with $D$ as a training set in terms of metric $E$?''

In the next sections, we consider these two cases: \emph{estimating expected performance}
and \emph{comparing algorithms}.  Before that, we discuss a strategy to make the best use
of the finite amount of data we have available.

\subsection{Sampling strategy}

When dealing with a data-driven solution, the available data is a representation of the
real world.  So, we have to make the best use of the data we have to estimate how well our
solution is expected to be in production.

As we have seen, the more data we use to search for a solution, the better the solution is
expected to be.  Thus, we use the whole dataset for deploying a solution.  But, what
method for preprocessing and learning should we use?  How well is that technique
expected to perform in the real world?

Let us say we fix a certain technique, let us call it $A$.  Let $M$ be the solution found
by $A$ using the whole dataset $D$.  If we assess $M$ using the whole dataset $D$, the
performance $p$ we get is optimistic.  This is because $M$ has been trained and tested
on the same data.

One could argue that we could use a hold-out set to estimate the performance of $M$ ---
i.e., splitting the dataset into a training set and a test set once.  However, this does
not solve the problem.  The performance $p$ we observe in the test set might be an overestimation
or an underestimation of the performance of $M$ in production.  This is because the
randomly chosen test set might be an ``outlier'' in the representation of the real world,
containing cases that are too easy or too hard to predict.

The correct way to estimate the performance of $M$ is to address performance as a
random variable, since both the data and the learning process are stochastic.
By doing so, we can study the distribution of the performance, not particular values.

As with any statistical evaluation, we need to generate samples
of the performance of the possible solutions that $A$ is able to obtain. To do so, we use
a sampling strategy to generate datasets $D_1, D_2, \ldots$ from $D$.  Each
dataset is further divided into a training set and a test set, which must be disjoint.
Each training set is thus used to find a solution --- $M_1, M_2, \ldots$ for each
training set --- and the test set is used to evaluate the performance --- $p_1, p_2,
\ldots$ for each test set --- of the solution.  The test set emulates the real-world
scenario, where the model is used to make predictions on new data.

The most common sampling strategy is the \emph{cross-validation}.  It assumes that data are
independent and identically distributed (i.i.d.).  This sampling strategy divides
the dataset into $r$ folds randomly, with the same size.  Each part (fold) is used as a
test set once and as a training set $r-1$ times.  So, first we use as training set folds
$2, 3, \ldots, r$ and as test set fold $1$.  Then, we use as training set folds $1, 3,
\ldots, r$ and as test set fold $2$. And so on.  See \cref{fig:cross-validation}.

\begin{figurebox}[label=fig:cross-validation]{Cross-validation}
  \centering
  \begin{tikzpicture}
    \foreach \i in {1, 2, 3, 4} {
      \node at (2 * \i, 0) {Fold \i};
      \foreach \j in {1, 2, 3, 4} {
        % if \i == \j, then it is the test set
        \ifnum\i=\j
          \node [smallblock, minimum width=16mm, minimum height=6mm] (fold\i\j) at (2 * \i, -\j) {Test};
        \else
          \node [smalldarkblock, minimum width=16mm, minimum height=6mm] (fold\i\j) at (2 * \i, -\j) {Training};
        \fi
      }
    }
    \foreach \j in {1, 2, 3, 4} {
      \node [draw, dashed, fit={(fold1\j) (fold4\j)}] {};
    }
  \end{tikzpicture}
  \tcblower
  Cross-validation is a technique to sample training and test sets.  It divides the
  dataset into $r$ folds, using $r-1$ folds as a training set and the remaining fold as a
  test set.
\end{figurebox}

If possible, one should use repeated cross-validation, where this process is repeated many
times, each having a different fold partitioning chosen at random.  Also, when dealing with
classification problems, we should use stratified cross-validation, where the distribution
of the classes is preserved in each fold.

% TODO Performance Visualization
% Also, the application of a single cross-validation sampling enables us to create a
% predicted vector for the whole dataset.  This is done by concatenating the predictions for
% each fold.  (Note however that the predictions are not totally independent, as they share
% some training data.  This dependency should be taken into account when analyzing the
% results.) This vector can be used to perform hypothesis tests --- like McNemar's test, see
% \cref{sub:comparison} --- or to plot ROC (Receiver Operating Characteristic) curves or DET
% (Detection Error Tradeoff) curves --- see \cref{sec:evaluation}.

\subsection{Collecting evidence}

Once we understand the sampling strategy, we can design the experimental plan to collect
evidence about the performance of the solution.  The plan involves the following steps.

The solution search algorithm $A$ involves both a
given data preprocessing chain and a machine learning method.  Both of them generate a
different result for each dataset $D_k$ used as an input.  In other words, the parameters
$\phi$ of the data preprocessing step are adjusted --- see \cref{chap:preprocess} --- and the
parameters $\theta$ of the machine learning model are adjusted --- see \cref{chap:slt}.
These parameters, $\left[\phi_k, \theta_k\right]$ are the solution $M_k$, and must be
calculated exclusively using the training set $D_{k,\text{train}}$.

Once the parameters $\phi_k$ and $\theta_k$ are fixed, we apply them
in the test set $D_{k,\text{test}}$.  For each sample $(x_i, y_i) \in D_{k,\text{test}}$,
we calculate the prediction $\hat{y}_i = f_{\phi,\theta}(x_i)$.  The target value $y$ is
called the ground-truth or expected outcome.

Given a performance metric $R$, for each dataset $D_k$, we calculate
$$p_k = R\!\left(\left[y_i : i\right], \left[\hat{y}_i : i\right]\right)\text{.}$$
Note that, by definition, $p_k$ is free of \gls{leakage}, as $\left[\phi_k,
\theta_k\right]$ are found without the use of the data in $D_{k,\text{test}}$ and to
calculate $\hat{y}_i$ we use only $x_i$ (with no target $y_i$).

For a detailed explanation of this process for each sampling, consult
\cref{sec:evaluation}.
A summary of the experimental plan for estimating expected performance is shown in
\cref{fig:plan-single}.

\begin{figurebox}[label=fig:plan-single]{Experimental plan for estimating expected performance of a solution.}
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tikzpicture}
    \node [darkcircle] (data) at (0, 0) {Data};
    \node [block] (sampling) at (0, -2) {Sampling strategy};
    \path [line] (data) -- (sampling);

    \foreach \i in {1, 2, 4, 5} {
      \draw [dashed] (-7 + 2 * \i, -4.5) rectangle (-5.1 + 2 * \i, -3.5);
      \path [line] (sampling) -- (-6.1 + 2 * \i, -3.5);

      \node [smalldarkblock] (train\i) at (-6.4 + 2 * \i, -4) {Training};
      \node [smallblock] (test\i) at (-5.6 + 2 * \i, -4) {Test};

      \path [line] (-6.1 + 2 * \i, -4.5) -- (-6.1 + 2 * \i, -5.5);
    }
    \node [anchor=center] at (0, -4) {\dots};

    \draw [dashed] (-5, -5.5) rectangle (4.9, -10.5);

    \node [smalldarkblock, font=\small, inner sep=4pt] (train) at (-4, -7) {Training};
    \node [smallblock, inner sep=4pt] (test) at (-4, -9) {Test (no target)};

    \draw [dashed] (-3, -6) rectangle (3, -8);
    \node [anchor=south] at (0, -6.1) {Solution search algorithm};

    \node [block] (handling) at (-1.5, -7) {Data handling pipeline};
    \node [block] (learning) at (1.5, -7) {Machine learning};
    \node (model) at (4, -7) {%
      % bracket array with \theta and \phi
      $\left[
      \begin{array}{c}
        \phi \\
        \theta \\
      \end{array}
      \right]$
    };

    \path [line] (train) -- (handling);
    \path [line] (handling) -- (learning);
    \path [line, dashed] (3, -7) -- (model);

    \node [block] (preprocess) at (-1.5, -9) {Preprocessor};
    \node [block] (prediction) at (1.5, -9) {Model};

    \path [line, dashed] (handling) -- (preprocess);
    \path [line, dashed] (learning) -- (prediction);

    \path [line] (test) -- (preprocess);
    \path [line] (preprocess) -- (prediction);

    \node [smallblock, inner sep=4pt] (predicted) at (4, -9) {predictions};
    \node (performance) at (4, -10) {$p$};
    \path [line] (prediction) -- (predicted);

    \node [smallblock, inner sep=4pt] (labels) at (-4, -10) {Test (target)};
    \path [line] (labels) -- (performance);
    \path [line] (predicted) -- (performance);

    \node (perfs) at (-4.2, -12) {%
      $\left[
        \begin{array}{c}
          p_1 \\
          p_2 \\
          \vdots \\
        \end{array}
      \right]$
    };

    \node [block] (hypothesis) at (-1, -12) {Hypothesis test};

    \path [line, dashed] (-4.2, -10.5) -- (perfs);
    \path [line] (perfs) -- (hypothesis);
  \end{tikzpicture}
  }
  \tcblower
  % A brief description of the experimental plan.
  The experimental plan for estimating the expected performance of a solution involves
  sampling the data, training and testing the solution, evaluating the performance, and
  validating the results.
\end{figurebox}

Finally, we can study the sampled performance values $p_1, p_2, \ldots$ like any other
statistical data to prove (or disprove) the hypothesis.  This process is called
validation.

\begin{defbox}{Validation}{validation}
  While we call evaluation the process of assessing the performance of a solution using a
  test set; validation, on the other hand, is the process of interpreting or confirming
  the meaning of the evaluation results.  Validation is the process of determining the
  degree to which the evaluation results support the intended use of the solution (unseen
  data).
\end{defbox}

The results are not the ``real'' performance of the solution
$M$ in the real world, as that would require new data to be collected.  However, we can
safely interpret the performance samples as being sampled from the same distribution as
the real-world performance of the solution $M$.

% TODO: Visualization of results
% Talk about summary statistics, visualization (boxplot, roc and det curves), and Bayesian
% analysis.

\subsection{Estimating expected performance}
\label{sub:expected-performance}

We have seen that we need a process of interpreting or confirming the meaning of the
evaluation results.
Sometimes, it is as simple as calculating the mean and standard deviation of the
performance samples.  Other times, we need to use more sophisticated techniques, like
hypothesis tests or Bayesian analysis.

Let us say our goal is to reach a certain performance threshold $p_0$.  After an
experiment done with $10$ repeated $10$-fold cross-validation, we have the average
performance $\bar{p}$ and the standard deviation $\sigma$.  If $\bar{p} - \sigma \gg
p_0$, it is very likely that the solution will reach the threshold in production.
Although this is not a formal validation, it is a good and likely indication.

Also, it is common to use visualization techniques to analyze the results.  Box plots are
a good way to see the distribution of the performance samples.

A more sophisticated technique is to use Bayesian analysis.  In this case, we use the
performance samples to estimate the probability distribution of the performance of the
algorithm.  This distribution can be used to calculate the probability of the performance
being better than a certain threshold.

\textcite{Benavoli2017}\footfullcite{Benavoli2017} propose an interesting Bayesian test that accounts for the
overlapping training sets in the cross-validation\footnote{%
This is actually a particular case of the proposal in the paper, where the authors
consider the comparison between two performance vectors --- which is the case described in
\cref{sub:comparison}.}.
Let $z_k = p_k - p^{*}$ be the
difference between the performance of the $k$-th fold and the performance goal $p^{*}$,
a generative model for the data is
\begin{equation*}
  \vec{z} = \vec{1}\mu + \vec{v}\text{,}
\end{equation*}
where $\vec{z} = (z_1, z_2, \ldots, z_n)$ is the vector of performance gains, $\vec{1}$ is a
vector of ones, $\mu$ is the parameter of interest (the mean performance gain), and
$\vec{v} \sim \operatorname{MVN}(0, \Sigma)$ is a multivariate normal noise with zero mean
and covariance matrix $\Sigma$.  The covariance matrix $\Sigma$ is characterized as
\begin{equation*}
  \Sigma_{ii} = \sigma^2\text{,}\quad
  \Sigma_{ij} = \sigma^2\rho\text{,}
\end{equation*}
for all $i \neq j \in \{1, 2, \ldots, n\}$, where $\rho$ is the correlation (between folds)
and $\sigma^2$ is the variance.  The likelihood model of the data is
\begin{equation*}
  \Prob(\vec{z} \mid \mu, \Sigma) =
    \exp\left(-\frac{1}{2}(\vec{z} - \vec{1}\mu)^T \Sigma^{-1} (\vec{z} - \vec{1}\mu)\right)
    \frac{1}{(2\pi)^{n/2} \sqrt{\lvert \Sigma \rvert}}\text{.}
\end{equation*}
According to them, such likelihood does not allow to estimate the correlation from data,
as the maximum likelihood estimate of $\rho$ is zero regardless of the observations.
Since $\rho$ is not identifiable, the authors suggest using the heuristic where $\rho$ is
the ratio between the number of folds and the total number of performance samples.

To estimate the probability of the performance of the solution being greater than the
threshold, we first estimate the parameters $\mu$ and $\nu = \sigma^{-2}$ of the
generative model.  \citeauthor{Benavoli2017} consider the prior
\begin{equation*}
  \Prob(\mu, \nu \mid \mu_0, \kappa_0, a, b) = \operatorname{NG}(\mu, \nu; \mu_0, \kappa_0, a, b)\text{,}
\end{equation*}
which is a Normal-Gamma distribution with parameters $(\mu_0, \kappa_0, a, b)$.  This is a
conjugate prior to the likelihood model.  Choosing the prior parameters $\mu_0 = 0$,
$\kappa_0 \to \infty$, $a = -1/2$, and $b = 0$, the posterior distribution of $\mu$ is a
location-scale Student distribution.  Mathematically, we have
\begin{equation*}
  \Prob(\mu \mid \vec{z}, \mu_0, \kappa_0, a, b) =
    \operatorname{St}(\mu; n - 1, \bar{z}, \left(
      \frac{1}{n} + \frac{\rho}{1 - \rho}
    \right)s^2)\text{,}
\end{equation*}
where
\begin{equation*}
  \bar{z} = \frac{1}{n} \sum_{i=1}^n z_i\text{,}
\end{equation*}
and
\begin{equation*}
  s^2 = \frac{1}{n - 1} \sum_{i=1}^{n-1} (z_i - \bar{z})^2\text{.}
\end{equation*}

Thus, validating that the solution obtained by the algorithm in production will surpass
the threshold $p^{*}$ consists of calculating the probability
\begin{equation*}
  \Prob(\mu > 0 \mid \vec{z}) > \gamma\text{,}
\end{equation*}
where $\gamma$ is the confidence level.

Note that the Bayesian analysis is a more sophisticated technique than null hypothesis
significance testing, as it allows us to estimate the probability of the hypothesis
instead of the probability of observing the data given the hypothesis.
\textcite{Benavoli2017}\footfullcite{Benavoli2017} thoroughly discuss the subject.

Also, be aware that the choice of the model and the prior distribution can affect the
results.  \citeauthor{Benavoli2017} suggest using 10 repetitions of 10-fold cross-validation
to estimate the parameters of the generative model.  They also show experimental evidence
that their procedure is robust to the choice of the prior distribution.  However, one
should be aware of the limitations of the model.

\subsection{Comparing strategies}
\label{sub:comparison}

When we have two or more strategies to solve a problem, we need to compare them to see
which one is better.  This is a common situation in data science projects, as we usually
have many techniques to solve a problem.

One way to look at this problem is to consider that the algorithm\footnote{That includes
both data preprocessing and machine learning.} $A$ has \emph{hyperparameters} $\lambda \in
\Lambda$.  A hyperparameter here is a parameter that is not learned by the algorithm, but
is set by the user.  For example, the number of neighbors in a k-NN algorithm is a
hyperparameter.  For the sake of generality, we can consider that the hyperparameters may
also include different learning algorithms or data handling pipelines.

Let us say we have a baseline algorithm $A(\lambda_0)$ --- for instance, something that is
in production, the result of the last sprint or a well-known algorithm --- and a new candidate algorithm $A(\lambda)$.
Suppose $\vec{p}(\lambda_0)$ and $\vec{p}(\lambda)$ are the performance vectors of the
baseline and the candidate algorithms, respectively, that are calculated using the same
strategy described in \cref{sub:expected-performance}.  Note that the
same samplings must be used to compare the algorithms --- i.e., performance samples must be
paired, each one of them coming from the same sampling, and consequently, from the same
training and test datasets.

We can validate whether the
candidate is better than the baseline by
\begin{equation*}
  \Prob(\mu > 0 \mid \vec{z}) > \gamma\text{,}
\end{equation*}
where $\vec{z}$ is now $\vec{p}(\lambda) - \vec{p}(\lambda_0)$.  The interpretation of the
results is similar; $\gamma$ is the chosen confidence level and $\mu$ is the expected
performance gain of the candidate algorithm --- or the performance loss, if negative.

This strategy can be applied iteratively to compare many algorithms.  For example, we can
compare $A(\lambda_1)$ with $A(\lambda_0)$, $A(\lambda_2)$ with $A(\lambda_1)$, and so on,
keeping the best algorithm found so far as the baseline. In the cases where the confidence
level is not reached, but the expected performance gain is positive, we can consider
additional characteristics of the algorithms, like the interpretability of the model, the
computational cost, or the ease of implementation, to decide which one is better. However,
one should pay attention to whether the probability
\begin{equation*}
  \Prob(\mu < 0 \mid \vec{z})
\end{equation*}
is too high or not.  Always ask yourself if the risk of performance loss is worth it in
the real-world scenario.

\subsection{About nesting experiments}

Mathematically speaking, there is no difference between assessing the choice of
$\left[\phi, \theta\right]$ and the choice of $\lambda$.  Thus, some techniques --- like
grid search --- can be used to find the best hyperparameters using a nested experimental
plan.

The idea is the same: we assess how good the expected choice of the
hyperparameter-optimization technique $B$ is to find the appropriate hyperparameters.  Similarly,
the choice of the hyperparameters and the parameters that go to production is the
application of $B$ to the whole dataset.  However, never use the choices of the
hyperparameters in the experimental plan to make decisions about what goes to production.
(The same is true for the parameters $\left[\phi, \theta\right]$ in the traditional case.)

Although nesting experiments usually lead to a general understanding of the performance of
the solution, it is not always the best choice.  Nested experiments are computationally
expensive, as the possible combinations are multiplied.  Also, the size of the dataset
in the inner experiment is smaller, which can lead to a less reliable estimate of the
performance.

Nonetheless, we can always unnest the search by taking the options as different
algorithms two by two, like we described in \cref{sub:comparison}.  This solves the
problem of the size of the dataset in the inner experiment, but it does not solve the
problem of the computational cost --- often increasing it.

\section{Final remarks}

In this chapter, we presented a framework for experimental planning that can be used in
most data science projects for inductive tasks.  One major limitation of the framework is
that it assumes that the data is i.i.d.  This is not always the case, as the data can be
dependent on time or space.  In these cases, the sampling strategy must be adjusted to
account for the dependencies.

Unfortunately, changing the sampling strategy also means that the validation method must
be adjusted.  That is why tasks like time-series forecasting and spatial data analysis
require a different approach to experimental planning.

% vim: spell spelllang=en
