\chapter{Data handling}
\label{chap:handling}
\glsresetall

\chapterprecishere{%
  $\dagger$ It's dangerous to go alone! Take this.
  \par\raggedleft--- \textup{Unnamed Old Man}, The Legend of Zelda}

In the previous chapter, I discussed the relationship between data format and data
semantics.  We also saw in \cref{chap:project} that data tasks --- specifically
integration and tidying --- must adjust the available data to reflect the kind of
input we expect in production.  Data handling consists of operating on this data.

For those tasks, we must be careful with the operations we perform on the data. At the
stage of data preparation, for example, we should never parametrize our data handling
pipeline in terms of information retrieved\footnote{For instance, imputation by the mean
of a column.} by the values of the data.  This is because such operations lead to \gls{leakage}
during evaluation and other biases in our conclusions.

In this chapter, we consider that tables are rectangular data structures in which values
of the same column share the same properties (i.e. the same type, same restrictions, etc.)
and each column has a name.  Moreover, we assume that any value is possibly
\emph{missing}.

From a mathematical definition of such tables, we can define a set of operations that can
be applied to them.  These operations are the building blocks of data handling pipelines:
combinations of operations that transform a dataset into another dataset.

Finally, I highlight some important properties of these operations.  Especially, the
split-invariance property, which ensures that the operations do not add bias to the data
due to the way the data was collected.

\begin{mainbox}{Chapter remarks}

  \boxsubtitle{Contents}

  \startcontents[chapters]
  \printcontents[chapters]{}{1}{}
  \vspace{1em}

  \boxsubtitle{Context}

  \begin{itemize}
    \itemsep0em
    \item Data handling consists of operating on tables.
    \item Properties of the operations are important to avoid bias.
    \item Data handling pipelines are a way to organize these operations.
  \end{itemize}

  \boxsubtitle{Objectives}

  \begin{itemize}
    \itemsep0em
    \item Define a formal structure for tables.
    \item Define a set of operations that can be applied to tables.
  \end{itemize}

  \boxsubtitle{Takeaways}

  \begin{itemize}
    \itemsep0em
    \item Split-invariant operations avoid sampling bias.
    \item One must understand the properties and premises of the operations.
  \end{itemize}
\end{mainbox}

{}
\clearpage

\section{Formal structured data}
\label{sec:formal-structured-data}

\newcommand{\domainof}[1]{\mathcal{D}\!\left(#1\right)}
\newcommand{\missing}{\text{?}}
\newcommand{\rowcard}[1][k_1, \dots, k_k]{\operatorname{card}\!\left(#1\right)}

In this section, I present a formal definition of structured data.  This definition is
compatible with the relational model and tidy data presented in \cref{chap:data}.
My definition takes into account the index\footnote{Also called grouping variables.} of
the table, which is a key concept in data handling.  We also consider that values can be
missing.  Repeated rows are represented by allowing cells to contain sets of values.
In this chapter, we consider dataset and table as synonyms.

\begin{defbox}{Indexed table}{itable}
An indexed table $T$ is a tuple $(K, H, c)$, where $K = \left\{K_i : i = 1, \dots,
k\right\}$ is the set of index columns, $H$ is the set of (non-index) columns, and $c :
\domainof{K_1} \times \dots \times \domainof{K_k} \times H \to \mathcal{V}$ is the cell function.
Here, $\mathcal{V}$ represents the space of all possible tuples of values, which
may include missing values $\missing$.  Values have arbitrary types, such as integers,
real numbers, strings, etc.
Each index column $K_i$ has a domain $\domainof{K_i}$, which is an enumerable set of
values.
\end{defbox}

A possible row $r$ of the table is indexed by a tuple $r = (k_1, \dots,
k_k)$, where $k_i \in \domainof{K_i}$.  Each row has a cardinality $\rowcard[r]$, which
represents how many times the entity represented by the row is present in the table.
A row $r$ with $\rowcard[r] = 0$ is considered to be missing.

A cell is then represented by a row $r$ and a column $h \in H$.  The value of the cell,
$\vec{v} = c(r, h)$ is a tuple of values in the domain $\domainof{h} \cup \{\missing\}$,
such that $|\vec{v}| = \rowcard[r]$.  We say that $\domainof{h}$ is the valid domain of
the column $h$.
The order of the elements in the tuple $\vec{v}$ is arbitrary but fixed.

\begin{defbox}{Nested row}{nested-row}
A \emph{nested
row} consists of a tuple of values that associates different columns with the same
repetition of the entity, i.e. \[
  \Big[ v^h_i : h \in H \Big]\text{,}
\]
where $c(r, h) = [v^h_i : i = 1, \dots, \rowcard[r]]$, assuming an arbitrary fixed order
of the columns $h$.
\end{defbox}

We can stack nested rows to form a matrix of values.  This matrix is called the value
matrix of the row $r$.

\begin{defbox}{Value matrix}{vmatrix}
The value matrix $V = (v_{i, j})$ of the row $r$ is \[
  \Big[ c(r, h) : h \in H \Big]\text{,}
\] with dimensions $\rowcard[r] \times |H|$.
\end{defbox}

We assume that value matrices --- and consequently row cardinalities --- are minimal. This
means that there are no nested rows $$v_{i, 1}, \dots, v_{i, |H|}$$ in the value matrices
such that $v_{i, j} = \missing$ for all $j$.

From these concepts, we can define the basic operations and properties that can be applied
to tables.

\subsection{Splitting and binding}

Split and bind are very basic operations that can be applied to tables.  They are
inverses of each other and are used to divide and combine tables, respectively.
They are important in the data science process because they play a key role in
data semantics and validation of solutions.

\begin{defbox}{Split operation}{split}
Given an indicator function $s : \domainof{K_1} \times \dots \times \domainof{K_k} \to
\left\{0, 1\right\}$, the split operation creates two tables, $T_0$ and $T_1$, that
contain only the rows for which
$s(r) = 0$ and $s(r) = 1$, respectively.

Mathematically, the split operation is defined as \[
  \operatorname{split}(T, s) = \left(T_0, T_1\right)\text{,}
\] where $T = (K, H, c)$, $T_i = (K, H, c_i)$, and \[
  c_i(r, h) = \begin{dcases}
    c(r, h) & \text{if } s(r) = i \\
    () & \text{otherwise.}
  \end{dcases}
\]
\end{defbox}

\emph{Note that, by definition, the split operation never ``breaks'' a row.  So, the
indices define the indivisible entities of the table.}  The resulting tables are
disjoint:

\begin{defbox}{Disjoint tables}{disjoint-tables}
  Two tables $T_0 = (K, H, c_0)$ and $T_1 = (K, H, c_1)$ are said to be disjoint if
  $$\rowcard[r; c_0] > 0 \Rightarrow \rowcard[r; c_1] = 0\text{ and}$$
  $$\rowcard[r; c_1] > 0 \Rightarrow \rowcard[r; c_0] = 0$$ for any $r$.
\end{defbox}

The binding operation is the inverse of the split operation.

\begin{defbox}{Bind operation}{bind}
  Given two disjoint tables
  $T_0 = (K, H, c_0)$ and $T_1 = (K, H, c_1)$, the binding operation creates a new table $T$
  that contains all the rows of $T_0$ and $T_1$.

  Mathematically, the binding operation is defined as \[
    \operatorname{bind}(T_0, T_1) = (K, H, c)\text{,}
  \] where $T_i = (K, H, c_i)$ and \[ c(r, h) = c_0(r, h) + c_1(r, h)\text{.} \]
  The operator $+$ stands for the tuple concatenation operator%
  \footnote{The order of the concatenation here is not an issue since we guarantee
  that at least one of the operands is empty.}.
\end{defbox}

\emph{Thus, a requirement for the binding operation is that the tables are disjoint in
terms of the row entities they have.}

\paragraph{Premises in real-world applications}

One important aspect of these functions is that we assume that the entities represented by
the rows are indivisible, and that any binding operation will never occur for tables that
share the same entities.

In real-world applications, this is not always true.  Many times, we do not know the
process someone else has used to collect the data.  In these cases, we must be careful
about the guarantees we discuss in this chapter.  On the other hand, one can consider the
premises we use as a guideline to design good data collection processes.

We can see data collection as the result of a splitting operation in the universe set of
all possible entities.  This is a good way to think about data collection, as we can try
to ensure that we collect all possible information about the entities we are interested
in.

This, of course, depends on what we define as the index columns of the table.  Consider
the example of collecting information about grades of students.  If we define the student's
name and year as the indexes, we must ensure that we collect all the grades of all
subjects a student has taken in a year.  We do not need, though, to collect information
from all students or all years.  On the other hand, if we define only the student's name
as the index column, we must collect all the grades of all subjects a student has taken
in all years.

In summary, the fewer variables we define as index columns, the more information we must
collect about each entity.  However, in the next sections, we show that assuming many index columns
leads to restrictions in the operations we can perform on the table.

This conceptual trade-off is important to understand when structuring the problem we are
trying to solve.  Neglecting these issues can lead to strong statistical biases and
incorrect conclusions.

\subsection{Split invariance}

One property we can study about data handling operations is whether they are distributive
over the bind operation.  This property is called \emph{split invariance}.

From now on, we will denote \[
  T_0 + T_1 = \operatorname{bind}(T_0, T_1)\text{,}
\] for any tables $T_0$ and $T_1$ to simplify the notation.

\begin{defbox}{Split invariance}{split-invariance}
An arbitrary data handling operation $f(T)$ is said to be split-invariant
if, for any table $T$ and split function $s$, the following equation holds \[
  f\!\left(T_0 + T_1\right) =
    f\!\left(T_0\right) + f\!\left(T_1\right)\text{,}
\] where $T_0, T_1 = \operatorname{split}\!\left(T; s\right)$.
\end{defbox}

Split invariance is a desirable property for data handling operations during the data
tasks described in \cref{chap:project}: integration and tidying.  Even while exploring
data, we should take effort to use split-invariant operations.

The reason is that split invariance ensures that the operation does not depend on the
split performed (usually unknown to us) to create the table we have in hand.  This
property is important to avoid \gls{leakage} or to bias the results of the
analysis\footnote{Note that split invariance is a sufficient but not necessary condition
for preventing leakage.  Split invariance provides not only a ``safe by default''
guarantee, but also a way to pinpoint possible sources of leakage.}.

\subsection{Illustrative example}

\begin{tablebox}[label=tab:grades1]{Data table of student grades.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{cccc}
    \toprule
    \textbf{student} & \textbf{subject} & \textbf{year} & \textbf{grade} \\
    \midrule
    Alice & Chemistry & 2020 & 6 \\
    Alice & Math & 2019 & 8 \\
    Alice & Physics & 2019 & 7 \\
    Bob & Chemistry & 2018 & ? \\
    Bob & Chemistry & 2019 & 7 \\
    Bob & Math & 2019 & 9 \\
    Bob & Physics & 2019 & 4 \\
    Bob & Physics & 2020 & 8 \\
    Carol & Biology & 2020 & 8 \\
    Carol & Chemistry & 2020 & 3 \\
    Carol & Math & 2020 & 10 \\
    \bottomrule
  \end{tabular}
  \tcblower
  Data collected about student grades.  All information that is available is presented.
\end{tablebox}

Consider the example of data collected about student grades.  \Cref{tab:grades1}
exemplifies all information we can possibly have about the grades of students.  A missing
value in a cell of that table indicates that, for some reason, the information is not
retrievable.

The domains of the variables are:
\begin{itemize}
  \itemsep0em
  \item $\domainof{\text{student}} = \left\{\text{Alice}, \text{Bob}, \text{Carol}\right\}$;
  \item $\domainof{\text{subject}} = \left\{\text{Biology}, \text{Chemistry}, \text{Math},
    \text{Physics}\right\}$;
  \item $\domainof{\text{year}} = \mathbb{Z}$; and
  \item $\domainof{\text{grade}} = \left[0, 10\right]$.
\end{itemize}

Of course, in practice, we have no guarantee that the data we have is complete nor a
clear specification of the domain of the variables.  Instead, we must choose good
premises about the data we are working with.

Knowing that the data is complete, we can safely assume that:
\begin{enumerate}
  \itemsep0em
  \item Alice has never taken Biology;
  \item Bob passed Physics, although at the second attempt;
  \item Carol has only taken classes in 2020.
\end{enumerate}

\begin{tablebox}[label=tab:grades2]{Data table of student grades assuming student and subject as indices.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{ccccc}
    \toprule
    \textbf{s} & \textbf{student} & \textbf{subject} & \textbf{year} & \textbf{grade} \\
    \midrule
    0 & Alice & Chemistry & (2020) & (6) \\
    1 & Alice & Math & (2019) & (8) \\
    1 & Alice & Physics & (2019) & (7) \\
    0 & Bob & Chemistry & (2018, 2019) & (?, 7) \\
    0 & Bob & Math & (2019) & (9) \\
    1 & Bob & Physics & (2019, 2020) & (4, 8) \\
    0 & Carol & Biology & (2020) & (8) \\
    0 & Carol & Chemistry & (2020) & (3) \\
    1 & Carol & Math & (2020) & (10) \\
    \bottomrule
  \end{tabular}
  \tcblower
  Indexed table with data from \cref{tab:grades1} assuming student and
  subject as indices.  The column $s$ is the split indicator.
\end{tablebox}

Now consider an arbitrary collection mechanism that considers student and subject as the
indices of the table.  \Cref{tab:grades2} shows the table we have in hand.  The column $s$
is the split indicator.  Only rows with $s = 1$ are available to us.

Now, about the statements we made before:
\begin{enumerate}
  \itemsep0em
  \item There is no way we can know if Alice has taken Biology or not.  It could be that
    the data collection mechanism failed to collect this information or that the
    information simply does not exist.
  \item We can safely assume that Bob has passed Physics in his second attempt, once all
    information about (Bob, Physics) is assumed to be available.
  \item There is no guarantee that Carol has only taken classes in 2020.  It could be that
    some row (Carol, subject) with a year different from 2020 is missing in the table.
\end{enumerate}

\begin{tablebox}[label=tab:grades3]{Data table of student grades assuming student as the index.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{ccp{2.6cm}p{1.8cm}>{\raggedright\arraybackslash}p{1.2cm}}
    \toprule
    \textbf{s} & \textbf{student} & \textbf{subject} & \textbf{year} & \textbf{grade} \\
    \midrule
    1 & Alice & (Chemistry, Math, Physics) & (2020, 2019, 2019) & (6, 8, 7) \\
    0 & Bob & (Chemistry, Chemistry, Math, Physics, Physics) & (2018, 2019, 2019, 2019, 2020) & (?, 7, 9, 4, 8) \\
    1 & Carol & (Biology, Chemistry, Math) & (2020, 2020, 2020) & (8, 3, 10) \\
    \bottomrule
  \end{tabular}
  \tcblower
  Indexed table with data from \cref{tab:grades1} assuming student
  as index.  The column $s$ is the split indicator and only rows with $s = 1$ are available to us.
\end{tablebox}

Now consider an arbitrary collection mechanism that considers student as the index of the table.
Imposing this restriction would difficult the data collection process, but it would
guarantee that we have all information about each student.  \Cref{tab:grades3} shows the
table we have in hand.  As before, the column $s$ is the split indicator and only rows with
$s = 1$ are available to us.

Our conclusions may change again:
\begin{enumerate}
  \itemsep0em
  \item We can safely assume that Alice has never taken the Biology class, as $\text{Biology}
    \not\in c(\text{Alice}, \text{subject})$.
  \item There is no information about Bob's grades, so we can not affirm nor deny anything
    about his grades.
  \item We can safely assume that Carol has only taken classes in 2020, as $c(\text{Carol},
    \text{year})$ contains only values with 2020.
\end{enumerate}

It is straightforward to see that the fewer index columns we have, the more information we
have about the present entities.  Also, it is clear how important the assumptions on the
index columns are to the conclusions we can draw from the data. Consequently,
split-invariant operations can preserve valid conclusions about the data even when
information is missing\footnote{Absence can be due to incomplete data collection or
artificial splitting for validation; consult \cref{chap:planning}.}.

\section{Data handling pipelines}

In the literature and in software documentation, you will find a variety of terms used to
describe data handling operations\footnote{%
  The terminology ``data handling'' itself is not universal.  Some authors and libraries
  call it ``data manipulation'', ``data wrangling'', ``data shaping'', or ``data
  engineering''.  I use the term ``data handling'' because it seems more generic.
  Also, it avoids confusion with the term ``data
  manipulation'' which has a negative connotation in some contexts.}. %
They often refer to the same or similar operations, but the terminology can be confusing.
In this section, I present a summary of these operations mostly based on
\textcite{Wickham2023} definitions\footnote{Which they call \emph{verbs}.}.


During the preparation of data for our project, we will need to perform a set of operations
on possibly multiple datasets.  These operations are organized in a pipeline, where the
outputs of one operation are the inputs of the next one.
Operations are extensively parameterized, for instance, most of them can use predicates to
define the groups, arrangements, or conditions under which they should be applied.

\begin{figurebox}[label=fig:pipeline]{Example of data handling pipeline.}
  \centering
  \begin{tikzpicture}[every node/.style={font=\small, inner sep=4pt}]
    \node (s1) [darkcircle] at (0, 0) {Source 1};
    \node (s2) [darkcircle] at (0, -2) {Source 2};
    \node (f1) [mediumblock] at (2, 0) {$f_1$};
    \node (f2) [mediumblock] at (4, 0) {$f_2$};
    \node (f3) [mediumblock] at (2, -2) {$f_3$};
    \node (f4) [mediumblock] at (4, -2) {$f_4$};
    \node (f5) [mediumblock] at (6, -1) {$f_5$};
    \node (data) [darkcircle, minimum width=15mm] (data) at (8, -1) {Data};

    \path [line] (s1) -- (f1);
    \path [line] (f1) -- (f2);
    \path [line] (f1.east) -- (f4);
    \path [line] (s2) -- (f3);
    \path [line] (f3) -- (f4);
    \path [line] (f2) -- (f5);
    \path [line] (f4) -- (f5);
    \path [line] (f5) -- (data);
  \end{tikzpicture}
  \tcblower
  A data handling pipeline is a set of operations that transform a dataset into
  another dataset.  We can have more than one source dataset and the output is a single
  dataset where each row represents a sample in the observational unit we are interested
  in.
\end{figurebox}

In \cref{fig:pipeline}, we show an example of a data handling pipeline.  The pipeline
starts with two source datasets, Source 1 and Source 2.  The datasets are processed by a
set of operations, $f_1, f_2, f_3, f_4, f_5$, and the output is a single dataset,
Data.  Our goal at the data tasks --- see \cref{sub:workflow} --- is to create a dataset
that is representative of the observational unit we are interested in.  Representative
here means that the dataset is tidy\footnote{Remember that our definition of tidiness
depends on the observational unit.  That means, in practice, that if the original data
sources are in a observational unit different from the one we are interested in, after
joining them, the connecting variables may need to be removed to eliminate transitive
dependencies.  Consult \cref{sub:tidy-not-tidy,sub:change-unit}.} and that the priors,
i.e. the distribution of the data is faithful to the real distribution of the phenomenon.

A pipeline is more flexible than a chain of operations because it can handle more complex
structures, where different branches (forks) of processing occur simultaneously, and then
come together (merges) later in the workflow.  For instance, the output of $f_1$ is the
input of $f_2$ and $f_4$ (fork), and $f_5$ has as input the outputs of $f_2$ and $f_4$
(merge).

Pipelines are great conceptual tools to organize the data handling process.  They allow
for the separation of concerns, where each operation is responsible for a single task.
Also, declaring the whole pipeline at once allows for the optimization of the operations
and the use of parallel processing.  This is important when dealing with large datasets.
The declarative approach, as opposed to the imperative one, makes it easier to reason about
and maintain the code\footnote{Tidyverse and Polars are examples of
libraries that use a declarative approach to data handling.}.

\section{Split-invariant operations}
\label{sec:split-invariant-ops}

In this section, I present a set of operations that are split-invariant.  One can safely
apply these operations to the data without worrying about biasing the dataset.

For each operation, we discuss its application on some tidying issues presented in
\cref{sub:messy}.  The issues I address here\footnote{%
The issue of multiple types of observational units stored in the same table is better
dealt with by database normalization.  More on this subject is discussed in
\cref{sub:projection}.}:
\begin{itemize}
  \itemsep0em
  \item Headers are values, not variable names;
  \item Multiple variables are stored in one column; % [mutate/select problem]
  \item Variables are stored in both rows and columns;
  \item A single observational unit is stored in multiple tables.
\end{itemize}



\subsection{Tagged splitting and binding}

We saw that one trivial, yet important, operation is to bind datasets.  This is the
process of combining two or more datasets into a single dataset.  To make the
operation reversible, we can parametrize it with a split column that indicates the source
of each row.

\begin{defbox}{Tagged bind operation}{tagged-bind}
  Given two or more disjoint tables $T_i = (K, H, c_i)$, $i = 0, 1, \dots$, the tagged
  bind operation creates a new table $T = (K, H \cup \{s\}, c)$ that contains all the rows
  of tables $T_i$.  The split column $s$ is a new column that indicates the source of each
  row.

  Mathematically, the tagged bind operation is defined as \[
    \operatorname{bind}_{s}(T_0, T_1, \dots) = T\text{,}
  \] where $c(r, h) = c_0(r, h) + c_1(r, h) + \dots$ if $h \in H$ and \[
    c(r, s) = \left[ i \right]^{d} \text{,}
  \]
  where $i$ is the index of the table $T_i$ that contains the row $r$, i.e. $d =
  \rowcard[r; c_i] > 0$.
\end{defbox}

When binding datasets by rows, the datasets must have the same columns.  In practice,
one can assume, if a column is missing, that all values in that column are missing.

The indication of the source table usually captures some hidden semantics that has split
the tables in the first place. For instance, if each table represents data collected in a
different year, one can create a new column \emph{year} that contains the year of the
data.  It is important to pay attention to the semantics of the split column, as it can
also contain extra information.

\begin{tablebox}[label=tab:gas-usage]{Gas usage datasets.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{ccc}
    \toprule
    \textbf{month} & \textbf{gas} & \textbf{distance} \\
    \midrule
    1 & 48.7 & 1170 \\
    2 & 36.7 & 1100 \\
    3 & 37.8 & 970 \\
    \dots & \dots & \dots \\
    \bottomrule
  \end{tabular}
  ~
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{ccc}
    \toprule
    \textbf{month} & \textbf{gas} & \textbf{distance} \\
    \midrule
    1 & 143.7 & 1470 \\
    2 & 156.7 & 1700 \\
    3 & 170.8 & 1870 \\
    \dots & \dots & \dots \\
    \bottomrule
  \end{tabular}
  \tcblower
  Monthly gas usage data from US (left) and Brazil (right) residents.
\end{tablebox}

Consider \cref{tab:gas-usage}, which contains monthly gas usage data from
US and Brazil residents.  From the requirements described in the previous section, we can
safely bind these datasets --- as they are disjoint.  We can use as a tag a new column to
represent the country.  However, an attentive reader will notice that the unit of
measurement of the gas usage and distance are different in each table: gallons and miles
in the US dataset and liters and kilometers in the Brazil dataset.  Ideally, thus, we
should create two other columns to represent the units of measurement.

It is straightforward to see that this operation solves the issue of a single
observational unit being stored in multiple tables described in \cref{sub:messy}.

The reverse function consists of splitting the dataset using as a predicate the split column.

\begin{defbox}{Tagged split operation}{tagged-split}
  Let $s$ be a non-index column of a table $T = (K, H \cup \{s\}, c)$ with $\domainof{s}$
  known and finite, and such that $c(r, s)$ contains only unique values. The tagged split
  operation parametrized by $s$ creates disjoint tables $T_i = (K, H, c_i)$ that contain
  only the rows $r$ for which $c(r, s) = i$.

  Mathematically, the tagged split operation is defined as \[
    \operatorname{split}_{s}(T) = \left(T_0, T_1, \dots\right)\text{,}
  \] where $c_i(r, h) = c(r, h)$ if $i \in c(r, s)$ and $c_i(r, h) = ()$ otherwise.
\end{defbox}

Note that the tagged split is split-invariant by definition, since we assume that the
nested rows of the input table $T$ contain only one value for column $s$ for all
rows\footnote{We consider a slightly different definition of split invariance here, where
the binding operation is applied to each element of the output of the split operation.}.
Failing to meet this assumption can lead to a biased split.  Also, in practice, it is
good practice to keep the column $s$ in the output tables to preserve information about
the source of the rows.  In terms of storage, smart strategies can be used to avoid the
unnecessary repetition of the same value in column $s$.

\subsection{Pivoting}

Another important operation is pivoting datasets.  There are two types of pivoting:
long-to-wide and wide-to-long.  These operations are reversible and are the inverse
of each other.

Pivoting long-to-wide requires a name column --- whose discrete and finite
possible values will become the names of the new columns --- and a value column --- whose
values will be \emph{spread} across the rows.  Other than these columns, all remaining columns
must be indexes.

\begin{defbox}{Pivot long-to-wide operation}{pivot-l2w}
  Let $T = (K \cup \{\text{name}\}, \{\text{value}\}, c)$. The pivot long-to-wide
  operation is defined as \[
    \operatorname{pivot}_\text{name}(T) = T'\text{,}
  \] where $T' = (K, \domainof{\text{name}}, c')$ and \[
    c'(r, h) = c\left(r + (h),~\text{value}\right)\text{,}
  \]
  for all valid row $r$ and $h \in \domainof{\text{name}}$.
\end{defbox}

Note however that the operation only works if $\rowcard[r + (h); c]$ is constant for all
$h \in \domainof{\text{name}}$.  If this is not the case, one must aggregate the rows
before applying the pivot operation.  This is discussed in \cref{sub:aggregation}.

Pivoting wide-to-long\footnote{Also known as unpivot.} is the reverse operation. One must
specify all the columns whose names are the values of the previously called ``name column.''
The values of these columns will be \emph{gathered} into a new column. As before, all
remaining columns are indexes.

\begin{defbox}{Pivot wide-to-long operation}{pivot-w2l}
  Let $T = (K, H, c)$ be a table.  The pivot wide-to-long
  operation is defined as \[
    \operatorname{pivot}^{-1}(T) = T'\text{,}
  \] where $T' = (K \cup \{\text{name}\}, \{\text{value}\}, c')$, $\domainof{\text{name}}
  = H$ and \[
    c'((r, h), \text{value}) = c(r, h)\text{,}
  \] for all valid row $r$ and $h \in H$.
\end{defbox}

In practical applications, where not all remaining columns are indexes, one must aggregate
rows or drop extra non-indexed columns beforehand.  This is discussed in
\cref{sub:aggregation,sub:selection}.

\begin{tablebox}[label=tab:pivot]{Pivoting example.}
    \centering
    \rowcolors{2}{black!10!white}{}
    \begin{tabular}{ccc}
      \toprule
      \textbf{city} & \textbf{year} & \textbf{qty.} \\
      \midrule
      A & 2019 & 1 \\
      A & 2020 & 2 \\
      A & 2021 & 3 \\
      B & 2019 & 4 \\
      B & 2020 & 5 \\
      B & 2021 & 6 \\
      \bottomrule
    \end{tabular}
    ~
    \rowcolors{2}{black!10!white}{}
    \begin{tabular}{cccc}
      \toprule
      \textbf{city} & \textbf{2019} & \textbf{2020} & \textbf{2021} \\
      \midrule
      A & 1 & 2 & 3 \\
      B & 4 & 5 & 6 \\
      \bottomrule
    \end{tabular}
  \tcblower
  The left-hand-side table is in the long format and the right-hand-side table is in the
  wide format.
\end{tablebox}

\Cref{tab:pivot} shows an example of pivoting.  Here, we can consider \emph{city} and
\emph{year} as the index columns.  The left-hand-side table is in the long format and the
right-hand-side table is in the wide format.  Using the pivot long-to-wide operation with
\emph{year} as the name column and \emph{qty.} as the value column, we can obtain the
right-hand-side table.  The reverse operation will give us the left-hand-side table.

To show that the pivot operation is split-invariant, one can see that, given
$T_0 = (K, H, c_0)$ and $T_1 = (K, H, c_1)$ disjoint tables,
\[
  \operatorname{pivot}_\text{name}\!\left(T_0\right) + \operatorname{pivot}_\text{name}\!\left(T_1\right) = \\
    (K, \domainof{\text{name}}, c_0') + (K, \domainof{\text{name}}, c_1')\text{,}
\] where $c_i'(r, h) = c_i(r + (h), \text{value})$.  However, by the disjoint property of
the tables, we have that \[
  c_0(r + (h), \text{value}) + c_1(r + (h), \text{value}) = c(r + (h), \text{value})\text{,}
\] for the table $T = (K, H, c) = T_0 + T_1$. So,
\begin{multline*}
  (K, \domainof{\text{name}}, c_0') + (K, \domainof{\text{name}}, c_1') =
    (K, \domainof{\text{name}}, c') =\\
    \operatorname{pivot}_\text{name}\!\left(T\right)\text{,}
\end{multline*}
where $c'(r, h) = c(r + (h), \text{value})$.

Similarly, the reverse operation is also split-invariant.

Using the pivot operation, we can solve the issues of headers being values, not variable
names and variables being stored in both rows and columns.  In the first case, we can pivot
the table to have the headers as the domain of a new index (name column).  In the second
case, we have to pivot both long-to-wide and wide-to-long to solve the issue.

\subsection{Joining}

Joining is the process of combining two datasets into a single dataset based on common
columns.  This is one of the two fundamental operations in relational algebra. We will see
the conditions under which the operation is split invariant. However, the join operation
has some other risks you should be aware of; consult \cref{sec:normalization} for more
details.

Adapting the definitions of join in our context, we can define it as follows.
For the sake of simplicity, we denote $r[U]$ as the row $r$ restricted to the index
columns in $U$, i.e.
\begin{equation*}
  % \label{eq:row-subscript}
  r[U] = (k_i : k_i \in \domainof{K_i} \forall K_i \in U)\text{.}
\end{equation*}

\begin{defbox}{Join operation}{join}
  Let $T' = (K', H', c')$ and $T'' = (K'', H'', c'')$ be two tables such that $K'
  \cap K'' \neq \emptyset$ and $H' \cap H'' = \emptyset$.  The join operation is
  defined as \[
    \operatorname{join}(T', T'') = T\text{,}
  \] where $T = (K' \cup K'', H' \cup H'', c)$ and \[
    c(r, h) = ()
  \] if $\rowcard[{r[K']; c'}] = 0$ or $\rowcard[{r[K'']; c''}] = 0$, for all $h$.
  Otherwise: \[
    c(r, h) = \begin{dcases}
      c'(r[K'], h) & \text{if } h \in H'\text{,} \\
      c''(r[K''], h) & \text{if } h \in H''\text{.}
    \end{dcases}
  \]
\end{defbox}

The join of two tables is the operation that returns a new table with the columns of both
tables.  Let $U$ be the common set of index columns.  For each occurring value of $U$ in
the first table, the operation will look for the same value in the second table.  If it
finds it, it will create a new row with the columns of both tables.  If it does not find
it, no row will be created.

Note that, like in pivoting long-to-wide, one must ensure that the cardinality of the
joined rows is constant for all $h \in H' \cup H''$.  If this is not the case, one must
aggregate the rows before applying the join operation.  This is discussed in
\cref{sub:aggregation}.

Before we discuss whether the join operation is split-invariant\footnote{Note that up to
this point, we have defined this property only for unary operations.}, we can discuss a
variation of the join operation: the left join.  The left join is the same as the join
operation, but if the value of $U$ is missing in the second table, the operation will
create a new row with the columns of the first table and missing values for the columns of
the second table.

In our context, this operation is a unary operation, where the second table is
a fixed parameter.

\begin{defbox}{Left join operation}{left-join}
  Let $T' = (K', H', c')$ and $T'' = (K'', H'', c'')$ be two tables such that $K'
  \cap K'' \neq \emptyset$ and $H' \cap H'' = \emptyset$.  The left join operation is
  defined as \[
    \operatorname{join}(T'; T'') = \operatorname{join}_{T''}(T') = T\text{,}
  \] where $T = (K' \cup K'', H' \cup H'', c)$ and \[
    c(r, h) = ()
  \] if $\rowcard[{r[K']; c'}] = 0$ for all $h$. Otherwise:
  \[
    c(r, h) = \begin{dcases}
      c'(r[K'], h) & \text{if } h \in H'\text{,} \\
      c''(r[K''], h) & \text{if } h \in H''\text{.}
    \end{dcases}
  \]
\end{defbox}

The left join operation is split-invariant.  To see this, consider two disjoint tables
$T_0 = (K, H, c_0)$ and $T_1 = (K, H, c_1)$, and a third table $T' = (K', H', c')$ such
that $K \cap K' \neq \emptyset$ and $H \cap H' = \emptyset$.  We have that
\begin{multline*}
  \operatorname{join}_{T'}(T_0) + \operatorname{join}_{T'}(T_1) =
  T_0' + T_1' =\\
    (K \cup K', H \cup H', c_0') + (K \cup K', H \cup H', c_1')\text{,}
\end{multline*}
where the meaning of each term is clear from the \cref{def:left-join}.
It is straightforward to see that rows in $T_0'$ and $T_1'$ are disjoint, since at least
part of the indices in $K \cup K'$ are different between them.

Moreover, \[
  \operatorname{join}_{T'}(T_0 + T_1) = (K \cup K', H \cup H', c')
\] with $c'(r, h) = ()$ only if both $\rowcard[{r[K]; c_0}] = 0$ and $\rowcard[{r[K];
c_1}] = 0$.  Otherwise, $c'(r, h) = c_0(r[K], h) + c_1(r[K], h)$ if $h \in H$ and $c'(r,
h) = c_0(r[K], h)$ if $h \in H'$.

Thus, \[
  \operatorname{join}_{T'}(T_0 + T_1) = \operatorname{join}_{T'}(T_0) + \operatorname{join}_{T'}(T_1)\text{.}
\]

Our conclusion is that the left join operation given a fixed table is split-invariant.
So we can safely use it to join tables without worrying about biasing the dataset once we
fix the second table.

I conjecture that the (inner) join operation shares similar properties but it is not as
safe; nonetheless, a clear definition of split invariance for binary operations is needed.
This is left as a thought exercise for the reader.  Notice that the traditional join has
the ability to ``erase'' rows from any of the tables involved in the operation.  This is a
potential source of bias in the data.  This further emphasizes the importance of
understanding the semantics of the data schema before joining tables --- consult
\cref{sec:normalization}.

\subsection{Selecting}
\label{sub:selection}

Selecting is the process of choosing a subset of non-index columns from a dataset.  The
remaining columns are discarded.  Rows of the table remain unchanged.

Although very simple, the selection operation is useful for removing columns that are not
relevant to the analysis.  Also, it might be needed before other operations, such as
pivoting, to avoid unnecessary columns (wide-to-long) and to keep only the value column
(long-to-wide).

\begin{defbox}{Selection operation}{selection}
  Let $T = (K, H, c)$ be a table and $H' \subseteq H$.  The selection operation is
  defined as \[
    \operatorname{select}_{H'}(T) = T'\text{,}
  \] where $T' = (K, H', c)$.
\end{defbox}

Sometimes, it is useful to select columns based on a function of the column properties.
In other words, the selection operation can be parameterized by a predicate.  The predicate
is a function that returns a logical value given the column.

\begin{defbox}{Predicate selection operation}{predicate-selection}
  Let $T = (K, H, c)$ be a table and $P : H \to \{0, 1\}$ be a predicate.  The predicate
  selection operation is defined as \[
    \operatorname{select}_{P}(T) = T'\text{,}
  \] where $T' = (K, H', c)$ and $H' = \{h \in H : P(h) = 1\}$.
\end{defbox}

It is trivial to see that, if $P$ does not depend on the values of the columns (i.e., has no
access to $c$), the predicate selection operation is split-invariant.  This is because the
operation does not change the rows of the table nor does it depend on the values of the rows.

One example of the use of the predicate selection operation is to keep columns whose
values are in a specific domain.  For instance, to keep only columns that contain real
numbers, we choose $P(h) = 1$ if $\domainof{h} = \mathbb{R}$, and $P(h) = 0$ otherwise.

The case where the predicate depends on the values of the columns is discussed in
\cref{sub:grouped-arranged}.

\subsection{Filtering}
\label{sub:filtering}

Filtering is the process of selecting a subset of rows from a dataset based on a
predicate.

A predicate can be a combination of other predicates using logical operators, such as
logical disjunction (or) or logical conjunction (and).  In the general case, predicates
need to be robust enough to deal with value matrices of any size and those that contain missing
values.

After filtering, the dataset will contain only the rows that satisfy the predicate.
Columns remain unchanged.

In its simplest form, we can assume that $\rowcard[r] \leq 1$ for all $r$ and that the
predicates are applied to each row independently.  In this case, the value matrix $V(r)$
is just a tuple with a single value for each non-index column.

Without loss of generality, we can assume that predicates are combined using logical
disjunction (or)\footnote{The reason is that sequential application of filtering is
equivalent to combining the predicates using logical conjunction (and).}.

For instance, the predicate \code{age > 18} will select all rows where the value in the
age column is greater than 18.  Keeping each row independent, we can also generalize
predicates to deal with the values of the indexes as well.

\begin{defbox}{Filtering operation}{filtering}
  Let $T = (K, H, c)$ be a table and $P_1, \dots P_n$ be predicates.  The
  filtering operation is defined as \[
    \operatorname{filter}_{P_1, \dots, P_n}(T) = T'\text{,}
  \] where $T' = (K, H, c')$ and \[
    c'(r, h) = \begin{dcases}
      c(r, h) & \text{if } \bigvee_{i = 1}^{n} P_i(r, V(r)) = 1\text{,} \\
      () & \text{otherwise}\text{,}
    \end{dcases}
  \] where predicate $$P_i : \bigtimes_{K_i} K_i \times
  \bigtimes_{h\in H}\left(\domainof{h} \cup \{?\}\right) \to \{0, 1\}$$ is applied to the value
  matrix $V(r)$ of the row $r$.
\end{defbox}

It is also trivial to see that the filtering operation is split-invariant, even in its
generalized form where the value matrix has many rows.  This property comes from the fact
that rows are treated independently.  More complex cases are discussed in
\cref{sub:grouped-arranged}.

\subsection{Mutating}

Mutating is the process of creating new columns in a table.  The operation is reversible,
as the original columns are kept.  The new columns are added to the dataset.

The values in the new column are determined by a function of the rows.  The expression is
a function that returns a vector of values given the values in the other columns.
Similarly to filtering, in its simplest form, we can assume that $\rowcard[r] \leq 1$ for
all $r$ and that the predicates are applied to each row independently.

\begin{defbox}{Mutation operation}{mutating}
  Let $T = (K, H, c)$ be a table and $f$ be a transformation function.  The mutating
  operation is defined as \[
    \operatorname{mutate}_{f}(T) = T'\text{,}
  \] where $T' = (K, H \cup \{ h' \}, c')$ and \[
    c'(r, h) = \begin{dcases}
      c(r, h) & \text{if } h \in H\text{,} \\
      f(r, V(r)) & \text{if } h = h'\text{,}
    \end{dcases}
  \] where the function $$f : \bigtimes_{K_i \in K} K_i \times \bigtimes_{h\in H}
  \left(\domainof{h} \cup \{?\}\right) \to \domainof{h'} \cup \{?\}$$ is applied to the
  value matrix $V(r)$ of the row $r$.
\end{defbox}

The expression can be a simple function, such as \code{y = x + 1}, or a more complex
function, such as
\begin{center}
  \code{y = ifelse(x > 0, 1, 0)}.
\end{center}
Here, x and y are the names of an existing and the new column, respectively. The
\code{ifelse(a, b, c)} function is a conditional function that returns 1 if the condition
is true and 0 otherwise.

This function solves the issue of multiple variables stored in one column described in
\cref{sub:messy}.

As with filtering, the mutating operation is split-invariant even if $\rowcard[r] > 1$ for
any $r$\footnote{It just changes the input space of function $f$.}.  This is because the
operation is applied to each row independently.  In this general case, an extra
requirement is that the function $f$ must return tuples with the same cardinality as
the row it is applied to.

\subsection{Aggregating}
\label{sub:aggregation}

Many times, it is easier to reason about the table when all rows have cardinality 1.
Aggregation ensures that the table has this property.

\begin{defbox}{Aggregation operation}{aggregating}
  Let $T = (K, H, c)$ be a table and $f$ be an aggregation function.  The aggregation
  operation is defined as \[
    \operatorname{aggregate}_{f}(T) = T'\text{,}
  \] where $T' = (K, H, c')$ and \[
    c'(r, h) = f(r, V(r))[h]\text{,}
  \] where function $f$ is applied to the value matrix $V(r)$ of the row $r$ and it has
  an image $$\left(\domainof{h} \cup \{?\} : h \in H\right)\text{,}$$ independently of the
  input size.  The notation $v[h]$ refers to the value corresponding to the column $h$ in the
  output tuple.
\end{defbox}

As with mutation, aggregation is split-invariant as it treats each row independently,
even if the function $f$ considers order semantics of the values in the matrix.

\subsection{Ungrouping}
\label{sub:ungrouping}

We discussed that the fewer index columns a table has ---
assuming we guarantee that all information about that entity is present --- the safer
it is to infer conclusions from the data.  Thus, reducing the number of indices
must be done very carefully --- more on that in \cref{sub:projection}.

On the other hand, sometimes it is useful to increase the number of index columns.  For
instance, pivoting long-to-wide requires all columns except one to be indexes.  The
operation that transforms some of the columns in the table into indexes is called ungrouping.
The reason for the name is that the operation decreases the cardinality of rows
by creating new rows, effectively ungrouping the values.

\begin{defbox}{Ungrouping operation}{grouping}
  Let $T = (K, H, c)$ be a table and $h' \in H$ such that $\domainof{h'}$ is known and
  finite.  The ungrouping operation is defined as \[
    \operatorname{ungroup}_{h'}(T) = T'\text{,}
  \] where $T' = (K \cup \{h'\}, H \setminus \{h'\}, c')$, and \[
    c'(r + r', h) = (v_{i,h} : i)\text{,}
  \] where $r$ refers to values of the indices in $K$, $r'$ refers to the value of the new index
  $h'$, and $v_{i,h}$ is the value of the column $h$ in any $i$-th nested row of the value
  matrix $V(r; T)$ in the original table such that \[
    v_{i, h'} = r'\text{.}
  \]
\end{defbox}

Note that the operation requires that the column $h'$ has no missing values.

\begin{tablebox}[label=tab:ungroup]{Ungrouping example.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{ccc}
    \toprule
    \textbf{city} & \textbf{year} & \textbf{qty.} \\
    \midrule
    A & (2019, 2020, 2020, 2021) & (1, 2, 3, 4) \\
    B & (2019, 2020, 2021) & (5, 6, 7) \\
    \bottomrule
  \end{tabular}

  \vspace{1em}
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{ccc}
    \toprule
    \textbf{city} & \textbf{year} & \textbf{qty.} \\
    \midrule
    A & 2019 & 1 \\
    A & 2020 & (2, 3) \\
    A & 2021 & 4 \\
    B & 2019 & 5 \\
    B & 2020 & 6 \\
    B & 2021 & 7 \\
    \bottomrule
  \end{tabular}
  \tcblower
  The index of the top table is the column \emph{city}.  The bottom table is the result of
  ungrouping the column \emph{year}.
\end{tablebox}

\Cref{tab:ungroup} shows an example of ungrouping.  In the top table, there are two rows,
one with cardinality 4 and the other with cardinality 3.  The column \emph{year} is
ungrouped, creating new rows for each value in the nested row.  The bottom table is the
result of ungrouping the column \emph{year}.  Although there were 7 nested rows in the
original table, the bottom table has 6 rows --- the number of nested rows is preserved
however.  The reason is that the row (A, 2020) has cardinality 2.

The ungrouping operation is split-invariant.  To see this, consider two disjoint tables
$T_0 = (K, H, c_0)$ and $T_1 = (K, H, c_1)$, we have
\begin{multline*}
  \operatorname{ungroup}_{h'}(T_0) + \operatorname{ungroup}_{h'}(T_1) = \\
    \left(K \cup \{h'\}, H \setminus \{h'\}, c_0'\right) +
    \left(K \cup \{h'\}, H \setminus \{h'\}, c_1'\right)\text{,}
\end{multline*}
where $c_j'(r + r', h) = (v_{i,h} : i \text{ such that } v_{i, h'} = r')$. Since the
tables are disjoint, the rows of the output tables are also disjoint.  In other words,
For any $r$, either $\rowcard[{r + r'; c_0}] = 0$ or $\rowcard[{r + r'; c_1}] = 0$
independently of the value of $r'$.  The reason is that there is no possible $v_{i,h'} =
r'$ if $r$ is not present in the table.

Then,
\begin{multline*}
  \left(K \cup \{h'\}, H \setminus \{h'\}, c_0'\right) +
  \left(K \cup \{h'\}, H \setminus \{h'\}, c_1'\right) = \\
    \left(K \cup \{h'\}, H \setminus \{h'\}, c'\right)\text{,}
\end{multline*}
where $c'(r + r', h) = c_0'(r + r', h) + c_1'(r + r', h)$.

\section{Other operations}

We saw that, under reasonable premises, split-invariant operations are safe to use in the
context of tidying and data integration.  However, data handling does not happen only in
the context of tidying and integrating datasets\footnote{And, sometimes, we may need to
use other transformations even for these tasks.}.  It is also used in tasks like data
exploration and data preprocessing.  In these cases, other operations are needed.

In this section, we discuss some of these operations.  Instead of focusing on the
mathematical definitions, we will discuss the semantics of the operations and some
of their properties.

\subsection{Projecting or grouping}
\label{sub:projection}

Projection is one of the two fundamental operations in relational algebra --- consult
\cref{sec:normalization} for more details.  In database normalization theory, tables ---
called relations --- are slightly different from the tables we are discussing here. The
major difference is that they are sets of tuples, which means that each tuple is unique.
In our scenario, this is similar to what we call rows represented by the possible values
of the index columns of the table.

Adapting the definitions of projection to our context, we can define it as follows.

\begin{defbox}{Projection operation}{projection}
  Let $T = (K, H, c)$ be a table and $K' \subset K$ a subset of the columns.  The
  projection operation is defined as \[
    \operatorname{project}_{K'}(T) = T'\text{,}
  \] where $T' = (K', H \cup (K \setminus K'), c')$ and \[
    c'(r, h) = \begin{dcases}
      \sum_{r'} c(r + r', h) & \text{if } h \in H \\
      \sum_{k' \in \domainof{h}} k' & \text{if } h \in K \setminus K'\text{,}
    \end{dcases}
  \] for all valid row $r$ considering the indices $K'$ and for all tuples $r' = (k_i :
  i)$ such that $k_i \in \domainof{K_i}$ for all $K_i \in K \setminus K'$.
\end{defbox}

We can see that projection for our tables is a little more complex than the usual
projection in relational algebra.  Consider the example we discussed in
\cref{sec:normalization} as well, where we have a table with the columns \emph{student},
\emph{subject}, \emph{year}, and \emph{grade}.

\begin{tablebox}[label=tab:student-grade-handling]{Student grade table.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{cccc}
    \toprule
    \textbf{student} & \textbf{course} & \textbf{course credits} & \textbf{grade} \\
    \midrule
    Alice & Math & 4 & A \\
    Alice & Physics & 3 & B \\
    Bob & Math & 4 & B \\
    Bob & Physics & 3 & A \\
    Carol & Math & 4 & C \\
    \bottomrule
  \end{tabular}

  \vspace{1em}
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{cccc}
    \toprule
    \textbf{course} & \textbf{student} & \textbf{course credits} & \textbf{grade} \\
    \midrule
    Math & (Alice, Bob, Carol) & (4, 4, 4) & (A, B, C) \\
    Physics & (Alice, Bob, Carol) & (3, 3, ?) & (B, A, ?) \\
    \bottomrule
  \end{tabular}
  \tcblower
  (Top) An example of a table of students and their grades in courses.  The columns
  \emph{student} and \emph{course} are the index columns. (Bottom) The same table
  projected into the entity \emph{course}.
\end{tablebox}

\Cref{tab:student-grade-handling} (top) shows that table adapted for our definitions.
Suppose we want to project the table to have only the entity \emph{course}.
Now each row (bottom table) represents a course.  The column \emph{student} is not an
index column anymore, and the values in the column are exhaustive and unique, i.e.,
the whole set $\domainof{\text{student}}$ is represented in the column for each row.

Thus, projection is a very useful operation when we want to change the observational unit
of the data, particularly to the entity represented by a subset of the index columns.
Semantically, projection groups the rows by the values.

It is easy to see that the projection operation is not split-invariant.  Consider the
following example. If we split the top table in \cref{tab:student-grade-handling} so the
first row (Alice, Math) is in one table and the second row (Alice, Physics) is in another,
the bind operation between the projection into the entity \emph{student} of these two
tables is not allowed.  The reason is that the row (Alice) will be present in both tables,
violating the disjoint property of the tables.

The consequence is that a poor architecture of the data schema can lead to
incorrect conclusions in the face of missing information (due to split).  This is one of
the reasons why database normalization is so important.  The usage of parts of the tables
without fully denormalizing them is a bad practice that can lead to spurious information.

\subsection{Grouped and arranged operations}
\label{sub:grouped-arranged}

In practice, when we need more flexibility in the kind of operations we can perform ---
for instance, in data preprocessing ---, we use variations of some operations in
\cref{sec:split-invariant-ops} that are not split-invariant.  These operations are
parametrized by the groups and the order of the rows.

We use the following terminology to refer to the data handling parameters:
\begin{itemize}
  \itemsep0em
  \item \textbf{Aggregation function}: a function that returns a single value given a
    tuple of values; and
  \item \textbf{Window function}: a function that returns a tuple of values given a tuple
    of values of the same size;
\end{itemize}
where the order of the values may play a role in the result of the function.

Examples of aggregation functions are \code{sum} (summation), \code{mean} (average value),
\code{count} (number of elements), and \code{first} (first element of the tuple). Examples
of window functions are \code{cumsum} (cumulative sum), \code{lag} (a tuple with the
previous values), and \code{rank} (the rank of the value in the tuple given some
ordering).

Here, we consider that the rows of the table have cardinality equal to one --- as
discussed before, one can use ungrouping (\cref{sub:ungrouping}) and aggregation
(\cref{sub:aggregation}) to ensure this property.  Without loss of generality, we also
assume that there is only one index, called row number, such that each row has a unique
value for this index\footnote{Since the operations we describe here are not
split-invariant, we can assume a previous projection of the data, see
\cref{sub:projection}.}.

\subsubsection{Mutating with groups and order}

We can take as an example the operation of creating a new column.  To create a new column,
we use an expression that depends on the values of the other columns.  If the expression
depends on an aggregation or window function, one must specify the groups and/or the order
of the rows.

For example, the expression
\begin{center}
  \code{y = cumsum(x) group by category sort by date}
\end{center}
will create a new column \code{y} with the cumulative sum of the \code{x} column for each
\code{category} given the order of the rows defined by the \code{date} column.

\begin{figurebox}[label=fig:mutating-groups-order]{Mutating with groups and order.}
  \centering
  \begin{tikzpicture}[every node/.style={font=\small, inner sep=4pt}]
    \node (source) [darkcircle] at (0, -2) {Source};
    \node (group) [mediumblock] at (0, 0) {Group};
    \node (mutate) [mediumblock] at (2, 0) {Mutate};
    \node (ungroup) [mediumblock, text width=3.5em] at (4, 0) {Ungroup};
    \node (select) [mediumblock] at (6, 0) {Select};
    \node (temp) [darkcircle] at (6, -2) {};
    \node (join) [mediumblock] at (3, -2) {Join};
    \node (result) [darkcircle] at (3, -4) {Result};

    \path [line] (source) -- (group);
    \path [line] (group) -- (mutate);
    \path [line] (mutate) -- (ungroup);
    \path [line] (ungroup) -- (select);
    \path [line] (select) -- (temp);
    \path [line] (temp) -- (join);
    \path [line] (source) -- (join);
    \path [line] (join) -- (result);
  \end{tikzpicture}
  \tcblower
  The mutating operation with groups and order is implemented as a pipeline.
\end{figurebox}

This operation can be implemented as a pipeline.  First, we group (project) the table by
the \code{category} column.  Then, we sort all the tuples by the \code{date} column. Finally,
we apply the \code{cumsum} function to the \code{x} column and ungroup everything.  In the
result table, we select only columns \code{category} and \code{y}.  Going back to the
original table, we can left-join the original table with the result table using the
\code{category} column.  Now, we have the new column \code{y} in the original table.
This is shown in \cref{fig:mutating-groups-order}.

Note that the trivial group would be the whole table, i.e., a column with a single value.
Thus, the grouping is always required, which makes the operation not split-invariant.
In practical applications, I suggest being as explicit as possible about the groups and
order criteria.  This helps to avoid errors and to make the code more readable.

One important aspect about mutating sorted values is that one can use nontrivial
strategies --- from completing missing values to rolling windows --- to deal with implicit
missing values.  This is a powerful tool to deal with time series data. For example, one
can use the \code{lead} function to create a new column with the next value of the
\code{x} column sorted by \code{year}.

If data contains both \code{x = (1, 3)} and
\code{year = (2019, 2021)}, the calculation of the lead will result in
\begin{center}
  \code{x = (1, ?, 3)}, \code{year = (2019, 2020, 2021)}, and \code{lead = (?, 3, ?)},
\end{center}
since the missing value for the year 2020 was implicit.

\subsubsection{Filtering with groups and order}

It is easy to see that to filter rows of the table taking into account groups and order,
we just need to create a new column with the expression that defines the predicate and
then filter the rows based on this column.  For instance, the predicate \code{age > mean(age)
group by country} will select the rows where the value in the \code{age} column is greater
than the mean of the \code{age} for each \code{country}. Another example is the predicate
\code{cumsum(price) < 100 sort by date}, which selects the rows that satisfy the condition
that the cumulative sum of the \code{price} column is less than 100 given the order of the
rows defined by the \code{date} column.

\section{An algebra for data handling}

In recent years, some researchers have made an effort to create a formal algebra for data
transformations.  The idea is to define a set of operations that can be combined to create
complex transformations and describe their main properties.

Note that statistical data handling differs from relational algebra, because the former focuses
on transformations and the latter on information retrieval.

\textcite{Song2021}\footfullcite{Song2021}, for example, propose a formal paradigm for statistical data
transformation.  They present a data model, an algebra, and a formal language.  Their goal
is to create a standard for statistical data transformation that can be used by different
statistical software.

However, in my opinion, the major deficiency of their work is that they mostly try to
``reverse engineer'' the operations that are commonly used in statistical software.  This
is useful for the translation of code between different software, but it is not productive
to advance the theoretical understanding of statistical transformations.

If one ought to tackle the challenge of formally expressing statistical transformations, I
think one should start from the basic operations.  By basic operations, I mean that they
are either irreducible --- i.e., they cannot be expressed as a sequence of other
operations --- or they are so common and intuitive that they are worth being considered
basic.

In this chapter, I try to shed some light on what could be a start for a formal algebra
for general data handling.  I present a set of operations and discuss their properties. I
also present the novel concept of split invariance, which is a property that I think is
important for the operations in the algebra.

For future directions, I suggest that one should try to express completeness in the data
handling context.  Drawing a parallel with computation theory, one could define a
computational model for data handling and try to prove that the operations in the algebra
are complete in the sense that they can express any transformation that can be expressed
in the model.  It would resemble a formal language for data handling that is ``Turing
complete.''

A formal ``complete'' algebra for data handling would be a powerful tool for the
development of new software and the translation of code between different software.  It
would also benefit performance optimizations and pave the way for semantic analysis of
data transformations.  It would be a step as significant as C was from assembly language!

% vim: spell spelllang=en
