\chapter{Model evaluation}
\label{chap:evaluation}

One fundamental step in the development of a data driven solution for a task is the
evaluation of the model. This chapter presents strategies to measure performance of
classifiers and regressors, and how to interpret the results.

We consider the following setup.  Let $D = \{(\vec{x}_i, y_i)\}_{i=1,\dots,n}$ be the
dataset, where $\vec{x}_i$ is a feature vector and $y_i$ is the target value.  We assume
that the dataset is split into a training set, given by indices
$\mathcal{I}_\text{training}$, and a test set, given by indices $\mathcal{I}_\text{test}$,
where $\mathcal{I}_\text{training} \cap \mathcal{I}_\text{test} = \emptyset$ and
$\mathcal{I}_\text{training} \cup \mathcal{I}_\text{test} = \{1,\dots,n\}$.

For evaluation, we assume that the model has been trained on the training set and that
predictions are made on the test set.  We denote the predicted values as $\hat{y}_i$ for
$i \in \mathcal{I}_\text{test}$, such that
\begin{equation*}
  \hat{y}_i = f_\theta(\vec{x}_i)\text{,}
\end{equation*}
where $\theta$ is the solution found by the learning algorithm from the training set, see
\cref{eq:risk}.

\begin{mainbox}{Chapter remarks}

  \boxsubtitle{Contents}

  \startcontents[chapters]
  \printcontents[chapters]{}{1}{}
  \vspace{1em}

  \boxsubtitle{Context}

  \begin{itemize}
    \item \dots
  \end{itemize}

  \boxsubtitle{Objectives}

  \begin{itemize}
    \item \dots
  \end{itemize}

  \boxsubtitle{Takeways}

  \begin{itemize}
    \item \dots
  \end{itemize}
\end{mainbox}

{}
\clearpage

\section{Binary classification evaluation}

In order to assess the quality of a binary classification model, we need to know which
samples in the test set were classified into which classes.  This information is
summarized in the \emph{confusion matrix}, which is the basis for performance measures in
classification tasks.

\subsection{Confusion matrix}

The confusion matrix is a table where the rows represent the true classes and the columns
represent the predicted classes.  The diagonal of the matrix represents the correct
classifications, while the off-diagonal elements represent errors.  For binary
classification, the confusion matrix is given by
\begin{equation*}
  \begin{blockarray}{cccc}
    & & \multicolumn{2}{c}{\text{Predicted}} \\
    & & 1 & 0 \\
    \begin{block}{l c (c c)}
      \text{Expected} & 1 & \text{TP} & \text{FN} \\
      & 0 & \text{FP} & \text{TN} \\
    \end{block}
  \end{blockarray}
\end{equation*}
where
\begin{itemize}
  \item FP is the number of false positives $|\{ i \in \mathcal{I}_\text{test} \mid y_i = 0 \land \hat{y}_i = 1 \}|$,
  \item FN is the number of false negatives $|\{ i \in \mathcal{I}_\text{test} \mid y_i = 1 \land \hat{y}_i = 0 \}|$,
  \item TP is the number of true positives $|\{ i \in \mathcal{I}_\text{test} \mid y_i = 1 \land \hat{y}_i = 1 \}|$, and
  \item TN is the number of true negatives $|\{ i \in \mathcal{I}_\text{test} \mid y_i = 0 \land \hat{y}_i = 0 \}|$.
\end{itemize}

\subsection{Performance measures}

From the confusion matrix, we can derive several performance measures.  The most common
ones are the accuracy, precision, recall, and F-score.

\paragraph{Accuracy} is the proportion of correct predictions over the total number of
samples in the test set, given by
\begin{equation*}
  \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}\text{.}
\end{equation*}
This measure is simple and easy to interpret, but it can be misleading when the classes
are imbalanced --- i.e. when the number of samples in each class is very different.
Consider the case where the dataset has 90\% of samples in class 0 and 10\% in class 1;
a classifier that always predicts class 0 will have an accuracy of 90\%, but it is not
useful for the task.

\paragraph{Specificity} is the proportion of true negative predictions over the total
number of samples that are actually negative, given by
\begin{equation*}
  \text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}}\text{.}
\end{equation*}
This measure is useful when the cost of false positives is high, as it quantifies the
ability of the classifier to avoid false positives.  A test with a higher specificity has
a lower type I error rate.  In terms of a medical diagnosis, a type I error corresponds to
diagnosing a patient as sick\footnote{Sick is the positive class} when they are healthy.
A classifier that always predicts class 1 will have a specificity of 0\% in the example
above.  On the other hand, a classifier that always predicts class 0 will have a specificity
of 100\%.

\paragraph{Precision} is the proportion of true positive predictions over the total number
of samples predicted as positive, given by
\begin{equation*}
  \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}\text{.}
\end{equation*}
This measure is useful when the cost of false positives is high, as it quantifies the
ability of the classifier to avoid false positives.  For example, in a medical diagnosis
task, precision is important to avoid unnecessary treatments.  However, precision does not
consider false negatives, which can be problematic in other scenarios.  For instance, in
fraud detection, a false negative means that a fraudulent transaction was not detected.
A classifier that always predicts class 1 will have a precision of 10\% in the example
above.  On the other hand, a classifier that always predicts class 0 will have a precision
of 0\% --- consider $0/0 = 0$.

\paragraph{Recall} is the proportion of true positive predictions over the total number of
samples that are actually positive, given by
\begin{equation*}
  \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}\text{.}
\end{equation*}
This measure is useful when the cost of false negatives is high, as it quantifies the
ability of the classifier to avoid false negatives.  It can also be interpreted as the
``completeness'' of the classifier: how many positive samples were correctly identified.
For example, in a medical diagnosis task, recall is important to avoid missing a
diagnosis.  A classifier that always predicts class 1 will have a recall of 100\% in the
example above.  On the other hand, a classifier that always predicts class 0 will have a
recall of 0\%.

\paragraph{F-score} is the weighted harmonic mean of precision and recall given by
\begin{equation*}
  \text{F-score}(\beta) =
    \frac%
      {(1 + \beta^2) \cdot \text{Precision} \cdot \text{Recall}}
      {\beta^2 \cdot \text{Precision} + \text{Recall}}\text{,}
\end{equation*}
where $\beta$ is a parameter that controls the weight of precision in the measure.  The
most common value for $\beta$ is 1, which gives the F$_1$-score.  The F-score is useful
when we want to balance precision and recall, as it considers both false positives and
false negatives.  For instance, a classifier that always predicts class 1 will have an
F$_1$-score of 0.18 in the example above.  On the other hand, a classifier that always
predicts class 0 will have an F$_1$-score of 0.  Note that, although guessing $1$ is
better than guessing $0$ in terms of F$_1$-score, this measure is much better than
accuracy to evaluate the performance of the classifier in imbalanced problems.

% TODO: things get bad if majority class is positive

\section{Regression estimation evaluation}

Performance measures for regression tasks are usally calculated based on the error or residual
$\epsilon_i = \hat{y}_i - y_i$ for all $i \in \mathcal{I}_\text{test}$.  The most common measures
are the mean absolute error, mean squared error.

\paragraph{Mean absolute error} is the average of the absolute values of the residuals,
given by
\begin{equation*}
  \text{MAE} = \frac{1}{n} \sum_{i=1}^n | \epsilon_i |\text{.}
\end{equation*}
This measure is easy to interpret and gives an idea of the average error of the model.

\paragraph{Mean squared error} is the average of the squared residuals, given by
\begin{equation*}
  \text{MSE} = \frac{1}{n} \sum_{i=1}^n \epsilon_i^2\text{.}
\end{equation*}
This measure penalizes large errors more than the mean absolute error, as the squared
residuals are summed.

\paragraph{Root mean squared error} is the square root of the mean squared error, given by
\begin{equation*}
  \text{RMSE} = \sqrt{\text{MSE}}\text{.}
\end{equation*}
This measure is in the same unit as the target variable, which makes it easier to
interpret.

% Deixar de fora.
% \paragraph{Coefficient of determination} is a measure of how well the model explains the
% variance of the target variable, given by
% \begin{equation*}
%   R^2 = 1 - \frac{1}{(n-1)s^2} \sum_{i=1}^n \epsilon_i^2\text{,}
% \end{equation*}
% where $s^2$ is the sample variance of the target variable $y_i : i \in
% \mathcal{I}_\text{test}$.  The coefficient of determination ranges from $-\infty$ to 1,
% where 1 indicates a perfect fit and 0 indicates that the model is as good as the mean of
% the target variable.  Negative values indicate that the model is worse than the mean.
% TODO: falar que não é diretamente comparável,
% TODO: assimétricas precisam manter as prioris

% Sometimes, \dots
% Falar do RMSLE, plotar a curva em função da diferença. RMSLE incurs a larger penalty for
% the underestimation of the Actual variable than the Overestimation.

\section{Probabilistic classification evaluation}

A particular case of the regression estimation is when we want to estimate the
probability\footnote{Although the term probability is used, the output of the regressor
does not need to be a probability in the strict sense.  It is a confidence level in the
interval $[0, 1]$ that can be interpreted as a probability.} of a sample belonging to the
positive class --- i.e. $y = 1$.  In this case, the output of the model should be a
probability in the interval $[0, 1]$.  We can use a threshold $\tau$ to convert the
probabilities into binary predictions.  The default threshold is usually $\tau = 0.5$ ---
a sample is positive if the probability is greater than or equal to 0.5 and negative
otherwise ---, but it can be adjusted to change the trade-off between recall and
specificity. A low threshold, $\tau \approx 0$, will increase recall at the expense of
specificity, while a high threshold, $\tau \approx 1$, will increase specificity at the
expense of recall.

Thus, any regressor $f_R : \mathcal{X} \rightarrow [0, 1]$ can be converted into a binary
classifier $f_C : \mathcal{X} \rightarrow \{0, 1\}$ by comparing the output with the
threshold $\tau$:
\begin{equation*}
  f_C(\vec{x}) = \begin{cases}
    1 & \text{if } f_R(\vec{x}) \geq \tau\text{,} \\
    0 & \text{otherwise}\text{.}
  \end{cases}
\end{equation*}

Before we discuss the performance measures for probabilistic classifiers, let us define
some rates that are used in the evaluation.  The true positive rate (TPR) is the proportion
of true positive predictions over the total number of samples that are actually positive,
\begin{equation*}
  \text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}\text{,}
\end{equation*}
and the false positive rate (FPR) is the proportion of false positive predictions over the
total number of samples that are actually negative,
\begin{equation*}
  \text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}\text{.}
\end{equation*}

The performance of the possible variations of the classifiers can be evaluated using
appropriate measures.  Consider the example below of a given test set and the predictions
of a regressor.  We first sort the samples by the predicted probabilities and then
% TODO: define TPR, FPR, FNR somewhere else
calculate the true positive rate and false positive rate for each threshold.
We need to consider only thresholds equal to the unique predicted values to understand the
variations.

\begin{tablebox}[label=tab:prob-reg-example]{Illustrative example of probability .}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{rrrr}
    \toprule
    \textbf{Predicted/Threshold} & \textbf{Expected} & \textbf{TPR} & \textbf{FPR}  \\
    \midrule
    - / $\infty$ & - & $0/5$ & $0/5$ \\
    $0.98$       & 1 & $1/5$ & $0/5$ \\
    $0.97$       & 1 & $2/5$ & $0/5$ \\
    $0.80$       & 0 & $2/5$ & $1/5$ \\
    $0.72$       & 1 & $3/5$ & $1/5$ \\
    $0.70$       & 1 & $4/5$ & $1/5$ \\
    $0.66$       & 0 & $4/5$ & $2/5$ \\
    $0.52$       & 0 & $4/5$ & $3/5$ \\
    $0.40$       & 1 & $5/5$ & $3/5$ \\
    $0.25$       & 0 & $5/5$ & $4/5$ \\
    $0.10$       & 0 & $5/5$ & $5/5$ \\
    \bottomrule
  \end{tabular}
\end{tablebox}

From this table, we can calculate the performance measures that are useful for
probabilistic classifiers.

\subsection{Receiver operating characteristic}

The receiver operating characteristic (ROC) curve is a graphical representation of the
trade-off between the true positive rate and the false positive rate as the threshold
$\tau$ is varied.  The ROC curve is obtained by plotting the TPR against the FPR for all
possible thresholds.  \Cref{fig:roc-example} is the ROC curve for the example in
\cref{tab:prob-reg-example}.

\begin{figurebox}[label=fig:roc-example]{Illustrative example of ROC curve.}
  \centering
  \begin{tikzpicture}
    \datavisualization [
      scientific axes=clean,
      visualize as line/.list={curve, diagonal},
      x axis={label={FPR}, include value=0.0, include value=1.0, length=5cm},
      y axis={label={TPR}, include value=0.0, include value=1.0, length=5cm},
      all axes={grid},
      style sheet=vary dashing,
    ] data [set=curve] {
      % based on the table above
      x, y
      0.0, 0.0
      0.0, 0.2
      0.0, 0.4
      0.2, 0.4
      0.2, 0.6
      0.2, 0.8
      0.4, 0.8
      0.6, 0.8
      0.6, 1.0
      0.8, 1.0
      1.0, 1.0
    } data [set=diagonal] {
      x, y
      0.0, 0.0
      1.0, 1.0
    };
  \end{tikzpicture}
  \tcblower
  ROC curve for the example in \cref{tab:prob-reg-example}.  The diagonal line represents
  a random classifier, and points above the diagonal are better than random.
\end{figurebox}

The ROC curve is useful to explore the trade-off between recall and specificity.  The
diagonal line represents a random classifier, and points above the diagonal are better
than random.  The area under the ROC curve (AUC) is a possible measure of the performance
of the classifier.  The AUC is scale invariant, which means that it measures how well
predictions are ranked, rather than their absolute values.  In our example, the AUC is
$0.8$.

\subsection{Detection error trade-off}

The detection error trade-off (DET) curve is a graphical representation of the trade-off
between the false positive rate and the false negative rate (FNR),
\begin{equation*}
  \text{FNR} = \frac{\text{FN}}{\text{TP} + \text{FN}} = 1 - \text{TPR}\text{.}
\end{equation*}
The DET curve is similar to the ROC curve, but by plotting only the FPR and FNR, it gives
a better view of the ``cost'' (errors) of different thresholds.  The DET curve is
especially useful when the cost of false positives and false negatives is different.
The DET curve of our example is shown in \cref{fig:det-example}.

\begin{figurebox}[label=fig:det-example]{Illustrative example of DET curve.}
  \centering
  \begin{tikzpicture}
    \datavisualization [
      scientific axes=clean,
      visualize as line,
      x axis={label={FPR}, include value=0.0, include value=1.0, length=5cm},
      y axis={label={FNR}, include value=0.0, include value=1.0, length=5cm},
      all axes={grid},
    ] data {
      % based on the table above
      x, y
      0.0, 1.0
      0.0, 0.8,
      0.0, 0.6,
      0.2, 0.6,
      0.2, 0.4,
      0.2, 0.2,
      0.4, 0.2,
      0.6, 0.2,
      0.6, 0.0,
      0.8, 0.0,
      1.0, 0.0,
    };
  \end{tikzpicture}
  \tcblower
  DET curve for the example in \cref{tab:prob-reg-example}.  The diagonal line represents
  a random classifier, and points below the diagonal are better than random.
\end{figurebox}

Usually, the DET curve is plotted in a normal deviate scale~\parencite{Martin1997}.  In
this scale, the axes are transformed to show the error rates in a more linear way.

\begin{figurebox}[label=fig:det-example-normal]{Illustrative example of DET curve (normal deviate scale).}
  \centering
  \begin{tikzpicture}
    \datavisualization [
      scientific axes=clean,
      visualize as line,
      x axis={%
        label={FPR},
        include value=0.001, include value=0.999,
        scaling=-3 at 0cm and 3 at 5cm,
        ticks={%
        %   major={at={0.001, 0.005, 0.02, 0.05, 0.1, 0.2, 0.5, 0.8, 0.9, 0.95, 0.98, 0.995, 0.999}},
          tick typesetter/.code={%
            \gnuplotNorm{##1}
            \pgfmathprintnumber{##1}$\sigma$% (\pgfmathprintnumber{\gnuplotResult})%
          },%
        },%
        % function=\pgfdvmathinvnorm{\pgfvalue}{\pgfvalue}%
      }%
    ] data {
      x,y
      -3.090232306167813,3.090232306167813
      -2.5758293035489,2.5758293035489
      -2.0537489106318225,2.053748910631822
      -1.6448536269514726,1.6448536269514715
      -1.2815515655446008,1.2815515655446008
      -0.8416212335729142,0.8416212335729144
      0,0
      0.8416212335729144,-0.8416212335729142
      1.2815515655446008,-1.2815515655446008
      1.6448536269514715,-1.6448536269514726
      2.053748910631822,-2.0537489106318225
      2.5758293035489,-2.5758293035489
      3.090232306167813,-3.090232306167813
    };
  \end{tikzpicture}
\end{figurebox}

% \gnuplotInvnorm{0.001}
% Result is \gnuplotResult

\section{Other variations}

Some other points:
\begin{itemize}
  \item measures for classification are asymmetric;
  \item prefer measures that work well with averaging;
  \item multiclass how to evaluate?
  \item customize to address the real problem.
\end{itemize}
