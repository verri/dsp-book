\section{Evaluation}
\label{sec:evaluation}

One fundamental step in the validation of a data driven solution for a task is the
\emph{evaluation} of the pair preprocessor and model. This chapter presents strategies to
measure performance of
classifiers and regressors, and how to interpret the results.

We consider the following setup.  Let $T = (K, H, c)$ be a table that represents the data
in the desired observational unit --- as defined in \cref{sec:formal-structured-data}.
Without loss of generality --- as the keys are not used in the modeling process ---, we
can consider $K = \{1, 2, \dots\}$ such that $\rowcard[i] = 1$, if $i \in \{1, \dots,
n\}$, and $\rowcard[i] = 0$, otherwise.  That means that every row $r \in \{1, \dots, n\}$
is present in the table.

The table is split into a training set, given by indices
$\mathcal{I}_\text{training} \in \{1, \dots, n\}$, and a test set, given by indices
$\mathcal{I}_\text{test} \in \{1, \dots, n\}$, such that $$\mathcal{I}_\text{training}
\cap \mathcal{I}_\text{test} = \emptyset$$ and $$\mathcal{I}_\text{training} \cup
\mathcal{I}_\text{test} = \{1,\dots,n\}\text{.}$$

The bridge between the table format (\cref{def:itable}) and the data format used in the
learning process (as described in \cref{sec:learning-problem}) is explained in the
following.  We say that the pair $(\vec{x}_i, y_i)$ contains the feature vector $\vec{x}_i$
and the target value $y_i$ of the sample with key $i$ in table $T$.  Mathematically,
given target variable $h \in H$, we have that $y_i = c(i, h)$ and $\vec{x}_i$ is the tuple
$$\big(c(i, h') : h' \in H \setminus \left\{ h \right\}\big)\text{.}$$

For evaluation, we consider a data preprocessing technique $F$ and a learning machine
$M$.  The following steps are taken.

\paragraph{Preprocessing}

Preprocessing technique $F$ is applied to the training set $T_\text{training} = (K, H,
c_\text{training})$ where \[
  c_\text{training}(i, h) = \begin{cases}
    c(i, h) & \text{if } i \in \mathcal{I}_\text{training}\text{,} \\
    () & \text{otherwise}\text{.}
  \end{cases}
\]  The result is an adjusted training set $T'_\text{training}$ and a fitted
preprocessor $f(\vec{x}; \phi) \equiv f_\phi(\vec{x})$, where $\vec{x} \in \mathcal{X}$
for some space $\mathcal{X}$ that does not include (or does not modify) the target
variable --- consult \cref{sub:formal-preprocessing}.  Note that, by definition, the size
of the adjusted training set can be different from the original due to sampling or
filtering.  The hard requirement is that the target variable $h$ is not changed.

\paragraph{Learning}

The learning machine $M$ is trained on the adjusted training set $D'_\text{training} =
\{(\vec{x}'_i, y'_i)\}$, where pairs $(\vec{x}'_i, y'_i)$ come from the table
$T'_\text{training}$.  The result is a model $f(\vec{x}'; \theta) \equiv
f_\theta(\vec{x}')$ --- consult \cref{chap:slt}.

\paragraph{Transformation}

The preprocessor $f_\phi$ is applied to the test set $T_\text{test} = (K, H,
c_\text{test})$ where \[
  c_\text{test}(i, h) = \begin{cases}
    c(i, h) & \text{if } i \in \mathcal{I}_\text{test}\text{,} \\
    () & \text{otherwise}\text{.}
  \end{cases}
\]  The result is a preprocessed test set $T'_\text{test}$ from which we can obtain the
    set $D'_\text{test} = \{(\vec{x}'_i, y_i) : i \in \mathcal{I}_\text{test}\}$ such
    that $\vec{x}'_i = f_\phi(\vec{x}_i)$.  Note that, to avoid \gls{leakage} and other
    issues, the preprocessor has no access to the target values $y_i$ (even if the
    adjusted training set use the label somehow).

\paragraph{Prediction}

The model $f_\theta$ is used to make predictions on the preprocessed test set
    $D'_\text{test}$ to obtain predicted values $\hat{y}_i = f_\theta(\vec{x}'_i)$ for all
    $i \in \mathcal{I}_\text{test}$.

\paragraph{Evaluation}

By comparing $\hat{y}_i$ with $y_i$ for all $i \in \mathcal{I}_\text{test}$, we
evaluate how good the choice of $\phi$ (parameters of the preprocessor) and $\theta$
(parameters of the model) is.

\subsection{Binary classification evaluation}

In order to assess the quality of a solution for a binary classification task, we need to know which
samples in the test set were classified into which classes.  This information is
summarized in the \emph{confusion matrix}, which is the basis for performance metrics in
classification tasks.

\subsubsection{Confusion matrix}

The confusion matrix is a table where the rows represent the true classes and the columns
represent the predicted classes.  The diagonal of the matrix represents the correct
classifications, while the off-diagonal elements represent errors.  For binary
classification, the confusion matrix is given by
\begin{equation*}
  \begin{blockarray}{cccc}
    & & \multicolumn{2}{c}{\text{Predicted}} \\
    & & 1 & 0 \\
    \begin{block}{l c (c c)}
      \text{Expected} & 1 & \text{TP} & \text{FN} \\
      & 0 & \text{FP} & \text{TN} \\
    \end{block}
  \end{blockarray}
\end{equation*}
where TP is the number of true positives
$$|\{ i \in \mathcal{I}_\text{test} : y_i = 1 \land \hat{y}_i = 1 \}|\text{,}$$
TN is the number of true negatives
$$|\{ i \in \mathcal{I}_\text{test} : y_i = 0 \land \hat{y}_i = 0 \}|\text{,}$$
FN is the number of false negatives
$$|\{ i \in \mathcal{I}_\text{test} : y_i = 1 \land \hat{y}_i = 0 \}|\text{,}$$
and FP is the number of false positives
$$|\{ i \in \mathcal{I}_\text{test} : y_i = 0 \land \hat{y}_i = 1 \}|\text{.}$$

\subsubsection{Performance metrics}

From the confusion matrix, we can derive several performance metrics.  Each of them focus
on different aspects of the classification task, and the choice of the metric depends on
the problem at hand.  Each metric prioritizes different types of errors and yields
a value between 0 and 1, where 1 is the best possible value.

\paragraph{Accuracy} is the proportion of correct predictions over the total number of
samples in the test set, given by
\begin{equation*}
  \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}\text{.}
\end{equation*}
This metric is simple and easy to interpret: a classifier with an accuracy of 1 is
perfect, while a classifier with an accuracy of 0.5 misses half of the predictions.
Accuracy assigns the same weight to any kind of error --- i.e. false positives and false
negatives.  As a result, if the proportion of positive and negative samples is imbalanced,
the value of accuracy may become misleading.  Let $\pi$ be the ratio of positive samples in
the test set --- consequently, $1-\pi$ is the ratio of negative samples ---, then a
classifier that correctly predicts all positive samples and none of the negative samples
will have an accuracy of $\pi$.  If $\pi$ is close to 1, the classifier will have a high value
of accuracy even if it is not good at predicting the negative class.

This issue is not impeditive for the usage of accuracy in imbalanced datasets, but one
needs to be aware that accuracy values lower than $\max(\pi, 1-\pi)$ are not better than
guessing.

\paragraph{Balanced accuracy} aims to solve this interpretation issue of the accuracy.  It
is the average of the true positive rate (TPR) and the true negative rate (TNR), given by
\begin{equation*}
  \text{Balanced Accuracy} = \frac{\text{TPR} + \text{TNR}}{2}\text{,}
\end{equation*}
where
\[
  \text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}\text{,}
\]
and
\[
  \text{TNR} = \frac{\text{TN}}{\text{TN} + \text{FP}}\text{.}
\]
Each term penalizes a different type of error independently: TPR penalizes false
negatives, while TNR penalizes false positives.  Balanced accuracy is useful when the cost
of errors on the minority class is higher than the cost of errors on the majority class.
This way, any value greater than 0.5 is better than random guessing.

A limitation of the balanced accuracy is that it ``automatically'' assigns the weight of
errors based on the class proportion, which may not be the best choice for the problem.
Other metrics focus only one of the classes and are more flexible to adjust the weight of
errors.

\paragraph{Precision} is an asymmetrical metric that focuses on the positive class.  It is
the proportion of true positive predictions over the total number of samples predicted as
positive, given by
\begin{equation*}
  \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}\text{.}
\end{equation*}
This metric is useful when the cost of false alarm is high, as it quantifies the
ability of the classifier to avoid false positives.  For example, in a medical diagnosis
task, precision is important to avoid unnecessary treatments (false positive diagnosis).
Semantically, precision measures how confident we can be that a positive prediction is
actually positive.  Note that it measures nothing about the ability of the classifier in
terms of the negative predictions.

\paragraph{Recall} is another asymmetrical metric that also focuses on the positive class.
It is the proportion of true positive predictions over the total number of
samples that are actually positive, given by
\begin{equation*}
  \text{Recall} = \text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}\text{.}
\end{equation*}
This metric is useful when the cost of missing a positive sample is high, as it quantifies the
ability of the classifier to avoid false negatives.  It can also be interpreted as the
``completeness'' of the classifier: how many positive samples were correctly retrieved.
For example, in a medical diagnosis task, recall is important to avoid missing a
diagnosis.

\paragraph{F-score} is way of balancing both kinds of errors, false positives and false
negatives, while maintaining the focus on the positive class. It is the weighted harmonic
mean of precision and recall given by
\begin{equation*}
  \text{F-score}(\beta) = \text{F}_\beta\text{-score} =
    \frac%
      {(1 + \beta^2) \cdot \text{Precision} \cdot \text{Recall}}
      {\beta^2 \cdot \text{Precision} + \text{Recall}}\text{,}
\end{equation*}
where $\beta > 0$ is a parameter that controls the weight of precision in the metric.
The most common value for $\beta$ is 1, which gives the F$_1$-score.  Higher values of
$\beta$ give more weight to precision ($\beta > 1$), while lower values give more weight
to recall ($\beta < 0$).

\paragraph{Specificity} goes in the opposite direction of recall, focusing on the negative
class.  It is the proportion of true negative predictions over the total
number of samples that are actually negative, given by
\begin{equation*}
  \text{Specificity} = \text{TNR} = \frac{\text{TN}}{\text{TN} + \text{FP}}\text{.}
\end{equation*}
This metric is very common in the medical literature, but less common in other contexts.
The probable reason is that it is easier to interpret the metrics that focus on the
positive class, as the negative class is usually the majority class --- and, thus, less
interesting.

\subsubsection{Interpretation of metrics}

\Cref{tab:classification-metrics} summarizes the properties of the classification
performance metrics.  Accuracy and balanced accuracy are good metrics when no particular
class is more important than the other.  Remember, however, that balanced accuracy gives
more weight to errors on the minority class.  Precision and recall are useful to evaluate
the performance of the solution in terms of the positive.  They are complementary metrics,
and looking at only one of them may give a biased view of the performance --- more on that
below.  The F-score is a way to balance precision and recall with a controlable parameter.

\begin{tablebox}[label=tab:classification-metrics]{Summary of the properties of
  data classification performance metrics.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{l c c}
    \toprule
    \textbf{Metric} & \textbf{Focus} & \textbf{Interpretation} \\
    \midrule
    Accuracy           & Symmetrical & Penalizes all \\
    Balanced Accuracy  & Symmetrical & Penalizes all (weighted) \\
    Recall (TPR)       & Positive & Penalizes FN \\
    Precision          & Positive & Penalizes FP \\
    F-score            & Positive & Penalizes all (weighted) \\
    Specificity (TNR)  & Negative & Penalizes FP \\
    % Fall-out (FPR)     & Negative & Not affected & Penalizes TN \\
    % FPR = 1 - TNR
    \bottomrule
  \end{tabular}
\end{tablebox}

A common misconception about the asymmetrical metrics (especially precision) is that they
are always robust to class imbalance.  Observe \cref{tab:classification-metrics-ex}, which shows
the behavior of the classification performance metrics for three (useless) classifiers: one
that always predicts the positive class (Guess 1), another that always predicts the
negative class (Guess 0), and a classifier that randomly guesses the class independently
of the class priors (Random).

\begin{tablebox}[label=tab:classification-metrics-ex]{Behavior of classification
  performance metrics for different classifiers.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{l c c c}
    \toprule
    \textbf{Metric} & \textbf{Guess 1} & \textbf{Guess 0} & \textbf{Random} \\
    \midrule
    Accuracy$^\dagger$ & $\pi$ & $1 - \pi$ & $0.5$ \\
    Balanced Accuracy & $0.5$ & $0.5$ & $0.5$ \\
    Recall (TPR) & $1$ & $0$ & $0.5$ \\
    Precision$^\dagger$ & $\pi$ & $0/0 = 0$ & $\pi$ \\
    F$_1$-score$^\dagger$ & $\frac{2 \pi}{1 + \pi}$ & 0 & $\frac{2 \pi}{1 + 2\pi}$ \\
    Specificity (TNR) & $0$ & $1$ & $0.5$ \\
    \bottomrule
  \end{tabular}
  \tcblower
  Performance of different classifiers in the example of a dataset with ratio $\pi$ of
  positive and $1-\pi$ of negative samples.  Metrics affected by class imbalance are
  marked with $^\dagger$.
\end{tablebox}

We can see that, as $\pi \to 1$, i.e. the positive class dominates the dataset, guessing
the positive class achieves maximum values for metrics like accuracy, precision, and
F$_1$-score.  Even for random guessing the class, precision (and F$_1$-score) is affected
by the class imbalance, yielding $1$ (and $2/3$) as $\pi \to 1$.  As a result, these
metrics should be preferred when the positive class is the minority class, so the results
are not errouneously inflated --- and, consequently, mistakenly interpreted as good.
\citeauthor{Williams2021}\footfullcite{Williams2021} provides an interesting discussion on
that.

Finally, besides accuracy, the other metrics do not behave well when the evaluation set is
too small.  In this case, the metrics may be too sensitive to the particular samples in
the test set or may not be able to be calculated at all.

\subsection{Regression estimation evaluation}

Performance metrics for regression tasks are usally calculated based on the error (also
called residual) $$\epsilon_i = \hat{y}_i - y_i$$ for all $i \in \mathcal{I}_\text{test}$ or
a scaled version $$\epsilon_i^{(f)} = f(\hat{y}_i) - f(y_i)\text{,}$$ for some scaling
function $f$.

\subsubsection{Performance metrics}

From the errors, we can calculate several performance metrics that give us useful
information about the behavior of the model.  Specifically, we are interested in
understanding what kind of errors the model is making and how large they are.  Unlike
classification, the higher the value of the metric, the worse the model is.

\paragraph{Mean absolute error} is probably the simplest performance metric for regression
estimation tasks.  It is the average of the absolute values of the errors,
given by
\begin{equation*}
  \text{MAE} = \frac{1}{n} \sum_{i=1}^n | \epsilon_i |\text{.}
\end{equation*}
This metric is easy to interpret, is in the same unit as the target variable, and gives an
idea of the average error of the model.  It ignores the direction of the errors, so it is
not useful to understand if the model is systematically overestimating or underestimating
the target variable.

\paragraph{Mean squared error} is the average of the squared residuals, given by
\begin{equation*}
  \text{MSE} = \frac{1}{n} \sum_{i=1}^n \epsilon_i^2\text{.}
\end{equation*}
This metric penalizes large errors more than the mean absolute error, as the squared
residuals are summed.

\paragraph{Root mean squared error} is the square root of the mean squared error, given by
\begin{equation*}
  \text{RMSE} = \sqrt{\text{MSE}}\text{.}
\end{equation*}
This metric is in the same unit as the target variable, which makes it easier to
interpret.  It keeps the same properties as the mean squared error, such as penalizing
large errors more than the mean absolute error.

Both MAE and RMSE (or MSE) work well for positive and negative values of the target
variable.  However, they might be misleading when the range of the target variable is
large.

\paragraph{Mean absolute percentage error} is an alternative when the target variable (and
the predictions) assume only strictly positive values, i.e. $y_i > 0$ and $\hat{y}_i > 0$.
It is the average of the relative errors, given by
\begin{equation*}
  \text{MAPE} = \frac{1}{n} \sum_{i=1}^n \frac{|\epsilon_i|}{y_i}\text{.}
\end{equation*}
This metric is useful when the range of the target variable is large, as it gives an idea
of the relative error of the model, not the absolute error.

\paragraph{Mean absolute logarithmic error} is an alternative for the MAPE under the same
premises of the target values.  It aims to reduce the influence of outliers in the error
calculation, especially when the target variable prior follows a long tail distribution
--- many small values and few large values.  Distributions like that are common in
practice, e.g. in sales, income, and population data.  It is given by
\begin{equation*}
  \text{MALE} = \frac{1}{n} \sum_{i=1}^n | \epsilon_i^{(\ln)} | =
    \frac{1}{n} \sum_{i=1}^n | \ln\hat{y}_i - \ln y_i |\text{.}
\end{equation*}

\subsubsection{Interpretation of metrics}

Note that, unlike the classification performance metrics, the scale of the regression
performance metrics is not bounded between 0 and 1.  This makes it potentially harder to interpret
the results, as the values depend on the scale of the target variable.

Absolute error metrics, like MAE and RMSE, are useful to understand the central tendency
of the magnitude of the errors.  They are easy to interpret because they are in the same
unit as the target variable.  However, they tend to be less informative when the target
variable has a large range or when the errors are not normally distributed.

In those situations, relative error metrics, like MAPE and MALE, are more useful.  For
instance, imagine we are predicting house prices.  The error of \$20,000
for a house that costs \$100,000 is more significant than the same error for a house that
costs \$1,000,000.  The absolute error is the same in both cases, but the relative error
is different.

In that example, the MAPE would be 20\% for the first house and 2\% for the second house.
Note, however, that MAPE punishes overestimating more than underestimating in
multiplicative terms.  Consider the example in \cref{tab:MAPEvsMALE}.  In the first row,
the prediction is ten times larger than the actual value, which results in a MAPE of
900\%.  In the second row, the prediction is one tenth of the actual value, which results
in a MAPE of 90\%.

\begin{tablebox}[label=tab:MAPEvsMALE]{Comparison of relative error metrics.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{r r r r r}
    \toprule
    $\hat{y}$ & $y$ & $\epsilon$ & MAPE & $\exp(\text{MALE})$ \\
    \midrule
    100 & 10 & 90 & 9.0 & 10 \\
      1 & 10 &  9 & 0.9 & 10 \\
    \bottomrule
  \end{tabular}
  \tcblower
  MAPE and MALE for two predictions.  The MAPE punishes overestimating more than
  underestimating.
\end{tablebox}

If multiplicative factors of the error are important, one should consider using MALE.
Observe that $\ln(\hat{y}) - \ln(y) = \ln(\hat{y}/y)$, which is the logarithm of the
ratio of the prediction to the actual value.  In the case of the absolute value, we have
another interesting property: \[
  |\ln\hat{y} - \ln y| =
    |\ln\frac{\hat{y}}{y}| =
    |\ln\frac{y}{\hat{y}}| =
    \ln\max\left(\frac{\hat{y}}{y}, \frac{y}{\hat{y}}\right)\text{.}
\]
\textcite{Tofallis2015}\footfullcite{Tofallis2015} discuss some of these advantages.  To
interpret MALE, we can use the exponential function, which gives us a multiplicative
factor of the error.  In the example in \cref{tab:MAPEvsMALE}, we have that \[
  \exp\ln\max\left(\frac{100}{10}, \frac{10}{100}\right) =
    \max\left(\frac{100}{10}, \frac{10}{100}\right) = 10\text{.}
\]

Finally, for the experimental plan we propose in this book, we should avoid metrics like
coefficient of determination, $R^2$, as we do not make assumptions about model --- in this
case, we do not assume that the model is linear.  Similarly to data classification, we
should prefer metrics work well with small test sets.

\subsection{Probabilistic classification evaluation}

A particular case of the regression estimation is when we want to estimate the
probability\footnote{Although the term probability is used, the output of the regressor
does not need to be a probability in the strict sense.  It is a confidence level in the
interval $[0, 1]$ that can be interpreted as a probability.} of a sample belonging to the
positive class --- i.e. $y = 1$.  In this case, the output of the model should be a
value in the interval $[0, 1]$.  We can use a threshold $\tau$ to convert the
probabilities into binary predictions.  The default threshold is usually $\tau = 0.5$ ---
a sample is positive if the probability is greater than or equal to 0.5, and it is negative,
otherwise.

However, the threshold can be adjusted to change the trade-off between recall and
specificity. A low threshold, $\tau \approx 0$, will increase recall at the expense of
specificity, while a high threshold, $\tau \approx 1$, will increase specificity at the
expense of recall.

Thus, any regressor $f_R : \mathcal{X} \rightarrow [0, 1]$ can be converted into a binary
classifier $f_C : \mathcal{X} \rightarrow \{0, 1\}$ by comparing the output with the
threshold $\tau$:
\begin{equation*}
  f_C(\vec{x}; \tau) = \begin{cases}
    1 & \text{if } f_R(\vec{x}) \geq \tau\text{,} \\
    0 & \text{otherwise}\text{.}
  \end{cases}
\end{equation*}

Since the task is still a classification task, one should not use regression performance
metrics.  On the other hand, instead of choosing a particular threshold and measuring the
resulting classifier performance, we can summarize the performance of all possible
variations of the classifiers using appropriate metrics.

Before, diving into the metrics, consider the following error metric.  Let false positive
rate (FPR) be the proportion of false positive predictions over the total number of
samples that are actually negative,
\begin{equation*}
  \text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}\text{.}
\end{equation*}
It is the complement of the specificity, i.e. $\text{FPR} = 1 - \text{Specificity}$.

Consider the example in \cref{tab:prob-reg-out} of a given test set and the predictions of
a regressor. We can see that that a threshold of 0.5 would yield a classifier that errors
in 3 out of 9 samples.  We can adjust the threshold to understand the behavior of the
other possible classifiers.

\begin{tablebox}[label=tab:prob-reg-out]{Illustrative example of probability regressor
  output.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{rr}
    \toprule
    \textbf{Expected} & \textbf{Predicted} \\
    \midrule
    0 & 0.1  \\
    0 & 0.5  \\
    0 & 0.2  \\
    0 & 0.6  \\
    1 & 0.4  \\
    1 & 0.9  \\
    1 & 0.7  \\
    1 & 0.8  \\
    1 & 0.9  \\
    \bottomrule
  \end{tabular}
\end{tablebox}

We first sort the samples by the predicted probabilities and then calculate the TPR
(recall) and FPR for each threshold.  We need to consider only thresholds equal to the
predicted values to understand the variations.  In this case, TPR values become the
cummulative sum of the expected outputs divided by the total number of positive samples,
and FPR values become the cummulative sum of the complement of the expected outputs
divided by the total number of negative samples.

\begin{tablebox}[label=tab:prob-reg-example]{Illustrative example of classifiers derived
  from different threshold.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{rrrrr}
    \toprule
    \textbf{Expected} & \textbf{Threshold} & \textbf{TPR} & \textbf{FPR}  \\
    \midrule
    - & - / $\infty$ & $0/5$ & $0/4$ \\
    1 & $0.9$        & $1/5$ & $0/4$ \\
    1 & $0.9$        & $2/5$ & $0/4$ \\
    1 & $0.8$        & $3/5$ & $0/4$ \\
    1 & $0.7$        & $4/5$ & $0/4$ \\
    0 & $0.6$        & $4/5$ & $1/4$ \\
    0 & $0.5$        & $4/5$ & $2/4$ \\
    1 & $0.4$        & $5/5$ & $2/4$ \\
    0 & $0.2$        & $5/5$ & $3/4$ \\
    0 & $0.1$        & $5/5$ & $4/4$ \\
    \bottomrule
  \end{tabular}
  \tcblower
  Performance of different classifiers derived from the regressor output in
  \cref{tab:prob-reg-out}.  The threshold are equal to the predicted values.
\end{tablebox}

Note that, from the ordered list of preditions, we can easily see that a threshold of 0.7
would yield a classifier that commits only one error.  A way to summarize the performance
of all possible classifiers is presented in the following.

\subsubsection{Receiver operating characteristic}

The receiver operating characteristic (ROC) curve is a graphical representation of the
trade-off between TPR and FPR as the threshold
$\tau$ is varied.  The ROC curve is obtained by plotting the TPR against the FPR for all
possible thresholds.  \Cref{fig:roc-example} is the ROC curve for the example in
\cref{tab:prob-reg-example}.

\begin{figurebox}[label=fig:roc-example]{Illustrative example of ROC curve.}
  \centering
  \begin{tikzpicture}
    \datavisualization [
      scientific axes=clean,
      visualize as line/.list={curve, diagonal},
      visualize as scatter/.list={points},
      x axis={label={FPR}, include value=0.0, include value=1.0, length=5cm},
      y axis={label={TPR}, include value=0.0, include value=1.0, length=5cm},
      all axes={grid},
      style sheet={vary dashing},
    ] data [set=curve] {
      x, y
      0.0, 0.0
      0.0, 0.2
      0.0, 0.4
      0.0, 0.6
      0.0, 0.8
      0.25, 0.8
      0.50, 0.8
      0.50, 1.0
      0.75, 1.0
      1.0, 1.0
    } data [set=diagonal] {
      x, y
      0.0, 0.0
      1.0, 1.0
    } data [set=points] {
      x, y
      0.0, 0.0
      0.0, 0.2
      0.0, 0.4
      0.0, 0.6
      0.0, 0.8
      0.25, 0.8
      0.50, 0.8
      0.50, 1.0
      0.75, 1.0
      1.0, 1.0
    };
  \end{tikzpicture}
  \tcblower
  ROC curve for the example in \cref{tab:prob-reg-example}.  The diagonal line represents
  a random classifier, and points above the diagonal are better than random.
\end{figurebox}

The ROC curve is useful to explore the trade-off between recall and specificity.  The
diagonal line represents a random classifier, and points above the diagonal are better
than random.

The area under the ROC curve (AUC) is an interesting metric of the performance of the
family of classifiers.  It ranges between 0 and 1, where 1 is the best possible value.
The AUC is scale invariant, which means that it measures how well
predictions are ranked, rather than their absolute values.  It is also robust to
class imbalance, once both recall and specificity are considered.
In our example, the AUC is $0.9$.

% TODO: error visualization or summary of the DET curve
% \subsubsection{Detection error trade-off}
%
% The detection error trade-off (DET) curve is a graphical representation of the trade-off
% between the false positive rate and the false negative rate (FNR),
% \begin{equation*}
%   \text{FNR} = \frac{\text{FN}}{\text{TP} + \text{FN}} = 1 - \text{TPR}\text{.}
% \end{equation*}
% The DET curve is similar to the ROC curve, but by plotting only the FPR and FNR, it gives
% a better view of the ``cost'' (errors) of different thresholds.  The DET curve is
% especially useful when the cost of false positives and false negatives is different.
% The DET curve of our example is shown in \cref{fig:det-example}.
%
% \begin{figurebox}[label=fig:det-example]{Illustrative example of DET curve.}
%   \centering
%   \begin{tikzpicture}
%     \datavisualization [
%       scientific axes=clean,
%       visualize as line,
%       x axis={label={FPR}, include value=0.0, include value=1.0, length=5cm},
%       y axis={label={FNR}, include value=0.0, include value=1.0, length=5cm},
%       all axes={grid},
%     ] data {
%       % based on the table above
%       x, y
%       0.0, 1.0
%       0.0, 0.8,
%       0.0, 0.6,
%       0.2, 0.6,
%       0.2, 0.4,
%       0.2, 0.2,
%       0.4, 0.2,
%       0.6, 0.2,
%       0.6, 0.0,
%       0.8, 0.0,
%       1.0, 0.0,
%     };
%   \end{tikzpicture}
%   \tcblower
%   DET curve for the example in \cref{tab:prob-reg-example}.  The diagonal line represents
%   a random classifier, and points below the diagonal are better than random.
% \end{figurebox}
%
% Usually, the DET curve is plotted in a normal deviate scale~\parencite{Martin1997}.  In
% this scale, the axes are transformed to show the error rates in a more linear way.
%
% \begin{figurebox}[label=fig:det-example-normal]{Illustrative example of DET curve (normal deviate scale).}
%   \centering
%   \begin{tikzpicture}
%     \datavisualization [
%       scientific axes=clean,
%       visualize as line,
%       x axis={%
%         label={FPR},
%         include value=0.001, include value=0.999,
%         scaling=-3 at 0cm and 3 at 5cm,
%         ticks={%
%         %   major={at={0.001, 0.005, 0.02, 0.05, 0.1, 0.2, 0.5, 0.8, 0.9, 0.95, 0.98, 0.995, 0.999}},
%           tick typesetter/.code={%
%             \pgfmathprintnumber{##1}$\sigma$
%           },%
%         },%
%       }%
%     ] data {
%       x,y
%       -3.090232306167813,3.090232306167813
%       -2.5758293035489,2.5758293035489
%       -2.0537489106318225,2.053748910631822
%       -1.6448536269514726,1.6448536269514715
%       -1.2815515655446008,1.2815515655446008
%       -0.8416212335729142,0.8416212335729144
%       0,0
%       0.8416212335729144,-0.8416212335729142
%       1.2815515655446008,-1.2815515655446008
%       1.6448536269514715,-1.6448536269514726
%       2.053748910631822,-2.0537489106318225
%       2.5758293035489,-2.5758293035489
%       3.090232306167813,-3.090232306167813
%     };
%   \end{tikzpicture}
% \end{figurebox}
