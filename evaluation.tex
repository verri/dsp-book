\section{Evaluation}
\label{sec:evaluation}

One fundamental step in the validation of a data driven solution for a task is the
\emph{evaluation} of the pair preprocessor and model. This chapter presents strategies to
measure performance of
classifiers and regressors, and how to interpret the results.

We consider the following setup.  Let $$D = \{(\vec{x}_i, y_i)\}_{i=1,\dots,n}$$ be a
dataset\footnote{We assume that the dataset is tidy, in the appropriate observational
unit, and representative of the phenomenon of interest.}, where $\vec{x}_i$ is a feature
vector and $y_i$ is the target value.  We assume
that the dataset is split into a training set, given by indices
$\mathcal{I}_\text{training} \in \{1, \dots, n\}$, and a test set, given by indices
$\mathcal{I}_\text{test} \in \{1, \dots, n\}$, such that $$\mathcal{I}_\text{training}
\cap \mathcal{I}_\text{test} = \emptyset$$ and $$\mathcal{I}_\text{training} \cup
\mathcal{I}_\text{test} = \{1,\dots,n\}\text{.}$$

For evaluation, we consider a data preprocessing technique $F$ and a learning machine
$M$.  The following steps are taken:
\begin{enumerate}
  \item Preprocessing technique $F$ is applied to the training set $D_\text{training} =
    \{(\vec{x}_i, y_i) : i \in \mathcal{I}_\text{training}\}$ to obtain a preprocessed
    training set $D'_\text{training}$ and a fitted preprocessor $f(\vec{x}, y; \phi)
    \equiv f_\phi(\vec{x}, y)$ --- consult \cref{chap:preprocess}.
  \item The learning machine $M$ is trained on the preprocessed training set
    $D'_\text{training}$ to obtain a model $f(\vec{x}'; \theta) \equiv
    f_\theta(\vec{x}')$ --- consult \cref{chap:slt}.
  \item The preprocessor $f_\phi$ is applied to the test set $D_\text{test} =
    \{(\vec{x}_i, y_i) : i \in \mathcal{I}_\text{test}\}$ to obtain a preprocessed test
    set $D'_\text{test} = \{(\vec{x}'_i, y'_i) : i \in \mathcal{I}_\text{test}\}$ such
    that $(\vec{x}'_i, y'_i) = f_\phi(\vec{x}_i, y_i)$.
  \item The model $f_\theta$ is used to make predictions on the preprocessed test set
    $D'_\text{test}$ to obtain predicted values $\hat{y}_i = f_\theta(\vec{x}_i)$ for all
    $i \in \mathcal{I}_\text{test}$.
\end{enumerate}

Note that, by comparing $\hat{y}_i$ with $y_i$ for all $i \in \mathcal{I}_\text{test}$, we
evaluate how good the choice of $\phi$ (parameters of the preprocessor) and $\theta$
(parameters of the model) was.

\subsection{Binary classification evaluation}

In order to assess the quality of a solution for a binary classification task, we need to know which
samples in the test set were classified into which classes.  This information is
summarized in the \emph{confusion matrix}, which is the basis for performance metrics in
classification tasks.

\subsubsection{Confusion matrix}

The confusion matrix is a table where the rows represent the true classes and the columns
represent the predicted classes.  The diagonal of the matrix represents the correct
classifications, while the off-diagonal elements represent errors.  For binary
classification, the confusion matrix is given by
\begin{equation*}
  \begin{blockarray}{cccc}
    & & \multicolumn{2}{c}{\text{Predicted}} \\
    & & 1 & 0 \\
    \begin{block}{l c (c c)}
      \text{Expected} & 1 & \text{TP} & \text{FN} \\
      & 0 & \text{FP} & \text{TN} \\
    \end{block}
  \end{blockarray}
\end{equation*}
where TP is the number of true positives
$$|\{ i \in \mathcal{I}_\text{test} : y_i = 1 \land \hat{y}_i = 1 \}|\text{,}$$
TN is the number of true negatives
$$|\{ i \in \mathcal{I}_\text{test} : y_i = 0 \land \hat{y}_i = 0 \}|\text{,}$$
FN is the number of false negatives
$$|\{ i \in \mathcal{I}_\text{test} : y_i = 1 \land \hat{y}_i = 0 \}|\text{,}$$
and FP is the number of false positives
$$|\{ i \in \mathcal{I}_\text{test} : y_i = 0 \land \hat{y}_i = 1 \}|\text{.}$$

\subsubsection{Performance metrics}

From the confusion matrix, we can derive several performance metrics.  Each of them focus
on different aspects of the classification task, and the choice of the metric depends on
the problem at hand.  Each metric prioritizes different types of errors and yields
a value between 0 and 1, where 1 is the best possible value.

\paragraph{Accuracy} is the proportion of correct predictions over the total number of
samples in the test set, given by
\begin{equation*}
  \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}\text{.}
\end{equation*}
This metric is simple and easy to interpret: a classifier with an accuracy of 1 is
perfect, while a classifier with an accuracy of 0.5 misses half of the predictions.
Accuracy assigns the same weight to any kind of error --- i.e. false positives and false
negatives.  As a result, if the proportion of positive and negative samples is imbalanced,
the value of accuracy may become misleading.  Let $r$ be the ratio of positive samples in
the test set --- consequently, $1-r$ is the ratio of negative samples ---, then a
classifier that correctly predicts all positive samples and none of the negative samples
will have an accuracy of $r$.  If $r$ is close to 1, the classifier will have a high value
of accuracy even if it is not good at predicting the negative class.

This issue is not impeditive for the usage of accuracy in imbalanced datasets, but one
needs to be aware that accuracy values lower than $\max(r, 1-r)$ are not better than
guessing.

\paragraph{Balanced accuracy} aims to solve this interpretation issue of the accuracy.  It
is the average of the true positive rate (TPR) and the true negative rate (TNR), given by
\begin{equation*}
  \text{Balanced Accuracy} = \frac{\text{TPR} + \text{TNR}}{2}\text{,}
\end{equation*}
where
\[
  \text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}\text{,}
\]
and
\[
  \text{TNR} = \frac{\text{TN}}{\text{TN} + \text{FP}}\text{.}
\]
Each term penalizes a different type of error independently: TPR penalizes false
negatives, while TNR penalizes false positives.  Balanced accuracy is useful when the cost
of errors on the minority class is higher than the cost of errors on the majority class.
This way, any value greater than 0.5 is better than random guessing.

% TODO: move to the end
% \paragraph{Specificity} is the proportion of true negative predictions over the total
% number of samples that are actually negative, given by
% \begin{equation*}
%   \text{Specificity} = \frac{\text{TN}}{\text{TN} + \text{FP}}\text{.}
% \end{equation*}
% This measure is useful when the cost of false positives is high, as it quantifies the
% ability of the classifier to avoid false positives.  A test with a higher specificity has
% a lower type I error rate.  In terms of a medical diagnosis, a type I error corresponds to
% diagnosing a patient as sick\footnote{Sick is the positive class} when they are healthy.
% A classifier that always predicts class 1 will have a specificity of 0\% in the example
% above.  On the other hand, a classifier that always predicts class 0 will have a specificity
% of 100\%.

\paragraph{Precision} is the proportion of true positive predictions over the total number
of samples predicted as positive, given by
\begin{equation*}
  \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}\text{.}
\end{equation*}
This metric is useful when the cost of false positives is high, as it quantifies the
ability of the classifier to avoid false positives.  For example, in a medical diagnosis
task, precision is important to avoid unnecessary treatments.  However, precision does not
consider false negatives, which can be problematic in other scenarios.  For instance, in
fraud detection, a false negative means that a fraudulent transaction was not detected.
A classifier that always predicts class 1 will have a precision of 10\% in the example
above.  On the other hand, a classifier that always predicts class 0 will have a precision
of 0\% --- consider $0/0 = 0$.

\paragraph{Recall} is the proportion of true positive predictions over the total number of
samples that are actually positive, given by
\begin{equation*}
  \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}\text{.}
\end{equation*}
This measure is useful when the cost of false negatives is high, as it quantifies the
ability of the classifier to avoid false negatives.  It can also be interpreted as the
``completeness'' of the classifier: how many positive samples were correctly identified.
For example, in a medical diagnosis task, recall is important to avoid missing a
diagnosis.  A classifier that always predicts class 1 will have a recall of 100\% in the
example above.  On the other hand, a classifier that always predicts class 0 will have a
recall of 0\%.

\paragraph{F-score} is the weighted harmonic mean of precision and recall given by
\begin{equation*}
  \text{F-score}(\beta) =
    \frac%
      {(1 + \beta^2) \cdot \text{Precision} \cdot \text{Recall}}
      {\beta^2 \cdot \text{Precision} + \text{Recall}}\text{,}
\end{equation*}
where $\beta$ is a parameter that controls the weight of precision in the measure.  The
most common value for $\beta$ is 1, which gives the F$_1$-score.  The F-score is useful
when we want to balance precision and recall, as it considers both false positives and
false negatives.  For instance, a classifier that always predicts class 1 will have an
F$_1$-score of 0.18 in the example above.  On the other hand, a classifier that always
predicts class 0 will have an F$_1$-score of 0.  Note that, although guessing $1$ is
better than guessing $0$ in terms of F$_1$-score, this measure is much better than
accuracy to evaluate the performance of the classifier in imbalanced problems.

\subsubsection{Interpretation of metrics}

\begin{tablebox}[label=tab:classification-metrics]{Summary of the properties of
  data classification performance metrics.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{l c c}
    \toprule
    \textbf{Metric} & \textbf{Focus} & \textbf{Interpretation} \\
    \midrule
    Accuracy           & Symmetric & Penalizes all \\
    Balanced Accuracy  & Symmetric & Penalizes all \\
    Recall (TPR)       & Positive & Penalizes FN \\
    Precision          & Positive & Penalizes FP \\
    F-score            & Positive & Penalizes all \\
    Specificity (TNR)  & Negative & Penalizes FP \\
    % Fall-out (FPR)     & Negative & Not affected & Penalizes TN \\
    % FPR = 1 - TNR
    \bottomrule
  \end{tabular}
\end{tablebox}

\begin{tablebox}[label=tab:classification-metrics-ex]{Behavior of classification
  performance metrics for different classifiers.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{l c c c}
    \toprule
    \textbf{Metric} & \textbf{Imbalance} & \textbf{Guess 1} & \textbf{Guess 0} \\
    \midrule
    Accuracy & Affected & $r$ & $1 - r$ \\
    Balanced Accuracy & Not affected & $0.5$ & $0.5$ \\
    Recall (TPR) & Not affected & $1$ & $0$ \\
    Precision & Affected & $r$ & $0/0 = 0$ \\
    F$_1$-score & Affected & $\frac{2r}{1 + r}$ & 0 \\
    Specificity (TNR) & Not affected & $0$ & $1$ \\
    \bottomrule
  \end{tabular}
  \tcblower
  Performance of different classifiers in the example of a dataset with ratio $r$ of
  positive and $1-r$ of negative samples.
\end{tablebox}

% TODO: things get bad if majority class is positive

\subsection{Regression estimation evaluation}

Performance measures for regression tasks are usally calculated based on the error or residual
$\epsilon_i = \hat{y}_i - y_i$ for all $i \in \mathcal{I}_\text{test}$.  The most common measures
are the mean absolute error, mean squared error.

\paragraph{Mean absolute error} is the average of the absolute values of the residuals,
given by
\begin{equation*}
  \text{MAE} = \frac{1}{n} \sum_{i=1}^n | \epsilon_i |\text{.}
\end{equation*}
This measure is easy to interpret and gives an idea of the average error of the model.

\paragraph{Mean squared error} is the average of the squared residuals, given by
\begin{equation*}
  \text{MSE} = \frac{1}{n} \sum_{i=1}^n \epsilon_i^2\text{.}
\end{equation*}
This measure penalizes large errors more than the mean absolute error, as the squared
residuals are summed.

\paragraph{Root mean squared error} is the square root of the mean squared error, given by
\begin{equation*}
  \text{RMSE} = \sqrt{\text{MSE}}\text{.}
\end{equation*}
This measure is in the same unit as the target variable, which makes it easier to
interpret.

% Deixar de fora.
% \paragraph{Coefficient of determination} is a measure of how well the model explains the
% variance of the target variable, given by
% \begin{equation*}
%   R^2 = 1 - \frac{1}{(n-1)s^2} \sum_{i=1}^n \epsilon_i^2\text{,}
% \end{equation*}
% where $s^2$ is the sample variance of the target variable $y_i : i \in
% \mathcal{I}_\text{test}$.  The coefficient of determination ranges from $-\infty$ to 1,
% where 1 indicates a perfect fit and 0 indicates that the model is as good as the mean of
% the target variable.  Negative values indicate that the model is worse than the mean.
% TODO: falar que não é diretamente comparável,
% TODO: assimétricas precisam manter as prioris

% Sometimes, \dots
% Falar do RMSLE, plotar a curva em função da diferença. RMSLE incurs a larger penalty for
% the underestimation of the Actual variable than the Overestimation.

\subsection{Probabilistic classification evaluation}

A particular case of the regression estimation is when we want to estimate the
probability\footnote{Although the term probability is used, the output of the regressor
does not need to be a probability in the strict sense.  It is a confidence level in the
interval $[0, 1]$ that can be interpreted as a probability.} of a sample belonging to the
positive class --- i.e. $y = 1$.  In this case, the output of the model should be a
probability in the interval $[0, 1]$.  We can use a threshold $\tau$ to convert the
probabilities into binary predictions.  The default threshold is usually $\tau = 0.5$ ---
a sample is positive if the probability is greater than or equal to 0.5 and negative
otherwise ---, but it can be adjusted to change the trade-off between recall and
specificity. A low threshold, $\tau \approx 0$, will increase recall at the expense of
specificity, while a high threshold, $\tau \approx 1$, will increase specificity at the
expense of recall.

Thus, any regressor $f_R : \mathcal{X} \rightarrow [0, 1]$ can be converted into a binary
classifier $f_C : \mathcal{X} \rightarrow \{0, 1\}$ by comparing the output with the
threshold $\tau$:
\begin{equation*}
  f_C(\vec{x}; \tau) = \begin{cases}
    1 & \text{if } f_R(\vec{x}) \geq \tau\text{,} \\
    0 & \text{otherwise}\text{.}
  \end{cases}
\end{equation*}

Before we discuss the performance measures for probabilistic classifiers, let us define
some rates that are used in the evaluation.  The true positive rate (TPR) is the proportion
of true positive predictions over the total number of samples that are actually positive,
\begin{equation*}
  \text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}\text{,}
\end{equation*}
and the false positive rate (FPR) is the proportion of false positive predictions over the
total number of samples that are actually negative,
\begin{equation*}
  \text{FPR} = \frac{\text{FP}}{\text{FP} + \text{TN}}\text{.}
\end{equation*}

The performance of the possible variations of the classifiers can be evaluated using
appropriate measures.  Consider the example below of a given test set and the predictions
of a regressor.  We first sort the samples by the predicted probabilities and then
% TODO: define TPR, FPR, FNR somewhere else
calculate the true positive rate and false positive rate for each threshold.
We need to consider only thresholds equal to the unique predicted values to understand the
variations.

\begin{tablebox}[label=tab:prob-reg-example]{Illustrative example of probability .}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{rrrr}
    \toprule
    \textbf{Predicted/Threshold} & \textbf{Expected} & \textbf{TPR} & \textbf{FPR}  \\
    \midrule
    - / $\infty$ & - & $0/5$ & $0/5$ \\
    $0.98$       & 1 & $1/5$ & $0/5$ \\
    $0.97$       & 1 & $2/5$ & $0/5$ \\
    $0.80$       & 0 & $2/5$ & $1/5$ \\
    $0.72$       & 1 & $3/5$ & $1/5$ \\
    $0.70$       & 1 & $4/5$ & $1/5$ \\
    $0.66$       & 0 & $4/5$ & $2/5$ \\
    $0.52$       & 0 & $4/5$ & $3/5$ \\
    $0.40$       & 1 & $5/5$ & $3/5$ \\
    $0.25$       & 0 & $5/5$ & $4/5$ \\
    $0.10$       & 0 & $5/5$ & $5/5$ \\
    \bottomrule
  \end{tabular}
\end{tablebox}

From this table, we can calculate the performance measures that are useful for
probabilistic classifiers.

\subsubsection{Receiver operating characteristic}

The receiver operating characteristic (ROC) curve is a graphical representation of the
trade-off between the true positive rate and the false positive rate as the threshold
$\tau$ is varied.  The ROC curve is obtained by plotting the TPR against the FPR for all
possible thresholds.  \Cref{fig:roc-example} is the ROC curve for the example in
\cref{tab:prob-reg-example}.

\begin{figurebox}[label=fig:roc-example]{Illustrative example of ROC curve.}
  \centering
  \begin{tikzpicture}
    \datavisualization [
      scientific axes=clean,
      visualize as line/.list={curve, diagonal},
      x axis={label={FPR}, include value=0.0, include value=1.0, length=5cm},
      y axis={label={TPR}, include value=0.0, include value=1.0, length=5cm},
      all axes={grid},
      style sheet=vary dashing,
    ] data [set=curve] {
      % based on the table above
      x, y
      0.0, 0.0
      0.0, 0.2
      0.0, 0.4
      0.2, 0.4
      0.2, 0.6
      0.2, 0.8
      0.4, 0.8
      0.6, 0.8
      0.6, 1.0
      0.8, 1.0
      1.0, 1.0
    } data [set=diagonal] {
      x, y
      0.0, 0.0
      1.0, 1.0
    };
  \end{tikzpicture}
  \tcblower
  ROC curve for the example in \cref{tab:prob-reg-example}.  The diagonal line represents
  a random classifier, and points above the diagonal are better than random.
\end{figurebox}

The ROC curve is useful to explore the trade-off between recall and specificity.  The
diagonal line represents a random classifier, and points above the diagonal are better
than random.  The area under the ROC curve (AUC) is a possible measure of the performance
of the classifier.  The AUC is scale invariant, which means that it measures how well
predictions are ranked, rather than their absolute values.  In our example, the AUC is
$0.8$.

\subsubsection{Detection error trade-off}

The detection error trade-off (DET) curve is a graphical representation of the trade-off
between the false positive rate and the false negative rate (FNR),
\begin{equation*}
  \text{FNR} = \frac{\text{FN}}{\text{TP} + \text{FN}} = 1 - \text{TPR}\text{.}
\end{equation*}
The DET curve is similar to the ROC curve, but by plotting only the FPR and FNR, it gives
a better view of the ``cost'' (errors) of different thresholds.  The DET curve is
especially useful when the cost of false positives and false negatives is different.
The DET curve of our example is shown in \cref{fig:det-example}.

\begin{figurebox}[label=fig:det-example]{Illustrative example of DET curve.}
  \centering
  \begin{tikzpicture}
    \datavisualization [
      scientific axes=clean,
      visualize as line,
      x axis={label={FPR}, include value=0.0, include value=1.0, length=5cm},
      y axis={label={FNR}, include value=0.0, include value=1.0, length=5cm},
      all axes={grid},
    ] data {
      % based on the table above
      x, y
      0.0, 1.0
      0.0, 0.8,
      0.0, 0.6,
      0.2, 0.6,
      0.2, 0.4,
      0.2, 0.2,
      0.4, 0.2,
      0.6, 0.2,
      0.6, 0.0,
      0.8, 0.0,
      1.0, 0.0,
    };
  \end{tikzpicture}
  \tcblower
  DET curve for the example in \cref{tab:prob-reg-example}.  The diagonal line represents
  a random classifier, and points below the diagonal are better than random.
\end{figurebox}

Usually, the DET curve is plotted in a normal deviate scale~\parencite{Martin1997}.  In
this scale, the axes are transformed to show the error rates in a more linear way.

\begin{figurebox}[label=fig:det-example-normal]{Illustrative example of DET curve (normal deviate scale).}
  \centering
  \begin{tikzpicture}
    \datavisualization [
      scientific axes=clean,
      visualize as line,
      x axis={%
        label={FPR},
        include value=0.001, include value=0.999,
        scaling=-3 at 0cm and 3 at 5cm,
        ticks={%
        %   major={at={0.001, 0.005, 0.02, 0.05, 0.1, 0.2, 0.5, 0.8, 0.9, 0.95, 0.98, 0.995, 0.999}},
          tick typesetter/.code={%
            \pgfmathprintnumber{##1}$\sigma$
          },%
        },%
      }%
    ] data {
      x,y
      -3.090232306167813,3.090232306167813
      -2.5758293035489,2.5758293035489
      -2.0537489106318225,2.053748910631822
      -1.6448536269514726,1.6448536269514715
      -1.2815515655446008,1.2815515655446008
      -0.8416212335729142,0.8416212335729144
      0,0
      0.8416212335729144,-0.8416212335729142
      1.2815515655446008,-1.2815515655446008
      1.6448536269514715,-1.6448536269514726
      2.053748910631822,-2.0537489106318225
      2.5758293035489,-2.5758293035489
      3.090232306167813,-3.090232306167813
    };
  \end{tikzpicture}
\end{figurebox}

\subsection{Other variations}

Some other points:
\begin{itemize}
  \item measures for classification are asymmetric (benefit positive, positive should be minority);
  \item prefer measures that work well with averaging and good to compare;
  \item Be careful with priors.
  \item multiclass how to evaluate?
  \item customize to address the real problem.
\end{itemize}
