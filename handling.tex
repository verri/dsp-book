\chapter{Data handling}
\label{chap:handling}

\chapterprecishere{%
  {\fontspec[Scale=2]{Symbola}\color{black!80}\symbol{"1F5E1}}
  It's dangerous to go alone! Take this.
  \par\raggedleft--- \textup{Unnamed Old Man}, The Legend of Zelda}

% Important: avoid the term "data manipulation" as it has a negative connotation
% TODO: review this introduction after finishing the remaining of the chapter

In the previous chapter, I discussed the relationship between data format and data
semantics.  We also saw in \cref{chap:project} that data tasks --- specifically
integration and tidying --- must adjust the available data to reflect the kind of
input we expect in production.

For those tasks, we must be careful with the operations we perform on the data. At the
stage of data preparation, for example, we should never parametrize our data handling
pipeline in terms of information retrieved\footnote{For instance, imputation by the mean
of a column.} by sampling the data.  This is because such operations lead to \gls{leakage}
during evaluation and other biases in our conclusions.

In this chapter, we consider that tables are rectangular data structures in which values
of the same column share the same properties (i.e. the same type, same restrictions, etc.)
and each column has a name.  Moreover, we assume that any value is possibly
\emph{missing}.

From a mathematical definition of such tables, we can define a set of operations that can
be applied to them.  These operations are the building blocks of data handling pipelines:
combinations of operations that transform a dataset into another dataset.
I also highlight some important properties of these operations.

I show how these operations can be combined to create complex data handling pipelines by
using them to solve the issues presented in \cref{sub:messy}.

\begin{mainbox}{Chapter remarks}

  \boxsubtitle{Contents}

  \startcontents[chapters]
  \printcontents[chapters]{}{1}{}
  \vspace{1em}

  \boxsubtitle{Context}

  \begin{itemize}
    \item \dots
  \end{itemize}

  \boxsubtitle{Objectives}

  \begin{itemize}
    \item \dots
  \end{itemize}

  \boxsubtitle{Takeways}

  \begin{itemize}
    \item \dots
  \end{itemize}
\end{mainbox}

{}
\clearpage

\section{Formal structured data}

\newcommand{\domainof}[1]{\mathcal{D}\!\left(#1\right)}
\newcommand{\missing}{\text{?}}
\newcommand{\rowcard}[1][k_1, \dots, k_k]{\operatorname{card}\!\left(#1\right)}

In this section, I present a formal definition of structured data.  This definition is
compatible with the relational model and tidy data presented in \cref{chap:data}.
My definition takes into account the index\footnote{Also called grouping variables.} of
the table, which is a key concept in data handling.  We also consider that values can be
missing.  Repeated rows are represented by allowing cells to contain sets of values.
In this chapter, we consider dataset and table as synonyms.

\begin{defbox}{Indexed table}{itable}
An indexed table $T$ is a tuple $(K, H, c)$, where $K = \left\{K_i : i = 1, \dots,
k\right\}$ is the set of index columns, $H$ is the set of (non-index) columns, and $c :
\domainof{K_1} \times \dots \times \domainof{K_k} \times H \to \mathcal{V}$ is the cell function.
Here, $\mathcal{V}$ represents the space of all possible tuples of values, which
may include missing values $\missing$.  Values have arbitrary types, such as integers,
real numbers, strings, etc.
Each index column $K_i$ has a domain $\domainof{K_i}$, which is an enumerable set of
values.
\end{defbox}

A possible row $r$ of the table is thus indexed by a tuple $r = (k_1, \dots,
k_k)$, where $k_i \in \domainof{K_i}$.  Each row has a cardinality $\rowcard[r]$, which
represents how many times the entity represented by the row is present in the table.
A row $r$ with $\rowcard[r] = 0$ is considered to be missing.

A cell is then represented by a row $r$ and a column $h \in H$.  The value of the cell,
$\vec{v} = c(r, h)$ is a tuple of values in the domain $\domainof{h} \cup \{\missing\}$,
such that $|\vec{v}| = \rowcard[r]$.  We say that $\domainof{h}$ is the valid domain of
the column $h$.
The order of the elements in the tuple $\vec{v}$ is arbitrary but fixed.

\begin{defbox}{Nested row}{nested-row}
A \emph{nested
row} consists of a tuple of values that associates different columns with the same
repetition of the entity, i.e. \[
  \Big[ v^h_i : h \in H \Big]\text{,}
\]
where $c(r, h) = [v^h_i : i = 1, \dots, \rowcard[r]]$, assuming an arbitrary fixed order
of the columns $h$.
\end{defbox}

We can stack nested rows to form a matrix of values.  This matrix is called the value
matrix of the row $r$.

\begin{defbox}{Value matrix}{vmatrix}
The value matrix $V = (v_{i, j})$ of the row $r$ is \[
  \Big[ c(r, h) : h \in H \Big]\text{,}
\] with dimensions $\rowcard[r] \times |H|$.
\end{defbox}

We assume that value matrices --- and consequently row cardinalities --- are minimal. This
means that there are no nested row $v_{i, 1}, \dots, v_{i, |H|}$ in the value matrices
such that $v_{i, j} = \missing$ for all $j$.

From these concepts, we can define the basic operations and properties that can be applied
to tables.

\subsection{Splitting and binding}

Split and bind are very basic operations that can be applied to tables.  They are
inverses of each other and are used to divide and combine tables, respectively.
They are important in the data science process because they play a key role in
data semantics and validation of solutions.

\begin{defbox}{Split operation}{split}
Given an indicator function $s : \domainof{K_1} \times \dots \times \domainof{K_k} \to
\left\{0, 1\right\}$, the split operation creates two tables, $T_0$ and $T_1$, that
contains only the rows for which
$s(r) = 0$ and $s(r) = 1$, respectively.

Mathematically, the split operation is defined as \[
  \operatorname{split}(T, s) = \left(T_0, T_1\right)\text{,}
\] where $T = (K, H, c)$, $T_i = (K, H, c_i)$, and \[
  c_i(r, h) = \begin{dcases}
    c(r, h) & \text{if } s(r) = i \\
    () & \text{otherwise.}
  \end{dcases}
\]
\end{defbox}

\emph{Note that, by definition, the split operation never ``breaks'' a row.  So, the
indices define the indivisible entities of the table.}  The resulting tables are
disjoint:

\begin{defbox}{Disjoint tables}{disjoint-tables}
  Two tables $T_0 = (K, H, c_0)$ and $T_1 = (K, H, c_1)$ are said to be disjoint if
  $\rowcard[r; c_0] = 0$ if $\rowcard[r; c_1] > 0$ for any row $r$, and vice-versa.
\end{defbox}

The binding operation is the inverse of the split operation.  Given two disjoint tables
$T_0 = (K, H, c_0)$ and $T_1 = (K, H, c_1)$, the binding operation creates a new table $T$
that contains all the rows of $T_0$ and $T_1$.

\begin{defbox}{Bind operation}{bind}
  Mathematically, the binding operation is defined as \[
    \operatorname{bind}(T_0, T_1) = (K, H, c)\text{,}
  \] where $T_i = (K, H, c_i)$ and \[ c(r, h) = c_0(r, h) + c_1(r, h)\text{.} \]
  The operator $+$ stands for the tuple concatenation operator%
  \footnote{The order of the concatenation here is not an issue since we guarantee
  that at least one of the operands is empty.}.
\end{defbox}

\emph{Thus, a requirement for the binding operation is that the tables are disjoint in
terms of the row entities they have.}

\paragraph{Premises in real-world applications}

One important aspect of these functions is that we assume that the entities represented by
the rows are indivisible, and that any binding operation will never occur for tables that
share the same entities.

In real-world applications, this is not always true.  Many times, we do not know the
process someone else has used to collect the data.  In these cases, we must be careful
about the guarantees we explain in this chapter.  On the other hand, one can consider the
premises we use as a guideline to design good data collection processes.

We can see data collection as the result of a splitting operation in the universe set of
all possible entities.  This is a good way to think about data collection, as we can try
to ensure that we collect all possible information about the entities we are interested
in.

This, of course, depends on what we define as the index columns of the table.  Consider
the example of collecting information about grades of students.  If we define as the index
columns the student's name and year, we must ensure that we collect all the grades of all
subjects a student has taken in a year.  We do not need, though, to collect information
from all students or all years.  On the other hand, if we define as the index column only
the student's name, we must collect all the grades of all subjects a student has taken in
all years.

In summary, the less variables we define as index columns, the more information we must
collect about each entity.  However, in the next sections, we show that assuming many index columns
lead to restrictions in the operations we can perform on the table.

This conceptual trade-off is important to understand when structuring the problem we are
trying to solve.  Neglecting these issues can lead to strong statistical biases and
incorrect conclusions.

\subsection{Split-invariance}

One property we can study about data handling operations is whether they are distributive
over the bind operation.  This property is called \emph{split-invariance}.

From now on, we will denote \[
  T_0 + T_1 = \operatorname{bind}(T_0, T_1)\text{,}
\] for any tables $T_0$ and $T_1$ to simplify the notation.

\begin{defbox}{Split-invariance}{split-invariance}
An arbitrary data handling operation $f(T)$ is said to be split-invariant
if, for any table $T$ and split function $s$, the following equation holds \[
  f\!\left(T_0 + T_1\right) =
    f\!\left(T_0\right) + f\!\left(T_1\right)\text{,}
\] where $T_0, T_1 = \operatorname{split}\!\left(T; s\right)$.
\end{defbox}

Split-invariance is a desirable property for data handling operations during the data
tasks described in \cref{chap:project}: integration and tidying.  Even while exploring
data, we should take effort to use split-invariant operations.

The reason is that split-invariance ensures that the operation does not depend on the
split performed (usually unknown to us) to create the table we have in hand.  This
property is important to avoid \gls{leakage} or to bias the results of the analysis.

\subsection{Illustrative example}

\begin{tablebox}[label=tab:grades1]{Data table of student grades.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{cccc}
    \toprule
    \textbf{student} & \textbf{subject} & \textbf{year} & \textbf{grade} \\
    \midrule
    Alice & Chemistry & 2020 & 6 \\
    Alice & Math & 2019 & 8 \\
    Alice & Physics & 2019 & 7 \\
    Bob & Chemistry & 2018 & ? \\
    Bob & Chemistry & 2019 & 7 \\
    Bob & Math & 2019 & 9 \\
    Bob & Physics & 2019 & 4 \\
    Bob & Physics & 2020 & 8 \\
    Carol & Biology & 2020 & 8 \\
    Carol & Chemistry & 2020 & 3 \\
    Carol & Math & 2020 & 10 \\
    \bottomrule
  \end{tabular}
  \tcblower
  Data collected about student grades.  All information that is available is presented.
\end{tablebox}

Consider the example of data collected about student grades.  \Cref{tab:grades1}
exemplifies all information we can possibly have about the grades of students.  A missing
value in a cell of that table indicates that, for some reason, the information is not
retrievable.

The domain of the variables are:
\begin{itemize}
  \itemsep0em
  \item $\domainof{\text{student}} = \left\{\text{Alice}, \text{Bob}, \text{Carol}\right\}$;
  \item $\domainof{\text{subject}} = \left\{\text{Biology}, \text{Chemistry}, \text{Math},
    \text{Physics}\right\}$;
  \item $\domainof{\text{year}} = \mathbb{Z}$; and
  \item $\domainof{\text{grade}} = \left[0, 10\right] \cup \left\{\missing\right\}$.
\end{itemize}

Of course, in practice, we have no guarantee that the data we have is complete nor the
clear specification of the domain of the variables.  Instead, we must choose good
premises about the data we are working with.

Knowing that the data is complete, we can safely assume that:
\begin{enumerate}
  \itemsep0em
  \item Alice has never taken Biology;
  \item Bob passed Physics, although at the second attempt;
  \item Carol has only taken classes in 2020.
\end{enumerate}

\begin{tablebox}[label=tab:grades2]{Data table of student grades assuming student and subject as indices.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{ccccc}
    \toprule
    \textbf{s} & \textbf{student} & \textbf{subject} & \textbf{year} & \textbf{grade} \\
    \midrule
    0 & Alice & Chemistry & (2020) & (6) \\
    1 & Alice & Math & (2019) & (8) \\
    1 & Alice & Physics & (2019) & (7) \\
    0 & Bob & Chemistry & (2018, 2019) & (?, 7) \\
    0 & Bob & Math & (2019) & (9) \\
    1 & Bob & Physics & (2019, 2020) & (4, 8) \\
    0 & Carol & Biology & (2020) & (8) \\
    0 & Carol & Chemistry & (2020) & (3) \\
    1 & Carol & Math & (2020) & (10) \\
    \bottomrule
  \end{tabular}
  \tcblower
  Indexed table with data from \cref{tab:grades1} assuming student and
  subject as indices.  The column $s$ is the split indicator.
\end{tablebox}

Now consider an arbitrary collection mechanism that consider student and subject as the
indices of the table.  \Cref{tab:grades2} shows the table we have in hand.  The column $s$
is the split indicator.  Only rows with $s = 1$ are available to us.

Now, about the statements we made before:
\begin{enumerate}
  \itemsep0em
  \item There is no way we can know if Alice has taken Biology or not.  It could be that
    the data collection mechanism failed to collect this information or that the
    information simply does not exist.
  \item We can safely assume that Bob has passed Physics in the second try, once all
    information about (Bob, Physics) is assumed to be available.
  \item There is no guarantee that Carol has only taken classes in 2020.  It could be that
    some row (Carol, subject) with year different than 2020 is missing in the table.
\end{enumerate}

\begin{tablebox}[label=tab:grades3]{Data table of student grades assuming student as the index.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{ccp{2.6cm}p{1.8cm}>{\raggedright\arraybackslash}p{1.2cm}}
    \toprule
    \textbf{s} & \textbf{student} & \textbf{subject} & \textbf{year} & \textbf{grade} \\
    \midrule
    1 & Alice & (Chemistry, Math, Physics) & (2020, 2019, 2019) & (6, 8, 7) \\
    0 & Bob & (Chemistry, Chemistry, Math, Physics, Physics) & (2018, 2019, 2019, 2019, 2020) & (?, 7, 9, 4, 8) \\
    1 & Carol & (Biology, Chemistry, Math) & (2020, 2020, 2020) & (8, 3, 10) \\
    \bottomrule
  \end{tabular}
  \tcblower
  Indexed table with data from \cref{tab:grades1} assuming only student
  as the index.  The column $s$ is the split indicator.
\end{tablebox}

We can be even more restrictive and consider only the student as the index of the table.
Imposing this restriction would difficult the data collection process, but it would
guarantee that we have all information about each student.  \Cref{tab:grades3} shows the
table we have in hand.  As before, the column $s$ is the split indicator and only rows with
$s = 1$ are available to us.

Our conclusions may change again:
\begin{enumerate}
  \itemsep0em
  \item We can safely assume that Alice has never taken Biology, as $\text{Chemistry}
    \not\in c(\text{Alice}, \text{subject})$.
  \item We can assume nothing about Bob's grades, as all information about him is missing.
  \item We can safely assume that Carol has only taken classes in 2020, as $c(\text{Carol},
    \text{year})$ contains only values with 2020.
\end{enumerate}

It is straightforward to see that the less index columns we have, the more information we
have about the present entities.  Also, we can see how important it is the assumptions on
the index columns to the conclusions we can draw from the data.  Consequently,
split-invariant operations can preserve valid conclusions about the data even when
information is missing\footnote{Absence can be due to incomplete data collection or
artificial splitting for validation, consult \cref{chap:planning}.}.

\section{Data handling pipelines}

In the literature and in software documentation, you will find a variety of terms used to
describe data handling operations\footnote{%
  The terminology ``data handling'' itself is not universal.  Some authors and libraries
  call it ``data manipulation'', ``data wrangling'', ``data shaping'', or ``data
  engineering''.  I use the term ``data handling'' because it seems more generic.
  Also, it avoids confusion with the term ``data
  manipulation'' which has a negative connotation in some contexts.}. %
They often refer to the same or similar operations, but the terminology can be confusing.
In this section, I present a summary of these operations mostly based on
\textcite{Wickham2023} definitions\footnote{Which they call \emph{verbs}.}.


During the preparation of data for our project, we will need to perform a set of operations
on possibly multiple datasets.  These operations are organized in a pipeline, where the
outputs of one operation are the inputs of the next one.
Operations are extensively parametrized, for instance, most of them can use predicates to
define the groups, arrangements, or conditions under which they should be applied.

\begin{figurebox}[label=fig:pipeline]{Example of data handling pipeline.}
  \centering
  \begin{tikzpicture}[every node/.style={font=\small, inner sep=4pt}]
    \node (s1) [darkcircle] at (0, 0) {Source 1};
    \node (s2) [darkcircle] at (0, -2) {Source 2};
    \node (f1) [mediumblock] at (2, 0) {$f_1$};
    \node (f2) [mediumblock] at (4, 0) {$f_2$};
    \node (f3) [mediumblock] at (2, -2) {$f_3$};
    \node (f4) [mediumblock] at (4, -2) {$f_4$};
    \node (f5) [mediumblock] at (6, -1) {$f_5$};
    \node (data) [darkcircle, minimum width=15mm] (data) at (8, -1) {Data};

    \path [line] (s1) -- (f1);
    \path [line] (f1) -- (f2);
    \path [line] (f1.east) -- (f4);
    \path [line] (s2) -- (f3);
    \path [line] (f3) -- (f4);
    \path [line] (f2) -- (f5);
    \path [line] (f4) -- (f5);
    \path [line] (f5) -- (data);
  \end{tikzpicture}
  \tcblower
  A data handling pipeline is a set of operations that transform a dataset into
  another dataset.  We can have more than one source dataset and the output is a single
  dataset where each row represents a sample in the observational unit we are interested
  in.
\end{figurebox}

In \cref{fig:pipeline}, we show an example of a data handling pipeline.  The pipeline
starts with two source datasets, Source 1 and Source 2.  The datasets are processed by a
set of operations, $f_1, f_2, f_3, f_4, f_5$, and the output is a single dataset,
Data.  Our goal at the data tasks --- see \cref{sub:workflow} --- is to create a dataset
that is representative of the observational unit we are interested in.  Representative
here means that the dataset is tidy\footnote{Remember that our definition of tidiness
depend on the observational unit.  That means, in practice, that if the original data
sources are in a observational unit different from the one we are interested in, after
joining them, the connecting variables might be dropped to eliminate transitive
dependencies.  Consult \cref{sub:tidy-not-tidy,sub:change-unit}.} and that the priors,
i.e. the distribution of the data is faithful to the real distribution of the phenomenon.

A pipeline is more flexible than a chain of operations because it can handle more complex
structures, where different branches (forks) of processing occur simultaneously, and then
come together (merges) later in the workflow.  For instance, the output of $f_1$ is the
input of $f_2$ and $f_4$ (fork), and $f_5$ has as input the outputs of $f_2$ and $f_4$
(merge).

Pipelines are great conceptual tools to organize the data handling process.  They allow
for the separation of concerns, where each operation is responsible for a single task.
Also, declaring the whole pipeline at once allows for the optimization of the operations
and the use of parallel processing.  This is important when dealing with large datasets.
The declarative approach, opposed to the imperative one, makes it easier to reason about
and maintain the code\footnote{Tidyverse and Polars are examples of
libraries that use a declarative approach to data handling.}.

\section{Split-invariant operations}

In this section, I present a set of operations that are split-invariant.  These operations
can be safely applied to the data without worrying about biasing the dataset.

For each operation, we discuss its application on some tidying issues presented in
\cref{sub:messy}.  The issues I address here\footnote{%
The issue of multiple types of observational units stored in the same table is better
dealt with by database normalization.  More on this subject is discussed in
\cref{sub:projection}.}:
\begin{itemize}
  \itemsep0em
  \item Headers are values, not variable names;
  \item Multiple variables are stored in one column; % [mutate/select problem]
  \item Variables are stored in both rows and columns;
  \item A single observational unit is stored in multiple tables.
\end{itemize}



\subsection{Tagged splitting and binding}

We saw that one trivial, yet important, operation is to bind datasets.  This is the
process of combining two or more datasets into a single dataset.  In order to make the
operation reversible, we can parametrize it with a split column that indicates the source
of each row.

\begin{defbox}{Tagged bind operation}{tagged-bind}
  Given two or more disjoint tables $T_i = (K, H, c_i)$, $i = 0, 1, \dots$, the tagged
  bind operation creates a new table $T = (K, H \cup \{s\}, c)$ that contains all the rows
  of tables $T_i$.  The split column $s$ is a new column that indicates the source of each
  row.

  Mathematically, the tagged bind operation is defined as \[
    \operatorname{bind}_{s}(T_0, T_1, \dots) = T\text{,}
  \] where $c(r, h) = c_0(r, h) + c_1(r, h) + \dots$ if $h \in H$ and \[
    c(r, s) = \left[ i \right]^{d} \text{,}
  \]
  where $i$ is the index of the table $T_i$ that contains the row $r$, i.e. $d =
  \rowcard[r; c_i] > 0$.
\end{defbox}

When binding datasets by rows, the datasets must have the same columns.  In practice,
one can assume, if the column is missing, that all values in that column are missing.

The indication of the source table usually captures some hidden semantics that has split
the tables in first place. For instance, if each table represents data collected in a
different year, one can create a new column \emph{year} that contains the year of the
data.  It is important to pay attention on the semantics of the split column, as it can
also contain extra information.

\begin{tablebox}[label=tab:gas-usage]{Gas usage datasets.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{ccc}
    \toprule
    \textbf{month} & \textbf{gas} & \textbf{distance} \\
    \midrule
    1 & 48.7 & 1170 \\
    2 & 36.7 & 1100 \\
    3 & 37.8 & 970 \\
    \dots & \dots & \dots \\
    \bottomrule
  \end{tabular}
  ~
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{ccc}
    \toprule
    \textbf{month} & \textbf{gas} & \textbf{distance} \\
    \midrule
    1 & 143.7 & 1470 \\
    2 & 156.7 & 1700 \\
    3 & 170.8 & 1870 \\
    \dots & \dots & \dots \\
    \bottomrule
  \end{tabular}
  \tcblower
  Monthly gas usage data from US (left) and Brazil (right) residents.
\end{tablebox}

Consider \cref{tab:gas-usage} that contain monthly gas usage data from
US and Brazil residents.  From the requirements described in the previous section, we can
safely bind these datasets --- as they are disjoint.  We can use as tag a new column to
represent the country.  However, an attentive reader will notice that the unit of
measurement of the gas usage and distance are different in each table: gallons and miles
in the US dataset and liters and kilometers in the Brazil dataset.  Ideally, thus, we
should create two other columns to represent the units of measurement.

It is straightforward to see that this operation solves the issue of a single
observational unit is stored in multiple tables described in \cref{sub:messy}.

The reverse function consists of splitting the dataset using as predicate the split column.

\begin{defbox}{Tagged split operation}{tagged-split}
  Let $s$ be a non-index column of a table $T = (K, H \cup \{s\}, c)$ with $\domainof{s}$
  known and finite and that $c(r, s)$ contains only unique values. The tagged split
  operation parametrized by $s$ creates disjoint tables $T_i = (K, H, c_i)$ that contains
  only the rows $r$ for which $c(r, s) = i$.

  Mathematically, the tagged split operation is defined as \[
    \operatorname{split}_{s}(T) = \left(T_0, T_1, \dots\right)\text{,}
  \] where $c_i(r, h) = c(r, h)$ if $i \in c(r, s)$ and $c_i(r, h) = ()$ otherwise.
\end{defbox}

Note that the tagged split is split-invariant by definition, since we assume that the
nested rows of the input table $T$ contains only one value for column $s$ for all
rows\footnote{We consider a slightly different definition of split-invariance here, where
the binding operations is applied for each element of the output of the split operation.}.
Failing to meet this assumption can lead to a biased split.  Also, in practice, it is a
good practice to keep the column $s$ in the output tables to preserve the information
about the source of the rows.  In terms of storage, smart strategies can be used to
avoid the unnecessary repetition of the same value in column $s$.

\subsection{Pivoting}

Another important operation is to pivot datasets.  There are two types of pivoting:
long-to-wide and wide-to-long.  These operations are reversible  and they are the inverse
of each other.

Pivoting long-to-wide requires us to specify a name column --- whose discrete and finite
possible values will become the names of the new columns --- and a value column --- whose
values will be \emph{spread} across the rows.  Other than these columns, all remaining columns
must be indexes.

\begin{defbox}{Pivot long-to-wide operation}{pivot-l2w}
  Let $T = (K \cup \{\text{name}\}, \{\text{value}\}, c)$. The pivot long-to-wide
  operation is defined as \[
    \operatorname{pivot}_\text{name}(T) = T'\text{,}
  \] where $T' = (K, \domainof{\text{name}}, c')$ and \[
    c'(r, h) = c\left(r + (h),~\text{value}\right)\text{.}
  \]
\end{defbox}

Pivoting wide-to-long\footnote{Also known as unpivot.} is the reverse operation. One must
specify all the columns whose names are the values of the before called ``name column.''
The values of these columns will be \emph{gathered} into a new column. As before, all
remaining columns are indexes.

\begin{defbox}{Pivot wide-to-long operation}{pivot-w2l}
  Let $T = (K, H, c)$ be a table, the pivot wide-to-long
  operation is defined as \[
    \operatorname{pivot}^{-1}(T) = T'\text{,}
  \] where $T' = (K \cup \{\text{name}\}, \{\text{value}\}, c')$, $\domainof{\text{name}}
  = H$ and \[
    c'((r, h), \text{value}) = c(r, h)\text{,}
  \] for all valid row $r$ and $h \in H$.
\end{defbox}

In practical applications, where not all remaining columns are indexes, one must aggregate
rows or drop extra non-indexed columns beforehand.  This is discussed in
\cref{sub:aggregation,sub:selection}.

\begin{tablebox}[label=tab:pivot]{Pivoting example.}
    \centering
    \rowcolors{2}{black!10!white}{}
    \begin{tabular}{ccc}
      \toprule
      \textbf{city} & \textbf{year} & \textbf{qty.} \\
      \midrule
      A & 2019 & 1 \\
      A & 2020 & 2 \\
      A & 2021 & 3 \\
      B & 2019 & 4 \\
      B & 2020 & 5 \\
      B & 2021 & 6 \\
      \bottomrule
    \end{tabular}
    ~
    \rowcolors{2}{black!10!white}{}
    \begin{tabular}{cccc}
      \toprule
      \textbf{city} & \textbf{2019} & \textbf{2020} & \textbf{2021} \\
      \midrule
      A & 1 & 2 & 3 \\
      B & 4 & 5 & 6 \\
      \bottomrule
    \end{tabular}
  \tcblower
  The left-hand-side table is in the long format and the right-hand-side table is in the
  wide format.
\end{tablebox}

\Cref{tab:pivot} shows an example of pivoting.  Here, we can consider \emph{city} and
\emph{year} as the index columns.  The left-hand-side table is in the long format and the
right-hand-side table is in the wide format.  Using the pivot long-to-wide operation with
\emph{year} as the name column and \emph{qty.} as the value column, we can obtain the
right-hand-side table.  The reverse operation will give us the left-hand-side table.

To show that the pivot operation is split-invariant, one can see that, given
$T_0 = (K, H, c_0)$ and $T_1 = (K, H, c_1)$ disjoint tables,
\[
  \operatorname{pivot}_\text{name}\!\left(T_0\right) + \operatorname{pivot}_\text{name}\!\left(T_1\right) = \\
    (K, \domainof{\text{name}}, c_0') + (K, \domainof{\text{name}}, c_1')\text{,}
\] where $c_i'(r, h) = c_i(r + (h), \text{value})$.  However, by the disjoint property of
the tables, we have that \[
  c_0(r + (h), \text{value}) + c_1(r + (h), \text{value}) = c(r + (h), \text{value})\text{,}
\] for the table $T = (K, H, c) = T_0 + T_1$. So,
\begin{multline*}
  (K, \domainof{\text{name}}, c_0') + (K, \domainof{\text{name}}, c_1') =
    (K, \domainof{\text{name}}, c') =\\
    \operatorname{pivot}_\text{name}\!\left(T\right)\text{,}
\end{multline*}
where $c'(r, h) = c(r + (h), \text{value})$.

Similarly, the reverse operation is also split-invariant.

Using the pivot operation, we can solve the issues of headers are values, not variable
names and variables are stored in both rows and columns.  In the first case, we can pivot
the table to have the headers as the domain of new index (name column).  In the second
case, we have to pivot both long-to-wide and wide-to-long to solve the issue.

\subsection{Joining}

Joining is the process of combining two datasets into a single dataset based on common
columns.  This is one of the two fundamental operations in relational algebra. We will see
the conditions under with the operation is split invariant. However, the join operation
has some other risks you should be aware of; consult \cref{sec:normalization} for more
details.

Adapting the definitions of projection our context, we can define it as follows.
For the sake of simplicity, we denote $r[U]$ as the row $r$ restricted to the index
columns in $U$, i.e. \[
  r[U] = (k_i : k_i \in \domainof{K_i} \forall K_i \in U)\text{.}
\]

\begin{defbox}{Join operation}{join}
  Let $T' = (K', H', c')$ and $T'' = (K'', H'', c'')$ be two tables such that $K'
  \cap K'' \neq \emptyset$ and $H' \cap H'' = \emptyset$.  The join operation is
  defined as \[
    \operatorname{join}(T', T'') = T\text{,}
  \] where $T = (K' \cup K'', H' \cup H'', c)$ and \[
    c(r, h) = ()
  \] if $\rowcard[{r[K']; c'}] = 0$ or $\rowcard[{r[K'']; c''}] = 0$, for all $h$.
  Otherwise: \[
    c(r, h) = \begin{dcases}
      c'(r[K'], h) & \text{if } h \in H'\text{,} \\
      c''(r[K''], h) & \text{if } h \in H''\text{.}
    \end{dcases}
  \]
\end{defbox}

The join of two tables is the operation that returns a new table with the columns of both
tables.  Let $U$ be the common set of index columns.  For each occurring value of $U$ in
the first table, the operation will look for the same value in the second table.  If it
finds it, it will create a new row with the columns of both tables.  If it does not find
it, no row will be created.

Before we discuss whether the join operation is split invariant\footnote{Note that up to
this point, we have defined this property only for unary operations.}, we can discuss a
variation of the join operation: the left join.  The left join is the same as the join
operation, but if the value of $U$ is missing in the second table, the operation will
create a new row with the columns of the first table and missing values for the columns of
the second table.

In our context, we can see this operation as an unary operation, where the second table is
a fixed parameter.

\begin{defbox}{Left join operation}{left-join}
  Let $T' = (K', H', c')$ and $T'' = (K'', H'', c'')$ be two tables such that $K'
  \cap K'' \neq \emptyset$ and $H' \cap H'' = \emptyset$.  The left join operation is
  defined as \[
    \operatorname{join}(T'; T'') = \operatorname{join}_{T''}(T') = T\text{,}
  \] where $T = (K' \cup K'', H' \cup H'', c)$ and \[
    c(r, h) = ()
  \] if $\rowcard[{r[K']; c'}] = 0$ for all $h$. Otherwise:
  \[
    c(r, h) = \begin{dcases}
      c'(r[K'], h) & \text{if } h \in H'\text{,} \\
      c''(r[K''], h) & \text{if } h \in H''\text{.}
    \end{dcases}
  \]
\end{defbox}

The left join operation is split-invariant.  To see this, consider two disjoint tables
$T_0 = (K, H, c_0)$ and $T_1 = (K, H, c_1)$, and a third table $T' = (K', H', c')$ such
that $K \cap K' \neq \emptyset$ and $H \cap H' = \emptyset$.  We have that
\begin{multline*}
  \operatorname{join}_{T'}(T_0) + \operatorname{join}_{T'}(T_1) =
  T_0' + T_1' =\\
    (K \cup K', H \cup H', c_0') + (K \cup K', H \cup H', c_1')\text{,}
\end{multline*}
where the meaning of each term is clear from the \cref{def:left-join}.
It is straightforward to see that rows in $T_0'$ and $T_1'$ are disjoint, since at least
part of the indices in $K \cup K'$ are different between them.

Moreover, \[
  \operatorname{join}_{T'}(T_0 + T_1) = (K \cup K', H \cup H', c')
\] with $c'(r, h) = ()$ only if both $\rowcard[{r[K]; c_0}] = 0$ and $\rowcard[{r[K];
c_1}] = 0$.  Otherwise, $c'(r, h) = c_0(r[K], h) + c_1(r[K], h)$ if $h \in H$ and $c'(r,
h) = c_0(r[K], h)$ if $h \in H'$.

Thus, \[
  \operatorname{join}_{T'}(T_0 + T_1) = \operatorname{join}_{T'}(T_0) + \operatorname{join}_{T'}(T_1)\text{.}
\]

Our conclusion is that the left join operation given a fixed table is split-invariant.
So we can safely use it to join tables without worrying about biasing the dataset once we
fix the second table.

I conjecture that the (inner) join operation shares similar properties but it is not as
safe, nonetheless I clear definition of split invariance for binary operations is needed.
This is left as a thought exercise for the reader.  Notice that the traditional join has
the ability to ``erase'' rows from any of the tables involved in the operation.  This is a
potential source of bias in the data.  This further emphasizes the importance of the
understanding the semantics of the data schema before joining tables --- consult
\cref{sec:normalization}.

\subsection{Selecting}
\label{sub:selection}

Selecting is the process of choosing a subset of non-index columns from a dataset.  The
remaining columns are discarded.  Rows of the table remain unchanged.

Although very simple, the selection operation is useful to remove columns that are not
relevant to the analysis.  Also, it might be needed before other operations, such as
pivoting, to avoid unnecessary columns (wide-to-long) and to keep the only value column
(long-to-wide).

\begin{defbox}{Selection operation}{selection}
  Let $T = (K, H, c)$ be a table and $H' \subseteq H$.  The selection operation is
  defined as \[
    \operatorname{select}_{H'}(T) = T'\text{,}
  \] where $T' = (K, H', c)$.
\end{defbox}

Sometimes, it is useful to select columns based on a function of the column properties.
In other words, the selection operation can be parametrized by a predicate.  The predicate
is a function that returns a logical value given the column.

\begin{defbox}{Predicate selection operation}{predicate-selection}
  Let $T = (K, H, c)$ be a table and $P : H \to \{0, 1\}$ be a predicate.  The predicate
  selection operation is defined as \[
    \operatorname{select}_{P}(T) = T'\text{,}
  \] where $T' = (K, H', c)$ and $H' = \{h \in H : P(h) = 1\}$.
\end{defbox}

It is trivial to see that, if $P$ do not depend on the values of the columns (i.e. has no
access to $c$), the predicate selection operation is split-invariant.  This is because the
operation does not change the rows of the table nor it depends on the values of the rows.

One example of the use of the predicate selection operation is to keep columns whose
values are in a specific domain.  For instance, to keep only columns that contains real
numbers, we choose $P(h) = 1$ if $\domainof{h} = \mathbb{R}$, and $P(h) = 0$ otherwise.

The case where the predicate depends on the values of the columns is discussed in
\cref{sub:selection-value}.

\newpage

We use the following terminology to refer to the data handling parameters:
\begin{itemize}
  \item \textbf{Predicate}: a function that returns a logical value, used to filter
    rows/columns or to define the groups of rows/columns to be processed;
  \item \textbf{Aggregation function}: a function that returns a single value given a vector
    of values (in which, the order of the values may be important);
  \item \textbf{Window function}: a function that returns a vector of values given a vector
    of values in which, the order of the values is important;
  \item \textbf{Expression}: a function that returns a vector of values element-wise, used to create new
    columns or to modify existing ones.
\end{itemize}

\subsection{Filtering rows}

Filtering is the process of selecting a subset of rows from a dataset based on a
predicate.  If more than a single predicate is used, they are combined using a logical
operator, such as logical disjunction (or) or logical conjunction (and).

After filtering, the dataset will contain only the rows that satisfy the predicate.
Columns remain unchanged.  This operation is potentially irreversible, as the removed
rows are lost.

In the basic form, each row is treated independently.  For instance, the predicate
\code{age > 18} will select all rows where the value in the \code{age} column is
greater than 18.

However, if the predicate depends on an aggregation or window function, one must specify
the groups and/or the order of the rows.  For instance, the predicate \code{age >
mean(age) group by country} will select the rows where the value in the \code{age}
column is greater than the mean of the \code{age} for each \code{country}. Another
example is the predicate \code{cumsum(price) < 100 sort by date}, which selects the rows
that satisfy the condition that the cumulative sum of the \code{price} column is less
than 100 given the order of the rows defined by the \code{date} column.

The trivial group is the entire dataset, so it is usually not necessary to specify it
explicitly.  However, it is usually not sensible to not specify the order of the rows.

When dealing with real values, be aware of floating-point precision issues.  In other
words, do not use the equality operator to compare real numbers.  Most of libraries
provide operators to compare real numbers within a given tolerance.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use filtering to remove rows that are not relevant to your analysis;
    \item Use predicates to define the conditions under which rows should be removed;
    \item When aggregation functions are needed to define the predicate, specify the groups and
      the order of the rows;
    \item Be aware of floating-point precision issues when comparing real numbers.
  \end{itemize}
\end{hlbox}

\subsection{Mutating columns}

Mutating is the process of creating new columns.  The operation is reversible, as the
original columns are kept.  The new columns are added to the dataset.

The values in the new column are determined by an expression.  The expression is a
function that returns a vector of values given the values in the other columns.  The
expression can be a simple function, such as \code{y = x + 1}, or a more complex
function, such as
\begin{center}
  \code{y = ifelse(x > 0, 1, 0)}.
\end{center}
Here, \code{x} and \code{y} are
the names of an existing and the new column, respectively.

One may also use an aggregation and window function in the expression. This is particularly
useful when performing mutation considering a group.  In this case, the returned value is
repeated (aggregation function) for each row of the same group.  Like in filtering, the
more explicit you can be about order and groups, the better.

For example, the expression
\begin{center}
  \code{y = cumsum(x) group by category sort by date}
\end{center}
will create a new column \code{y} with the cumulative sum of the \code{x} column for each
\code{category} given the order of the rows defined by the \code{date} column.

Sometimes, the same expression can be used to create multiple columns.  This is useful
when the new columns are related.  To do so, one first specifies the columns in the same way as
when selecting columns.  Then, one needs to specify a rule to name the new columns.
For instance, \code{x\_new = x + 1 across x matches \textasciicircum{}col[0-9]+\$}.

Practically speaking, mutation can overwrite existing columns.  This is useful when the
new column is a replacement for the old one.  Formally, overwriting is just a sequence of
mutation and selection operations.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use mutating to create new columns that are relevant to your analysis;
    \item Use expressions to define the values of the new columns;
    \item Use aggregation and window functions in the expression to create new columns based on
      groups and order;
    \item Use the same expression to create multiple columns when the new columns are related.
  \end{itemize}
\end{hlbox}

\subsection{Aggregating rows}
\label{sub:aggregation}

We can aggregate the rows of a dataset to create a new dataset with fewer rows.    The
operation is not reversible, as the discarded rows are lost.  The columns are also lost,
only the new aggregate columns remain.

The values in the new columns are determined by an aggregation function.  Like filtering
and mutation, the aggregation function can be parametrized by specifying a group and/or an
order.

The resulting dataset will contain one row for each group.  The values in the new columns
are determined by the aggregation function applied to the values in the other columns.
All columns that define the groups are usually kept in the resulting dataset.  In this
case, as expected, values of such columns are equal for all rows in the same group.

For instance, the aggregation function \code{mean(x) group by category} will create a
new dataset with one row for each different value of \code{category} and a new column
with the mean of the \code{x} column for each group.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use aggregation to summarize the data in a dataset;
    \item Use aggregation functions to define the values of the new columns;
    \item Other columns are lost;
    \item Use the group and order parameters to define the groups and the behavior of the
      aggregation function.
  \end{itemize}
\end{hlbox}

\section{Other operations}

\dots

\subsection{Projecting and aggregating}
\label{sub:projection}

Projection is one of the two fundamental operations in relational algebra --- consult
\cref{sec:normalization} for more details.  In database normalization theory, tables ---
called relations --- are slightly different from the tables we are discussing here. The
major difference is that they are sets of tuples, which means that each tuple is unique.
In our scenario, this is similar to what we call rows represented by the possible values
of the index columns of the table.

Adapting the definitions of projection our context, we can define it as follows.

\begin{defbox}{Projection operation}{projection}
  Let $T = (K, H, c)$ be a table and $K' \subset K$ a subset of the columns.  The
  projection operation is defined as \[
    \operatorname{project}_{K'}(T) = T'\text{,}
  \] where $T' = (K', H \cup (K \setminus K'), c')$ and \[
    c'(r, h) = \begin{dcases}
      \sum_{r'} c(r + r', h) & \text{if } h \in H \\
      \sum_{k' \in \domainof{h}} k' & \text{if } h \in K \setminus K'\text{,}
    \end{dcases}
  \] for all valid row $r$ considering the indices $K'$ and for all tuples $r' = (k_i :
  i)$ such that $k_i \in \domainof{K_i}$ for all $K_i \in K \setminus K'$.
\end{defbox}

We can see that projection for our tables is a little more complex than the usual
projection in relational algebra.  Consider the example we discussed in
\cref{sec:normalization} as well, where we have a table with the columns \emph{student},
\emph{subject}, \emph{year}, and \emph{grade}.

\begin{tablebox}[label=tab:student-grade-handling]{Student grade table.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{cccc}
    \toprule
    \textbf{student} & \textbf{course} & \textbf{course credits} & \textbf{grade} \\
    \midrule
    Alice & Math & 4 & A \\
    Alice & Physics & 3 & B \\
    Bob & Math & 4 & B \\
    Bob & Physics & 3 & A \\
    Carol & Math & 4 & C \\
    \bottomrule
  \end{tabular}

  \vspace{1em}
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{cccc}
    \toprule
    \textbf{course} & \textbf{student} & \textbf{course credits} & \textbf{grade} \\
    \midrule
    Math & (Alice, Bob, Carol) & (4, 4, 4) & (A, B, C) \\
    Physics & (Alice, Bob, Carol) & (3, 3, ?) & (B, A, ?) \\
    \bottomrule
  \end{tabular}
  \tcblower
  (Top) An example of a table of students and their grades in courses.  The columns
  \emph{student} and \emph{course} are the index columns. (Bottom) The same table
  projected into the entity \emph{course}.
\end{tablebox}

\Cref{tab:student-grade-handling} (top) shows that table adapted for our definitions.
Suppose we want to project the table to have only the entity \emph{course}.
Now each row (bottom table) represents a course.  The column \emph{student} is not a
index column anymore, and the values in the column are exhaustive and unique, i.e
the whole set $\domainof{\text{student}}$ is represented in the column for each row.

It is easy to see that the projection operation is not split-invariant.  Consider the
following example. If we split the top table in \cref{tab:student-grade-handling} so the
first row (Alice, Math) is in one table and the second row (Alice, Physics) is in another,
the bind operation between the projection into the entity \emph{student} of these two
tables is not allowed.  The reason is that the row (Alice) will be present in both tables,
violating the disjoint property of the tables.

% TODO: relationship between projection and aggregation

% TODO: connection with database normalization

\subsection{Selecting columns by value}
\label{sub:selection-value}

There are two main ways to select columns: by name or by predicate.  The former is the
most common and is used to select a fixed set of columns.  The latter is used to select
columns that satisfy a given condition, i.e., the values in the columns are used to
determine which columns should be selected.

When selecting columns by name, one can use a list of column names or a regular
expression\footnote{Regular expressions are very general and powerful, but they are also
complex and error-prone.  An alternative is to use some form of hierarchical naming,
such as \code{type.column} to express groups of columns.}.
The latter is useful when the column names follow a pattern that reflects the semantics of
the columns.  For instance,
one can use the regular expression \code{col[0-9]+} to select all columns whose names
start with \code{col} followed by one or more digits.

When selecting columns by predicate, one can use a function that returns a logical value
to define the condition under which a column should be selected.  For instance, one can
use the predicate \code{isnumeric} to select all columns that contain numeric values.
Notice, however, that the predicate is applied to each column independently and returns a
single logical value for each column.

Like filtering, selecting predicates might contain aggregation functions.  Although it is
theoretically possible to consider the order of the values in the columns, it is not common
to do so.  (Especially because one would need to assume that the rows are previously
sorted by some criterion.) Groups, however, never make sense in this context, once the
predicate is applied to each column independently.

Depending on the context, it may be useful to ``drop'' columns instead of selecting them.
This is the same as selecting all columns except the ones specified.  This is useful when
the number of columns to be dropped is small compared to the total number of columns.
Strictly speaking, we just need to negate the predicate or the regular expression used to
select the columns.

Finally, it is very common to find libraries and framework in which the order of the
columns is important.  As a result, columns can be selected by position as well.
I find this practice error-prone and I recommend avoiding it whenever possible.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use selecting to remove columns that are not relevant to your analysis;
    \item Use column names or regular expressions (or hierarchical names) to select columns;
    \item Use predicates (many to one, with no aggregation functions) to define the conditions
      under which columns should be selected;
    \item Avoid depending on the order of the columns.
  \end{itemize}
\end{hlbox}


\section{An algebra for statistical transformations}

In recent years, some researchers made an effort to create a formal algebra for
statistical transformations.  The idea is to create a set of operations that can be
combined to create complex statistical transformations.  This is similar to the idea of
relational algebra, which is a set of operations that can be combined to create complex
queries.

The difference between relational algebra and a formal algebra for statistical
transformations is that the latter is more complex.  This is because statistical
transformations are more complex than queries.  For instance, the concept of missing data
is not present in relational algebra, but it is in statistical transformations.

\textcite{Song2021}, for example, propose a formal paradigm for statistical data
transformation.  They present a data model, an algebra, and a formal language.  Their goal
is to create a standard for statistical data transformation that can be used by different
statistical software.

However, in my opinion, the major deficiency of their work is that they mostly try to
``reverse engineer'' the operations that are commonly used in statistical software.  This
is useful for the translation of code between different software, but it is not productive
to advance in the theoretical understanding of statistical transformations.

If one ought to tackle the challenge of formally expressing statistical transformations, I
think one should start from the basic operations.  Basic operations mean that they are
irreducible, i.e., they cannot be expressed as a sequence of other operations.

Some thoughts about it:
\begin{itemize}
  \item Binding columns can be expressed as a join operation, thus it is not a basic
    operation.
  \item Some software provide features that can be better expressed in other (often simpler) ways.  Row
    naming is an example.  It is useful to keep track of the origin of each row, but names
    can be just another column.  I argue for excluding row naming in a formal algebra.
  \item Some operations are very useful and recurring, even if they are not basic.  Such
    operations must be omitted from the formal algebra for the sake of simplicity.
    However, any software that implements a language for the formal algebra can provide
    syntax sugar for these operations.
  \item Not defining your algebra in terms of a specific programming language is a good
    practice.  This is because the algebra is a theoretical concept and should be
    independent of any implementation.  It also gives opportunities to rethink the
    things that commonly done in a specific way.  This can lead to new insights and
    correct error-prone practices.
  \item Pivoting seems to be ``different'' enough to the other operations to be considered
    in the set of basic operations.  However, it is not hard to see that they can be
    rewritten as combinations with the meta tables containing the possible values of the
    attributes (or some sort of aggregation function).
\end{itemize}

% vim: spell spelllang=en
