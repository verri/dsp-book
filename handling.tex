\chapter{Data handling}
\label{chap:handling}

\chapterprecishere{%
  Tidy datasets are all alike, but every messy dataset is messy in its own way.
  \par\raggedleft--- \textup{Hadley Wickham}, Tidy Data}

% Important: avoid the term "data manipulation" as it has a negative connotation

Data handling is the process of adjusting data to make it suitable for analysis.
It involves three main tasks: data transformation, data cleaning, and data integration.

In this chapter, we consider that tables are rectangular data structures in which values
of the same column share the same properties (i.e. the same type, same restrictions, etc.)
and each column has a name.  Moreover, we assume that any value is possibly
\emph{missing}.

\begin{mainbox}{Chapter remarks}

  \boxsubtitle{Contents}

  \startcontents[chapters]
  \printcontents[chapters]{}{1}{}
  \vspace{1em}

  \boxsubtitle{Context}

  \begin{itemize}
    \item \dots
  \end{itemize}

  \boxsubtitle{Objectives}

  \begin{itemize}
    \item \dots
  \end{itemize}

  \boxsubtitle{Takeways}

  \begin{itemize}
    \item \dots
  \end{itemize}
\end{mainbox}

{}
\clearpage

\section{Data handling operators}

In the literature and in software documentation, you will find a variety of terms used to
describe data handling operations\footnote{%
  The terminology ``data handling'' itself is not universal.  Some authors and libraries
  call it ``data manipulation'', ``data wrangling'', ``data shaping'', or ``data
  engineering''.  I use the term ``data handling'' to avoid confusion with the term ``data
  manipulation'' which has a negative connotation in some contexts.}. %
They often refer to the same or similar operations, but the terminology can be confusing.
In this section, I present a summary of these operations mostly based on
\textcite{Wickham2023} definitions\footnote{Which are called \emph{verbs}.}.

% \begin{slidebox}{Data handling operators}{}
%   \begin{itemize}
%     \item Filtering rows;
%     \item Selecting columns;
%     \item Mutating columns;
%     \item Aggregating rows;
%     \item Binding datasets;
%     \item Joining datasets;
%     \item Pivoting (spreading) and unpivoting (gathering) datasets.
%   \end{itemize}
% \end{slidebox}

These operations are the building blocks of the data handling tasks we will discuss in the
next sections.  They can also be extensively parametrized and combined to create more
elaborate data handling pipelines.  For instance, most of them can use predicates to
define the groups, arrangements, or conditions under which they should be applied.

We use the following terminology to refer to the data handling parameters:
\begin{itemize}
  \item \textbf{Predicate}: a function that returns a logical value, used to filter
    rows/columns or to define the groups of rows/columns to be processed;
  \item \textbf{Aggregation function}: a function that returns a single value given a vector
    of values (in which, the order of the values may be important);
  \item \textbf{Window function}: a function that returns a vector of values given a vector
    of values in which, the order of the values is important;
  \item \textbf{Expression}: a function that returns a vector of values element-wise, used to create new
    columns or to modify existing ones.
\end{itemize}

% \begin{slidebox}{Data handling pipelines}{}
%   \begin{itemize}
%     \item Data handling operations can be combined to create complex pipelines;
%     \item Operators may be reversible;
%     \item Operators are vectorized;
%     \item They can be parametrized with predicates, aggregation functions, and expressions;
%     \item They operate on datasets and return new datasets as output.
%     \item They are declarative.
%   \end{itemize}
% \end{slidebox}

Operators are also vectorized, meaning that they can be applied to multiple columns or
rows at once.  This is a key feature of data handling operations, as it allows for
expressive and efficient data manipulation.

Many of them are also reversible, meaning that they can be undone.  This is important
because it allows for reproducibility and traceability of the data handling process.

They operate on a dataset (or more than one) given as input and return a new dataset as
output.  This is important because it allows for the creation of data handling pipelines,
where the output of one operation is the input of the next one.  Parameters like column
names, predicates, aggregation functions, and expressions can be passed to these operations to
customize their behavior.

Unlike traditional procedural programming, where conditional statements and loops are used
to manipulate data, data handling operations are declarative.  This means that they are
expressed in terms of what should be done, not how it should be done.  This is a powerful
abstraction that allows for the creation of complex pipelines with a few lines of code.

\subsection{Filtering rows}

Filtering is the process of selecting a subset of rows from a dataset based on a
predicate.  If more than a single predicate is used, they are combined using a logical
operator, such as \texttt{AND} or \texttt{OR}.

After filtering, the dataset will contain only the rows that satisfy the predicate.
Columns remain unchanged.  This operation is potentially irreversible, as the removed
rows are lost.

In the basic form, each row is treated independently.  For instance, the predicate
\texttt{age > 18} will select all rows where the value in the \texttt{age} column is
greater than 18.

However, if the predicate depends on an aggregation or window function, one must specify
the groups and/or the order of the rows.  For instance, the predicate \texttt{age >
mean(age) group by country} will select the rows where the value in the \texttt{age}
column is greater than the mean of the \texttt{age} for each \texttt{country}. Another
example is the predicate \texttt{cumsum(price) < 100 sort by date}, which selects the rows
that satisfy the condition that the cumulative sum of the \texttt{price} column is less
than 100 given the order of the rows defined by the \texttt{date} column.

The trivial group is the entire dataset, so it is usually not necessary to specify it
explicitly.  However, it is usually not sensible to not specify the order of the rows.

When dealing with real values, be aware of floating-point precision issues.  In other
words, do not use the equality operator to compare real numbers.  Most of libraries
provide operators to compare real numbers within a given tolerance.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use filtering to remove rows that are not relevant to your analysis;
    \item Use predicates to define the conditions under which rows should be removed;
    \item When aggregation functions are needed to define the predicate, specify the groups and
      the order of the rows;
    \item Be aware of floating-point precision issues when comparing real numbers.
  \end{itemize}
\end{hlbox}

\subsection{Selecting columns}

Selecting is the process of choosing a subset of columns from a dataset.  The remaining
columns are discarded.  This operation is not reversible, as the discarded columns are
lost.  Rows remain unchanged.

There are two main ways to select columns: by name or by predicate.  The former is the
most common and is used to select a fixed set of columns.  The latter is used to select
columns that satisfy a given condition, i.e., the values in the columns are used to
determine which columns should be selected.

When selecting columns by name, one can use a list of column names or a regular
expression\footnote{Regular expressions are very general and powerful, but they are also
complex and error-prone.  An alternative is to use some form of hierarchical naming,
such as \texttt{type.column} to express groups of columns.}.
The latter is useful when the column names follow a pattern that reflects the semantics of
the columns.  For instance,
one can use the regular expression \texttt{col[0-9]+} to select all columns whose names
start with \texttt{col} followed by one or more digits.

When selecting columns by predicate, one can use a function that returns a logical value
to define the condition under which a column should be selected.  For instance, one can
use the predicate \texttt{isnumeric} to select all columns that contain numeric values.
Notice, however, that the predicate is applied to each column independently and returns a
single logical value for each column.

Like filtering, selecting predicates might contain aggregation functions.  Although it is
theorically possible to consider the order of the values in the columns, it is not common
to do so.  (Especially because one would need to assume that the rows are previously
sorted by some criterion.) Groups, however, never make sense in this context, once the
predicate is applied to each column independently.

Depending on the context, it may be useful to ``drop'' columns instead of selecting them.
This is the same as selecting all columns except the ones specified.  This is useful when
the number of columns to be dropped is small compared to the total number of columns.
Strictly speaking, we just need to negate the predicate or the regular expression used to
select the columns.

Finally, it is very common to find libraries and framework in which the order of the
columns is important.  As a result, columns can be selected by position as well.
I find this practice error-prone and I recommend avoiding it whenever possible.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use selecting to remove columns that are not relevant to your analysis;
    \item Use column names or regular expressions (or hierarchical names) to select columns;
    \item Use predicates (many to one, with no aggregation functions) to define the conditions
      under which columns should be selected;
    \item Avoid depending on the order of the columns.
  \end{itemize}
\end{hlbox}

\subsection{Mutating columns}

Mutating is the process of creating new columns.  The operation is reversible, as the
original columns are kept.  The new columns are added to the dataset.

The values in the new column are determined by an expression.  The expression is a
function that returns a vector of values given the values in the other columns.  The
expression can be a simple function, such as \texttt{y = x + 1}, or a more complex
function, such as \texttt{y = ifelse(x > 0, 1, 0)}.  Here, \texttt{x} and \texttt{y} are
the names of an existing and the new column, respectively.

One may also use an aggregation and window function in the expression. This is particularly
useful when performing mutation considering a group.  In this case, the returned value is
repeated (aggregation function) for each row of the same group.  Like in filtering, the
more explicit you can be about order and groups, the better.

For example, the expression \texttt{y = cumsum(x) group by category sort by date} will
create a new column \texttt{y} with the cumulative sum of the \texttt{x} column for each
\texttt{category} given the order of the rows defined by the \texttt{date} column.

Sometimes, the same expression can be used to create multiple columns.  This is useful
when the new columns are related.  To do so, one first specifies the columns in the same way as
when selecting columns.  Then, one needs to specify a rule to name the new columns.
For instance, \texttt{x\_new = x + 1 across x matches \textasciicircum{}col[0-9]+\$}.

Practically speaking, mutation can overwrite existing columns.  This is useful when the
new column is a replacement for the old one.  Formally, overwriting is just a sequence of
mutation and selection operations.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use mutating to create new columns that are relevant to your analysis;
    \item Use expressions to define the values of the new columns;
    \item Use aggregation and window functions in the expression to create new columns based on
      groups and order;
    \item Use the same expression to create multiple columns when the new columns are related.
  \end{itemize}
\end{hlbox}

\subsection{Aggregating rows}

We can aggregate the rows of a dataset to create a new dataset with fewer rows.    The
operation is not reversible, as the discarded rows are lost.  The columns are also lost,
only the new aggregate columns remain.

The values in the new columns are determined by an aggregation function.  Like filtering
and mutation, the aggregation function can be parametrized by specifying a group and/or an
order.

The resulting dataset will contain one row for each group.  The values in the new columns
are determined by the aggregation function applied to the values in the other columns.
All columns that define the groups are usually kept in the resulting dataset.  In this
case, as expected, values of such columns are equal for all rows in the same group.

For instance, the aggregation function \texttt{mean(x) group by category} will create a
new dataset with one row for each different value of \texttt{category} and a new column
with the mean of the \texttt{x} column for each group.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use aggregation to summarize the data in a dataset;
    \item Use aggregation functions to define the values of the new columns;
    \item Other columns are lost;
    \item Use the group and order parameters to define the groups and the behavior of the
      aggregation function.
  \end{itemize}
\end{hlbox}

\subsection{Binding datasets}

One trivial, yet important, operation is to bind datasets.  This is the process of
combining two or more datasets into a single dataset.  The operation is reversible, as the
original datasets are kept.  The new dataset contains all the rows and columns of the
original datasets.

There are two ways to bind datasets: by rows or by columns.  The former is used to
combine datasets that have exactly the same columns but represent different parts of the
same dataset.  The latter is used to combine datasets that comprise the same observations
(rows) but captures different aspects of the same dataset.

When binding datasets by rows, the datasets must have the same columns\footnote{In
practice, it is usually required that they share the same order of the columns as well.
This is not a theoretical requirement, but a common limitation of most libraries.}.
The resulting dataset will contain all the rows of the original datasets.  The columns
remain unchanged.  It is a good practice to create a new column that represents the source
of each row.  For instance, if each table represents data collected in a different year,
one can create a new column \texttt{year} that contains the year of the data.

When binding datasets by columns, the datasets must have the same number of rows.  Each
matching row represent the same observation\footnote{Practically speaking, either the
order of the rows or a key column is used to match the rows of the datasets.  In both
situations, this is equivalent to a join operation by the row number or the key column;
assuming that both datasets contains the same observations.}. The resulting dataset will
contain all the columns of the original datasets.  The rows remain unchanged.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use binding to combine datasets that represent different parts of the same dataset;
    \item Use binding by rows to combine datasets that have the same columns --- in this
      case, create a new column that represents the source of each row;
    \item Use binding by columns to combine datasets that have the same number of rows.
  \end{itemize}
\end{hlbox}

{\color{red} Talk about splitting as the reverse function, and the reason why missing
columns may be a problem. Example of the unit of measurement.}

\subsection{Joining datasets}

Joining is the process of combining two datasets into a single dataset based on common
columns.  The operation may not be reversible, consult \cref{sec:normalization} for more
details.

The join of two tables is the operation that returns a new table with the columns of both
tables.  Let \texttt{U} be the common set of columns.  For each occurring value of
\texttt{U} in the first table, the operation will look for the same value in the second
table.  If it finds it, it will create a new row with the columns of both tables.  If it
does not find it, no row will be created.  This operation assumes that values in \texttt{U}
are unique in each table.

The variation described above is usually called natural or inner join.  Three other
variations are possible.
\begin{itemize}
  \item Left join: for each occurring value of \texttt{U} in the first table, the operation
    will look for the same value in the second table.  If it finds it, it will create a new
    row with the columns of both tables.  If it does not find it, it will create a new row
    with the columns of the first table and missing values for the columns of the second
    table.
  \item Right join: the same as the left join, but the roles of the tables are reversed.
  \item Outer join: for each different value of \texttt{U} in both tables, the operation
    will create a new row with the columns of both tables.  If a value is missing in one
    table, it will be filled with a missing value.
\end{itemize}

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use joining to integrate datasets;
    \item Be aware of the risks of joining datasets (\cref{sec:normalization}), for
      example, that some joins may create invalid rows;
    \item Use the appropriate variation of the join operation in applications.
  \end{itemize}
\end{hlbox}

\subsection{Pivoting and unpivoting}

Another important operation is to pivot and unpivot datasets.  These are the processes of
transforming a dataset from a long format to a wide format and vice versa.  The operations
are reversible and they are the inverse of each other.

Pivoting requires to specify a name column --- whose discrete and finite possible values
will become the names of the new columns --- and a value column --- whose values will be
spread across the rows.  All remaining columns are considered to be keys, uniquely
identifying each row of new the dataset.

Unpivoting\footnote{Which \citeauthor{Wickham2023} call pivot longer.} is the reverse
operation.  One must specify all the columns whose names are the values of the before
called name column.  The values of these columns will be gathered into a new column.
As before, all remaining columns are considered to be keys.

In practical applications, where not all remaining columns are keys, one must aggregate
rows beforehand.

\begin{tablebox}[label=tab:pivot]{Pivoting example.}
  \begin{minipage}{0.45\textwidth}
    \centering
    \rowcolors{2}{black!10!white}{}
    \begin{tabular}{ccc}
      \toprule
      \texttt{name} & \texttt{year} & \texttt{value} \\
      \midrule
      A & 2019 & 1 \\
      A & 2020 & 2 \\
      A & 2021 & 3 \\
      B & 2019 & 4 \\
      B & 2020 & 5 \\
      B & 2021 & 6 \\
      \bottomrule
    \end{tabular}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \centering
    \rowcolors{2}{black!10!white}{}
    \begin{tabular}{cccc}
      \toprule
      \texttt{name} & \texttt{2019} & \texttt{2020} & \texttt{2021} \\
      \midrule
      A & 1 & 2 & 3 \\
      B & 4 & 5 & 6 \\
      \bottomrule
    \end{tabular}
  \end{minipage}
  \tcblower
  The left table is in the long format and the right table is in the wide format.  The
  name column is \texttt{year} and the value column is \texttt{value}.
\end{tablebox}

\Cref{tab:pivot} shows an example of pivoting.  The left table is in the long format and
the right table is in the wide format.  The name column is \texttt{year}, the value column
is \texttt{value}, and the remaining column is \texttt{name} which is an unique identifier
of the rows in the wide format.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use pivoting to transform datasets from a long format to a wide format;
    \item Use unpivoting to transform datasets from a wide format to a long format;
    \item Be aware of the need to aggregate rows before unpivoting.
  \end{itemize}
\end{hlbox}

\subsection{An algebra for statistical transformations}

In recent years, some researchers made an effort to create a formal algebra for
statistical transformations.  The idea is to create a set of operations that can be
combined to create complex statistical transformations.  This is similar to the idea of
relational algebra, which is a set of operations that can be combined to create complex
queries.

The difference between relational algebra and a formal algebra for statistical
transformations is that the latter is more complex.  This is because statistical
transformations are more complex than queries.  For instance, the concept of missing data
is not present in relational algebra, but it is in statistical transformations.

\textcite{Song2021}, for example, propose a formal paradigm for statistical data
transformation.  They present a data model, an algebra, and a formal language.  Their goal
is to create a standard for statistical data transformation that can be used by different
statistical software.

However, in my opinion, the major deficiency of their work is that they mostly try to
``reverse engineer'' the operations that are commonly used in statistical software.  This
is useful for the translation of code between different software, but it is not productive
to advance in the theoretical understanding of statistical transformations.

If one ought to tackle the challenge of formally expressing statistical transformations, I
think one should start from the basic operations.  Basic operations mean that they are
irreducible, i.e., they cannot be expressed as a sequence of other operations.

Some thoughts about it:
\begin{itemize}
  \item Binding columns can be expressed as a join operation, thus it is not a basic
    operation.
  \item Some software provide features that can be better expressed in other (often simpler) ways.  Row
    naming is an example.  It is useful to keep track of the origin of each row, but names
    can be just another column.  I argue for excluding row naming in a formal algebra.
  \item Some operations are very useful and recurring, even if they are not basic.  Such
    operations must be omitted from the formal algebra for the sake of simplicity.
    However, any software that implements a language for the formal algebra can provide
    syntax sugar for these operations.
  \item Not defining your algebra in terms of a specific programming language is a good
    practice.  This is because the algebra is a theoretical concept and should be
    independent of any implementation.  It also gives opportunities to rethink the
    things that commonly done in a specific way.  This can lead to new insights and
    correct error-prone practices.
  \item Pivoting seems to be ``different'' enough to the other operations to be considered
    in the set of basic operations.  However, it is not hard to see that they can be
    rewritten as combinations with the meta tables containing the possible values of the
    attributes (or some sort of aggregation function).
\end{itemize}

\section{Data handling pipeline}

Before we study the data handling tasks, we need to understand that a data handling
pipeline is a sequence of operations that \emph{does} depend on the input data.
This might seem obvious, but the implications are not.

A common error in data handling is to perform operations ad hoc, usually leading to data
leakage.  For instance, one might impute missing values before splitting the data into
training and testing sets.  This is a mistake because the imputation is based on the
entire dataset, including the testing set.

To avoid this kind of error, one must declare\footnote{This is the declarative nature of
data handling operations.} the operations that will be performed on the data before
applying them.  This is usually done by creating the full data handling pipeline
beforehand.

The pipeline, like a model, must be ``fitted'' to the data.  This means that parameters of
the operations are not fixed until the first data is given as input.  Subsequent data fed
to the pipeline will be handled keeping the first ``learned'' parameters.

Consider the following example.  Suppose we have a dataset with missing values for
variable \texttt{A}.  We want to impute the missing values and then standardize
\texttt{A}.  The pipeline is created as follows:  \texttt{D -> impute\_zero(A) ->
standardize(A)}.

The operation \texttt{impute\_zero(A)} is parametrized by the value 0, which, in this case,
is fixed.  However, the operation \texttt{standardize(A)} is parametrized by the mean and
the standard deviation of the values in \texttt{A}.  These values are not fixed until the
first data is given as input.

\begin{hlbox}{A note about fixed parameters}
  Even if your data handling pipeline contains operations that have fixed parameters and
  can be safely applied to data before the model search, \emph{I strongly recommend} that
  you declare the pipeline as a whole.  This is because it is easier to maintain and
  reproduce the data handling process, especially in deployment.  Performing ad hoc
  handling in your data is a source of errors and important transformations can be
  forgotten when receiving new data.
\end{hlbox}

In a practical scenario, the source code of the \emph{model search} method includes not
only strategies for the model, but also the data handling pipeline.  Moreover, the deployment
of the model includes the data handling pipeline as well.  In other words, it does not
matter which model is used, in the example above, the mean and the standard deviation of
the values in \texttt{A} should be stored and used in deployed models.

In terms of reproducibility and validation, having a single consolidated pipeline is
crucial.

\begin{hlbox}{A note about ``filtering'' operations}
  Some operations may conditionally remove rows from the dataset.  For instance, after
  observing that there exists few missing values in an important column, one might decide
  to remove rows with missing values in it.  In production, this means that some new
  observations might be discarded before reaching the model itself.  However, the user
  still expects an answer from the model.  In this case, one must define either a default
  value for the answer or a default behavior to handle discarded examples.
\end{hlbox}

% \begin{slidebox}{Data handling pipeline}{}
%   \begin{itemize}
%     \item A data handling pipeline is a sequence of operations that depend on the input data;
%     \item The pipeline must be declared before applying the operations;
%     \item The pipeline is fitted to the data;
%     \item The selected pipeline is part of the model search and deployment.
%     \item Even operations that have fixed parameters or that can be safely applied to data
%       before the model search should be declared in the pipeline.
%   \end{itemize}
% \end{slidebox}

\textcolor{red}{XXX: maybe state that before reaching the pipeline data is already tidy,
this way simple integration (not enhancement), pivoting and aggregating are kept outside
the pipeline. These operations must depend only on variable names and not variable values.}

\section{Data transformation}

The first task in data handling is data transformation.  This is the process of adjusting
the format and the types of the data to make it suitable for analysis.

Usually, the starting point of data transformation is to make the data tidy, i.e., to have
each variable in a column and each observation in a row.  Remember that, depending on the
problem definition, we target a particular observational unit.  Having a clear picture of
the observational unit is important to define the columns and the rows of the dataset.

Then, when the data format is acceptable, we can perform a series of operations to make the
column's types and values suitable for modeling.  The reason for this is that most
machine learning methods require the input variables to follow some restrictions.  For
instance, some methods require the input variables to be real numbers, others require the
input variables are in a specific range, etc.

\subsection{Reshaping}

\textcolor{red}{TODO: pipeline exceptions: like pivoting and aggregating are kept outside
the pipeline.}

Reshaping is the process of changing the format of the data.  The most common reshaping
operations are pivoting and unpivoting, which we have already discussed.  However, there
are other reshaping operations that are useful in practice.

For instance, one can reshape a dataset by splitting a column into multiple columns.  This
is useful when a column contains multiple values that should be separated.  This can be
done with mutation with appropriate expressions.  Some frameworks might provide special functions
to do this, usually called splitting functions.

We can also consider reshaping the operations of filtering, selecting, and aggregating.
Filtering is usually done to reduce the scope of the data, given some conditions on the
variables.  Selecting is usually done to remove irrelevant variables or highly correlated
ones.  Aggregating in a reshaping task is usually applied together with pivoting to change the
observational unit of the dataset.

% \begin{slidebox}{Reshaping}{}
%   \begin{itemize}
%     \item Reshaping is the process of changing the format of the data;
%     \item The most common reshaping operations are pivoting and unpivoting;
%     \item Other common operation include:
%       \begin{itemize}
%         \item Splitting a column into multiple columns;
%         \item Filtering to reduce the scope of the data;
%         \item Selecting to remove irrelevant variables or highly correlated ones;
%         \item Aggregating to change the observational unit of the dataset.
%       \end{itemize}
%   \end{itemize}
% \end{slidebox}

\subsection{Type conversion}

Type conversion is the process of changing the type of the values in the columns.  This
is usually done to make the data suitable for modeling.  For instance, some machine
learning methods require the input variables to be real numbers.

The most common type conversions are from categorical to numerical and from numerical to
categorical.  The former is usually done by creating dummy variables, i.e., a new column
for each possible value of the categorical variable.  This transformation is also known as
one-hot encoding.  The latter is usually done by binning (discretizing) the numerical variable, either by
frequency or by range.

% \begin{slidebox}{Type conversion}{}
%   \begin{itemize}
%     \item Type conversion is the process of changing the type of the values in the columns;
%     \item Use one-hot encoding to convert categorical variables to numerical;
%     \item Use binning to convert numerical variables to categorical.
%   \end{itemize}
% \end{slidebox}

\subsection{Normalization}

Normalization is the process of scaling the values in the columns.  This is usually done to
keep data in a specific range or to make the data comparable.  For instance, some machine
learning methods require the input variables to be in the range $[0, 1]$.

The most common normalization methods are standardization and rescaling.  The former is done
by subtracting the mean and dividing by the standard deviation of the values in the column.
The latter is performed so the values are in a specific range, usually $[0, 1]$ or $[-1, 1]$.

\begin{hlbox}{Clamping after rescaling}
  In production, it is common to clamp the values after rescaling.  This is done to avoid
  the model to make predictions that are out of the range of the training data.
\end{hlbox}

Related to normalization is the log transformation.  This is usually done to make the data
more symmetric or to reduce the effect of outliers.  The log transformation is the process
of taking the logarithm of the values in the column.

% \begin{slidebox}{Normalization}{}
%   \begin{itemize}
%     \item Normalization is the process of scaling the values in the columns;
%     \item Use standardization to make the values have mean 0 and standard deviation 1;
%     \item Use rescaling to make the values be in a specific range;
%     \item Use the log transformation to make the data more symmetric or to reduce the effect
%       of outliers.
%   \end{itemize}
% \end{slidebox}

\subsection{Sampling}

Sampling is the process of selecting a random subset of the data.  This is usually done to
reduce the size of the data or to create a balanced dataset.  For instance, some machine
learning methods are heavily affected by the number of observations in each class.
Also, some methods are computationally expensive and a smaller dataset might be enough to
solve the problem.

The most common sampling methods are random sampling and resampling\footnote{Resampling is
the process of sampling with replacement, sometimes called bootstrapping.}.  The former is
done by selecting a random subset of the data.  The latter is done by selecting a random
subset of the data with replacement.

While random sampling is useful to reduce the size of the data, resampling can be used to
increase the size of the data.  (Although this has some caveats.)  Moreover, resampling
can also create variations of the original dataset with the same distribution of the
values.

\subsection{Dimensionality reduction}

Dimensionality reduction is the process of reducing the number of variables in the data.
This is usually done to reduce the complexity of the model or to identify irrelevant
variables.  The so-called \emph{curse of dimensionality} is a common problem in machine
learning, where the number of variables is much larger than the number of observations.

There are two main types of dimensionality reduction algorithms: feature selection and
feature extraction.  The former is done by selecting a subset of the variables that leads
to the best models.  The latter is done by creating new variables that are combinations
of the original ones.

Feature selection can be performed before modeling (filter), together with the model
search (wrapper), or as a part of the model itself (embedded).

Feature extraction is usually done by linear methods, such as principal component analysis
(PCA), or by non-linear methods, such as autoencoders.  These methods are able to
compress the information in the data into a smaller number of variables.

% \begin{slidebox}{Dimensionality reduction}{}
%   \begin{itemize}
%     \item Dimensionality reduction is the process of reducing the number of variables in the data;
%     \item Use feature selection to select a subset of the variables that leads to the best models;
%     \item Use feature extraction to create new variables that are combinations of the original ones.
%   \end{itemize}
% \end{slidebox}

\begin{hlbox}{Practice!}
  Can you identify which data transformation operations are used to make datasets
  presented in \cref{chap:data} tidy?
\end{hlbox}

\section{Data cleaning}

Data cleaning is the process of removing errors and inconsistencies from the data.  This is
usually done to make the data more reliable and to avoid bias in the analysis.

\subsection{Dealing with missing data}

Since most models do not cope with missing data, it is crucial to deal with it in the data
handling pipelines.

There are four main strategies to deal with missing data:
\begin{itemize}
  \item Remove the rows with missing data;
  \item Remove the columns with missing data;
  \item Impute the missing data;
  \item Use an indicator variable to mark the missing data.
\end{itemize}

Removing rows and columns are commonly used when the number of missing data is small
compared to the total number of rows or columns.  However, be aware that removing rows can
artificially change data distribution, especially when the missing data is not missing at
random.

Imputing the missing data is usually done by replacing the missing values with some
statistic of the available values in the column, such as the mean, the median, or the
mode.  This is a simple and effective strategy, but it can introduce bias in the data.
Also, it is not suitable when one is not sure whether the missing data is missing because
of a systematic error or phenomenon.

For this case, creating an indicator variable is a good strategy.  This is done by creating
a new column that contains a logical value indicating whether the data is missing or
not\footnote{Some kind of imputation is still needed, but we expect the model to deal
better with it}.  By doing so, the model can learn the importance of the missing
data\footnote{\color{red}Sometimes the indicator variable is already present: pregnancy and sex
example.}.

\subsection{Dealing with invalid and inconsistent information}

Sometimes, during data collection, information is recorded using special codes.  For
instance, the value 9999 might be used to indicate that the data is missing.  Such codes
must be replaced with more appropriate values before modeling.

Another common problem is inconsistent information.  For instance, the same category might
be represented by different names.  This is usually done by creating a dictionary that
maps the different names to a single one.

It is also useful to check whether all columns that store physical quantities have the
same unit of measurement.  If not, one must convert the values to the same unit.

If one knows that a variable has a specific range of values, it is useful to check
whether the values are within this range.  If not, one must replace the values wit
missing data or with the closest valid value.

\subsection{Outliers}

Outliers are observations that are significantly different from the other observations.
They can be caused by errors in the data collection process or by the presence of a
different phenomenon.  In both cases, it is important to deal with outliers before
modeling.

There are many outliers detection methods, consult TODO.

% \begin{slidebox}{Data cleaning}{}
%   \begin{itemize}
%     \item Data cleaning is the process of removing errors and inconsistencies from the data;
%     \item Use the following strategies to deal with missing data:
%       \begin{itemize}
%         \item Remove the rows with missing data;
%         \item Remove the columns with missing data;
%         \item Impute the missing data;
%         \item Use an indicator variable to mark the missing data.
%       \end{itemize}
%     \item Replace special codes with more appropriate values;
%     \item Create a dictionary to map different names to a single one;
%     \item Check whether all columns that store physical quantities have the same unit of
%       measurement;
%     \item Check whether the values are within the expected range;
%     \item Use outlier detection methods to deal with outliers.
%   \end{itemize}
% \end{slidebox}

\section{Data integration}

Data integration is the process of combining data from different sources into a single
dataset.  This is usually done to create a more complete dataset or to create a dataset
with a different observational unit.

To perform integration, consider the discussions in \cref{sec:normalization,sub:bridge}.

Additionally, one must consider the following points:
\begin{itemize}
  \item Sometimes the same column may have different names in different datasets.  Redundant
    columns must be removed.
  \item Separate datasets that share the same variables usually happen because there is a
    hidden variable that is not present in the datasets.  During integration, the new
    variable must be created.
\end{itemize}

% \begin{slidebox}{Data integration}{}
%   \begin{itemize}
%     \item Data integration is the process of combining data from different sources into a single dataset;
%     \item Not every join is possible, consider the discussions in \cref{sec:normalization,sub:bridge};
%     \item Remove redundant columns;
%     \item Create new variables to represent the hidden variables.
%   \end{itemize}
% \end{slidebox}

\textcolor{red}{Hard to incorporate in the pipeline when joins only, but data enhancement
works better inside the pipeline.}

% vim: spell spelllang=en
