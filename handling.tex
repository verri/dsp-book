\chapter{Data handling}
\label{chap:handling}

\chapterprecishere{%
  {\fontspec[Scale=2]{Symbola}\color{black!80}\symbol{"1F5E1}}
  It's dangerous to go alone! Take this.
  \par\raggedleft--- \textup{Unnamed Old Man}, The Legend of Zelda}

% Important: avoid the term "data manipulation" as it has a negative connotation

In the previous chapter, I discussed the relationship between data format and data
semantics.  We also saw in \cref{chap:project} that data tasks --- specifically
integration and tidying --- must adjust the data to reflect the kind of
input we expect in production.

For those tasks, we must be careful with the operations we perform on the data. At the
stage of data preparation, for example, we should never parametrize our data handling
pipeline in terms of information retrieved\footnote{For instance, imputation by the mean
of a column.} by sampling the data.  This is because such operations lead to \gls{leakage}.

In this chapter, we consider that tables are rectangular data structures in which values
of the same column share the same properties (i.e. the same type, same restrictions, etc.)
and each column has a name.  Moreover, we assume that any value is possibly
\emph{missing}.

From a mathematical definition of such tables, we can define a set of operations that can
be applied to them.  These operations are the building blocks of data handling tasks.
I highlight some important properties of them.

I show how these operations can be combined to create complex data handling pipelines by
using them to solve the issues presented in \cref{sub:messy}.

\begin{mainbox}{Chapter remarks}

  \boxsubtitle{Contents}

  \startcontents[chapters]
  \printcontents[chapters]{}{1}{}
  \vspace{1em}

  \boxsubtitle{Context}

  \begin{itemize}
    \item \dots
  \end{itemize}

  \boxsubtitle{Objectives}

  \begin{itemize}
    \item \dots
  \end{itemize}

  \boxsubtitle{Takeways}

  \begin{itemize}
    \item \dots
  \end{itemize}
\end{mainbox}

{}
\clearpage

\section{Formal structured data}

\newcommand{\domainof}[1]{\mathcal{D}\!\left(#1\right)}
\newcommand{\missing}{\text{?}}
\newcommand{\rowcard}[1][k_1, \dots, k_k]{\operatorname{card}\!\left(#1\right)}

In this section, I present a formal definition of structured data.  This definition is
compatible with the relational model and tidy data presented in \cref{chap:data}.
My definition takes into account the index\footnote{Also called grouping variables.} of
the table, which is a key concept in data handling.  We also consider that values can be
missing.  Repeated values are represented by allowing cells to contain sets of values.

So, an indexed table $T$ is a tuple $(K, H, c)$, where $K = \left\{K_i : i = 1, \dots,
k\right\}$ is the set of index columns, $H$ is the set of (non-index) columns, and $c :
\domainof{K_1} \times \dots \times \domainof{K_k} \times H \to \mathcal{V}$ is the cell function.
Here, $\mathcal{V}$ represents the space of all possible tuples of values, which
may include missing values $\missing$.  Values have arbitrary types, such as integers,
real numbers, strings, etc.

Each index column $K_i$ has a domain $\domainof{K_i}$, which is an enumerable set of
values. A possible row $r$ of the table is thus indexed by a tuple $r = (k_1, \dots,
k_k)$, where $k_i \in \domainof{K_i}$.  Each row has a cardinality $\rowcard[r]$, which
represents how many times the entity represented by the row is present in the table.
A row $r$ with $\rowcard[r] = 0$ is considered to be missing.

A cell is then represented by a row $r$ and a column $h \in H$.  The value of the cell,
$\vec{v} = c(r, h)$ is a tuple of values such that $|\vec{v}| = \rowcard[r]$.
The value matrix $V$ of the cell $r$ is \[
  \Big[ c(r, h) : h \in H \Big]\text{,}
\] with dimensions $\rowcard[r] \times |H|$, assuming an arbitrary fixed order of the
columns.

We assume that value matrices --- and consequently row cardinalities --- are minimal. This
means that there are no \emph{nested row} $v_{i, 1}, \dots, v_{i, |H|}$ in the value matrices
such that $v_{i, j} = \missing$ for all $j$.

From these concepts, we can define the basic operations and properties that can be applied
to tables.

\subsection{Splitting and binding}

Given an indicator function $s : \domainof{K_1} \times \dots \times \domainof{K_k} \to
\left\{0, 1\right\}$, the split operation creates two tables, $T_0$ and $T_1$, that
contains only the rows for which
$s(r) = 0$ and $s(r) = 1$, respectively.

Mathematically, the split operation is defined as \[
  \operatorname{split}(T = (K, H, c),\, s) =
    \left(T_0 = (K, H, c_0), T_1 = (K, H, c_1)\right)\text{,}
\] where $c_i(r, h) = c(r, h)$ if $s(r) = i$ and $c_i(r, h) = ()$ otherwise.

\emph{Note that, by definition, the split operation never ``breaks'' a row.  So, the
indices define the indivisible entities of the table.}

The binding operation is the inverse of the split operation.  Given two tables $T_0 = (K,
H, c_0)$ and $T_1 = (K, H, c_1)$, such that $\rowcard[r; c_0] = 0$ if $\rowcard[r; c_1] >
0$ for any row $r$, and vice-versa, the binding operation creates a new table $T$ that
contains all the rows of $T_0$ and $T_1$.

Mathematically, the binding operation is defined as \[
  \operatorname{bind}(T_0 = (K, H, c_0),~T_1 = (K, H, c_1)) =
    T = (K, H, c)\text{,}
\] where $c(r, h) = c_0(r, h) + c_1(r, h)$ and $+$ is the tuple concatenation
operator\footnote{The order of the concatenation here is not an issue since we guarantee
that at least one of the operands is
empty.}.

\paragraph{Premises in real-world applications}

One important aspect of these functions is that we assume that the entities represented by
the rows are indivisible, and that any binding operation will never occur for tables that
share the same entities.

In real-world applications, this is not always true.  Many times, we do not know the
process someone else has used to collect the data.  In these cases, we must be careful
about the guarantees we explain in this chapter.  On the other hand, one can consider the
premises we use as a guideline to design good data collection processes.

We can see data collection as the result of a splitting operation in the universe set of
all possible entities.  This is a good way to think about data collection, as we can try
to ensure that we collect all possible information about the entities we are interested
in.

This, of course, depends on what we define as the index columns of the table.  Consider
the example of collecting information about grades of students.  If we define as the index
columns the student's name and year, we must ensure that we collect all the grades of all
subjects a student has taken in a year.  We do not need, though, to collect information
from all students or all years.  On the other hand, if we define as the index column only
the student's name, we must collect all the grades of all subjects a student has taken in
all years.

In summary, the less variables we define as index columns, the more information we must
collect about each entity.  However, in the next sections, we show that assuming many index columns
lead to restrictions in the operations we can perform on the table.

This conceptual trade-off is important to understand when structuring the problem we are
trying to solve.  Neglecting these issues can lead to strong statistical biases and
incorrect conclusions.

\subsection{Split-invariance}

An arbitrary data handling operation $f(T)$ over table $T$is said to be split-invariant
if, for any split function $s$, the following equation holds \[
  f\!\left(T\right) =
    \operatorname{bind}\!\left(f\!\left(T_0\right), f\!\left(T_1\right)\right)\text{,}
\] where $T_0, T_1 = \operatorname{split}\!\left(T; s\right)$.

Split-invariance is a desirable property for data handling operations during the data
tasks described in \cref{chap:project}: integration and tidying.  Even while exploring
data, we should take effort to use split-invariant operations.

The reason is that split-invariance ensures that the operation does not depend on the
sampling performed (usually unknown to us) to create the table we have in hand.  This
property is important to avoid \gls{leakage} or to bias the results of the analysis.

\subsection{Illustrative example}

Consider the example of student grades.

\begin{tablebox}[label=tab:grades1]{Data table of student grades.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{cccc}
    \toprule
    \texttt{student} & \texttt{subject} & \texttt{year} & \texttt{grade} \\
    \midrule
    Alice & Chemistry & 2020 & 6 \\
    Alice & Math & 2019 & 8 \\
    Alice & Physics & 2019 & 7 \\
    Bob & Chemistry & 2019 & 7 \\
    Bob & Math & 2019 & 9 \\
    Bob & Physics & 2019 & 4 \\
    Bob & Physics & 2020 & 8 \\
    Carol & Biology & 2020 & 8 \\
    Carol & Chemistry & 2020 & 3 \\
    Carol & Math & 2020 & 10 \\
    \bottomrule
  \end{tabular}
  \tcblower
  Data collected about student grades.
\end{tablebox}

\begin{tablebox}[label=tab:grades2]{Data table of student grades assuming student and subject as indices.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{cccc}
    \toprule
    \texttt{student} & \texttt{subject} & \texttt{year} & \texttt{grade} \\
    \midrule
    Alice & Biology & () & () \\
    Alice & Chemistry & (2020) & (6) \\
    Alice & Math & (2019) & (8) \\
    Alice & Physics & (2019) & (7) \\
    Bob & Biology & () & () \\
    Bob & Chemistry & (2019) & (7) \\
    Bob & Math & (2019) & (9) \\
    Bob & Physics & (2019, 2020) & (4, 8) \\
    Carol & Biology & (2020) & (8) \\
    Carol & Chemistry & (2020) & (3) \\
    Carol & Math & (2020) & (10) \\
    Carol & Physics & () & () \\
    \bottomrule
  \end{tabular}
  \tcblower
  Indexed table with data from \cref{tab:grades1} assuming \texttt{student} and
  \texttt{subject} as indices.
\end{tablebox}

We cannot assume that Alice has not taken Biology, since this can be a missing value.

Talk about Bob and Physics.

We safely assume that Carol has failed Chemistry and gave up.  Changing year as part of
our index would lead to a different conclusion.

\section{Data handling operators}

In the literature and in software documentation, you will find a variety of terms used to
describe data handling operations\footnote{%
  The terminology ``data handling'' itself is not universal.  Some authors and libraries
  call it ``data manipulation'', ``data wrangling'', ``data shaping'', or ``data
  engineering''.  I use the term ``data handling'' because it seems more generic.
  Also, it avoids confusion with the term ``data
  manipulation'' which has a negative connotation in some contexts.}. %
They often refer to the same or similar operations, but the terminology can be confusing.
In this section, I present a summary of these operations mostly based on
\textcite{Wickham2023} definitions\footnote{Which are called \emph{verbs}.}.

% \begin{slidebox}{Data handling operators}{}
%   \begin{itemize}
%     \item Filtering rows;
%     \item Selecting columns;
%     \item Mutating columns;
%     \item Aggregating rows;
%     \item Binding datasets;
%     \item Joining datasets;
%     \item Pivoting (spreading) and unpivoting (gathering) datasets.
%   \end{itemize}
% \end{slidebox}

These operations are the building blocks of the data handling tasks we will discuss in the
next sections.  They can also be extensively parametrized and combined to create more
elaborate data handling pipelines.  For instance, most of them can use predicates to
define the groups, arrangements, or conditions under which they should be applied.

We use the following terminology to refer to the data handling parameters:
\begin{itemize}
  \item \textbf{Predicate}: a function that returns a logical value, used to filter
    rows/columns or to define the groups of rows/columns to be processed;
  \item \textbf{Aggregation function}: a function that returns a single value given a vector
    of values (in which, the order of the values may be important);
  \item \textbf{Window function}: a function that returns a vector of values given a vector
    of values in which, the order of the values is important;
  \item \textbf{Expression}: a function that returns a vector of values element-wise, used to create new
    columns or to modify existing ones.
\end{itemize}

% \begin{slidebox}{Data handling pipelines}{}
%   \begin{itemize}
%     \item Data handling operations can be combined to create complex pipelines;
%     \item Operators may be reversible;
%     \item Operators are vectorized;
%     \item They can be parametrized with predicates, aggregation functions, and expressions;
%     \item They operate on datasets and return new datasets as output.
%     \item They are declarative.
%   \end{itemize}
% \end{slidebox}

Operators are also vectorized, meaning that they can be applied to multiple columns or
rows at once.  This is a key feature of data handling operations, as it allows for
expressive and efficient data manipulation.

Many of them are also reversible, meaning that they can be undone.  This is important
because it allows for reproducibility and traceability of the data handling process.

They operate on a dataset (or more than one) given as input and return a new dataset as
output.  This is important because it allows for the creation of data handling pipelines,
where the output of one operation is the input of the next one.  Parameters like column
names, predicates, aggregation functions, and expressions can be passed to these operations to
customize their behavior.

Unlike traditional procedural programming, where conditional statements and loops are used
to manipulate data, data handling operations are declarative.  This means that they are
expressed in terms of what should be done, not how it should be done.  This is a powerful
abstraction that allows for the creation of complex pipelines with a few lines of code.

\subsection{Filtering rows}

Filtering is the process of selecting a subset of rows from a dataset based on a
predicate.  If more than a single predicate is used, they are combined using a logical
operator, such as \texttt{AND} or \texttt{OR}.

After filtering, the dataset will contain only the rows that satisfy the predicate.
Columns remain unchanged.  This operation is potentially irreversible, as the removed
rows are lost.

In the basic form, each row is treated independently.  For instance, the predicate
\texttt{age > 18} will select all rows where the value in the \texttt{age} column is
greater than 18.

However, if the predicate depends on an aggregation or window function, one must specify
the groups and/or the order of the rows.  For instance, the predicate \texttt{age >
mean(age) group by country} will select the rows where the value in the \texttt{age}
column is greater than the mean of the \texttt{age} for each \texttt{country}. Another
example is the predicate \texttt{cumsum(price) < 100 sort by date}, which selects the rows
that satisfy the condition that the cumulative sum of the \texttt{price} column is less
than 100 given the order of the rows defined by the \texttt{date} column.

The trivial group is the entire dataset, so it is usually not necessary to specify it
explicitly.  However, it is usually not sensible to not specify the order of the rows.

When dealing with real values, be aware of floating-point precision issues.  In other
words, do not use the equality operator to compare real numbers.  Most of libraries
provide operators to compare real numbers within a given tolerance.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use filtering to remove rows that are not relevant to your analysis;
    \item Use predicates to define the conditions under which rows should be removed;
    \item When aggregation functions are needed to define the predicate, specify the groups and
      the order of the rows;
    \item Be aware of floating-point precision issues when comparing real numbers.
  \end{itemize}
\end{hlbox}

\subsection{Selecting columns}

Selecting is the process of choosing a subset of columns from a dataset.  The remaining
columns are discarded.  This operation is not reversible, as the discarded columns are
lost.  Rows remain unchanged.

There are two main ways to select columns: by name or by predicate.  The former is the
most common and is used to select a fixed set of columns.  The latter is used to select
columns that satisfy a given condition, i.e., the values in the columns are used to
determine which columns should be selected.

When selecting columns by name, one can use a list of column names or a regular
expression\footnote{Regular expressions are very general and powerful, but they are also
complex and error-prone.  An alternative is to use some form of hierarchical naming,
such as \texttt{type.column} to express groups of columns.}.
The latter is useful when the column names follow a pattern that reflects the semantics of
the columns.  For instance,
one can use the regular expression \texttt{col[0-9]+} to select all columns whose names
start with \texttt{col} followed by one or more digits.

When selecting columns by predicate, one can use a function that returns a logical value
to define the condition under which a column should be selected.  For instance, one can
use the predicate \texttt{isnumeric} to select all columns that contain numeric values.
Notice, however, that the predicate is applied to each column independently and returns a
single logical value for each column.

Like filtering, selecting predicates might contain aggregation functions.  Although it is
theorically possible to consider the order of the values in the columns, it is not common
to do so.  (Especially because one would need to assume that the rows are previously
sorted by some criterion.) Groups, however, never make sense in this context, once the
predicate is applied to each column independently.

Depending on the context, it may be useful to ``drop'' columns instead of selecting them.
This is the same as selecting all columns except the ones specified.  This is useful when
the number of columns to be dropped is small compared to the total number of columns.
Strictly speaking, we just need to negate the predicate or the regular expression used to
select the columns.

Finally, it is very common to find libraries and framework in which the order of the
columns is important.  As a result, columns can be selected by position as well.
I find this practice error-prone and I recommend avoiding it whenever possible.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use selecting to remove columns that are not relevant to your analysis;
    \item Use column names or regular expressions (or hierarchical names) to select columns;
    \item Use predicates (many to one, with no aggregation functions) to define the conditions
      under which columns should be selected;
    \item Avoid depending on the order of the columns.
  \end{itemize}
\end{hlbox}

\subsection{Mutating columns}

Mutating is the process of creating new columns.  The operation is reversible, as the
original columns are kept.  The new columns are added to the dataset.

The values in the new column are determined by an expression.  The expression is a
function that returns a vector of values given the values in the other columns.  The
expression can be a simple function, such as \texttt{y = x + 1}, or a more complex
function, such as \texttt{y = ifelse(x > 0, 1, 0)}.  Here, \texttt{x} and \texttt{y} are
the names of an existing and the new column, respectively.

One may also use an aggregation and window function in the expression. This is particularly
useful when performing mutation considering a group.  In this case, the returned value is
repeated (aggregation function) for each row of the same group.  Like in filtering, the
more explicit you can be about order and groups, the better.

For example, the expression \texttt{y = cumsum(x) group by category sort by date} will
create a new column \texttt{y} with the cumulative sum of the \texttt{x} column for each
\texttt{category} given the order of the rows defined by the \texttt{date} column.

Sometimes, the same expression can be used to create multiple columns.  This is useful
when the new columns are related.  To do so, one first specifies the columns in the same way as
when selecting columns.  Then, one needs to specify a rule to name the new columns.
For instance, \texttt{x\_new = x + 1 across x matches \textasciicircum{}col[0-9]+\$}.

Practically speaking, mutation can overwrite existing columns.  This is useful when the
new column is a replacement for the old one.  Formally, overwriting is just a sequence of
mutation and selection operations.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use mutating to create new columns that are relevant to your analysis;
    \item Use expressions to define the values of the new columns;
    \item Use aggregation and window functions in the expression to create new columns based on
      groups and order;
    \item Use the same expression to create multiple columns when the new columns are related.
  \end{itemize}
\end{hlbox}

\subsection{Aggregating rows}

We can aggregate the rows of a dataset to create a new dataset with fewer rows.    The
operation is not reversible, as the discarded rows are lost.  The columns are also lost,
only the new aggregate columns remain.

The values in the new columns are determined by an aggregation function.  Like filtering
and mutation, the aggregation function can be parametrized by specifying a group and/or an
order.

The resulting dataset will contain one row for each group.  The values in the new columns
are determined by the aggregation function applied to the values in the other columns.
All columns that define the groups are usually kept in the resulting dataset.  In this
case, as expected, values of such columns are equal for all rows in the same group.

For instance, the aggregation function \texttt{mean(x) group by category} will create a
new dataset with one row for each different value of \texttt{category} and a new column
with the mean of the \texttt{x} column for each group.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use aggregation to summarize the data in a dataset;
    \item Use aggregation functions to define the values of the new columns;
    \item Other columns are lost;
    \item Use the group and order parameters to define the groups and the behavior of the
      aggregation function.
  \end{itemize}
\end{hlbox}

\subsection{Binding datasets}

One trivial, yet important, operation is to bind datasets.  This is the process of
combining two or more datasets into a single dataset.  The operation is reversible, as the
original datasets are kept.  The new dataset contains all the rows and columns of the
original datasets.

There are two ways to bind datasets: by rows or by columns.  The former is used to
combine datasets that have exactly the same columns but represent different parts of the
same dataset.  The latter is used to combine datasets that comprise the same observations
(rows) but captures different aspects of the same dataset.

When binding datasets by rows, the datasets must have the same columns\footnote{In
practice, it is usually required that they share the same order of the columns as well.
This is not a theoretical requirement, but a common limitation of most libraries.}.
The resulting dataset will contain all the rows of the original datasets.  The columns
remain unchanged.  It is a good practice to create a new column that represents the source
of each row.  For instance, if each table represents data collected in a different year,
one can create a new column \texttt{year} that contains the year of the data.

When binding datasets by columns, the datasets must have the same number of rows.  Each
matching row represent the same observation\footnote{Practically speaking, either the
order of the rows or a key column is used to match the rows of the datasets.  In both
situations, this is equivalent to a join operation by the row number or the key column;
assuming that both datasets contains the same observations.}. The resulting dataset will
contain all the columns of the original datasets.  The rows remain unchanged.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use binding to combine datasets that represent different parts of the same dataset;
    \item Use binding by rows to combine datasets that have the same columns --- in this
      case, create a new column that represents the source of each row;
    \item Use binding by columns to combine datasets that have the same number of rows.
  \end{itemize}
\end{hlbox}

{\color{red} Talk about splitting as the reverse function, and the reason why missing
columns may be a problem. Example of the unit of measurement.}

\subsection{Joining datasets}

Joining is the process of combining two datasets into a single dataset based on common
columns.  The operation may not be reversible, consult \cref{sec:normalization} for more
details.

The join of two tables is the operation that returns a new table with the columns of both
tables.  Let \texttt{U} be the common set of columns.  For each occurring value of
\texttt{U} in the first table, the operation will look for the same value in the second
table.  If it finds it, it will create a new row with the columns of both tables.  If it
does not find it, no row will be created.  This operation assumes that values in \texttt{U}
are unique in each table.

The variation described above is usually called natural or inner join.  Three other
variations are possible.
\begin{itemize}
  \item Left join: for each occurring value of \texttt{U} in the first table, the operation
    will look for the same value in the second table.  If it finds it, it will create a new
    row with the columns of both tables.  If it does not find it, it will create a new row
    with the columns of the first table and missing values for the columns of the second
    table.
  \item Right join: the same as the left join, but the roles of the tables are reversed.
  \item Outer join: for each different value of \texttt{U} in both tables, the operation
    will create a new row with the columns of both tables.  If a value is missing in one
    table, it will be filled with a missing value.
\end{itemize}

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use joining to integrate datasets;
    \item Be aware of the risks of joining datasets (\cref{sec:normalization}), for
      example, that some joins may create invalid rows;
    \item Use the appropriate variation of the join operation in applications.
  \end{itemize}
\end{hlbox}

\subsection{Pivoting and unpivoting}

Another important operation is to pivot and unpivot datasets.  These are the processes of
transforming a dataset from a long format to a wide format and vice versa.  The operations
are reversible and they are the inverse of each other.

Pivoting requires to specify a name column --- whose discrete and finite possible values
will become the names of the new columns --- and a value column --- whose values will be
spread across the rows.  All remaining columns are considered to be keys, uniquely
identifying each row of new the dataset.

Unpivoting\footnote{Which \citeauthor{Wickham2023} call pivot longer.} is the reverse
operation.  One must specify all the columns whose names are the values of the before
called name column.  The values of these columns will be gathered into a new column.
As before, all remaining columns are considered to be keys.

In practical applications, where not all remaining columns are keys, one must aggregate
rows beforehand.

\begin{tablebox}[label=tab:pivot]{Pivoting example.}
  \begin{minipage}{0.45\textwidth}
    \centering
    \rowcolors{2}{black!10!white}{}
    \begin{tabular}{ccc}
      \toprule
      \texttt{name} & \texttt{year} & \texttt{value} \\
      \midrule
      A & 2019 & 1 \\
      A & 2020 & 2 \\
      A & 2021 & 3 \\
      B & 2019 & 4 \\
      B & 2020 & 5 \\
      B & 2021 & 6 \\
      \bottomrule
    \end{tabular}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \centering
    \rowcolors{2}{black!10!white}{}
    \begin{tabular}{cccc}
      \toprule
      \texttt{name} & \texttt{2019} & \texttt{2020} & \texttt{2021} \\
      \midrule
      A & 1 & 2 & 3 \\
      B & 4 & 5 & 6 \\
      \bottomrule
    \end{tabular}
  \end{minipage}
  \tcblower
  The left table is in the long format and the right table is in the wide format.  The
  name column is \texttt{year} and the value column is \texttt{value}.
\end{tablebox}

\Cref{tab:pivot} shows an example of pivoting.  The left table is in the long format and
the right table is in the wide format.  The name column is \texttt{year}, the value column
is \texttt{value}, and the remaining column is \texttt{name} which is an unique identifier
of the rows in the wide format.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use pivoting to transform datasets from a long format to a wide format;
    \item Use unpivoting to transform datasets from a wide format to a long format;
    \item Be aware of the need to aggregate rows before unpivoting.
  \end{itemize}
\end{hlbox}

\subsection{An algebra for statistical transformations}

In recent years, some researchers made an effort to create a formal algebra for
statistical transformations.  The idea is to create a set of operations that can be
combined to create complex statistical transformations.  This is similar to the idea of
relational algebra, which is a set of operations that can be combined to create complex
queries.

The difference between relational algebra and a formal algebra for statistical
transformations is that the latter is more complex.  This is because statistical
transformations are more complex than queries.  For instance, the concept of missing data
is not present in relational algebra, but it is in statistical transformations.

\textcite{Song2021}, for example, propose a formal paradigm for statistical data
transformation.  They present a data model, an algebra, and a formal language.  Their goal
is to create a standard for statistical data transformation that can be used by different
statistical software.

However, in my opinion, the major deficiency of their work is that they mostly try to
``reverse engineer'' the operations that are commonly used in statistical software.  This
is useful for the translation of code between different software, but it is not productive
to advance in the theoretical understanding of statistical transformations.

If one ought to tackle the challenge of formally expressing statistical transformations, I
think one should start from the basic operations.  Basic operations mean that they are
irreducible, i.e., they cannot be expressed as a sequence of other operations.

Some thoughts about it:
\begin{itemize}
  \item Binding columns can be expressed as a join operation, thus it is not a basic
    operation.
  \item Some software provide features that can be better expressed in other (often simpler) ways.  Row
    naming is an example.  It is useful to keep track of the origin of each row, but names
    can be just another column.  I argue for excluding row naming in a formal algebra.
  \item Some operations are very useful and recurring, even if they are not basic.  Such
    operations must be omitted from the formal algebra for the sake of simplicity.
    However, any software that implements a language for the formal algebra can provide
    syntax sugar for these operations.
  \item Not defining your algebra in terms of a specific programming language is a good
    practice.  This is because the algebra is a theoretical concept and should be
    independent of any implementation.  It also gives opportunities to rethink the
    things that commonly done in a specific way.  This can lead to new insights and
    correct error-prone practices.
  \item Pivoting seems to be ``different'' enough to the other operations to be considered
    in the set of basic operations.  However, it is not hard to see that they can be
    rewritten as combinations with the meta tables containing the possible values of the
    attributes (or some sort of aggregation function).
\end{itemize}

% vim: spell spelllang=en
