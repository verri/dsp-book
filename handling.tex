\chapter{Data handling}
\label{chap:handling}

\chapterprecishere{%
  {\fontspec[Scale=2]{Symbola}\color{black!80}\symbol{"1F5E1}}
  It's dangerous to go alone! Take this.
  \par\raggedleft--- \textup{Unnamed Old Man}, The Legend of Zelda}

% Important: avoid the term "data manipulation" as it has a negative connotation
% TODO: review this introduction after finishing the remaining of the chapter

In the previous chapter, I discussed the relationship between data format and data
semantics.  We also saw in \cref{chap:project} that data tasks --- specifically
integration and tidying --- must adjust the available data to reflect the kind of
input we expect in production.

For those tasks, we must be careful with the operations we perform on the data. At the
stage of data preparation, for example, we should never parametrize our data handling
pipeline in terms of information retrieved\footnote{For instance, imputation by the mean
of a column.} by sampling the data.  This is because such operations lead to \gls{leakage}
during evaluation and other biases in our conclusions.

In this chapter, we consider that tables are rectangular data structures in which values
of the same column share the same properties (i.e. the same type, same restrictions, etc.)
and each column has a name.  Moreover, we assume that any value is possibly
\emph{missing}.

From a mathematical definition of such tables, we can define a set of operations that can
be applied to them.  These operations are the building blocks of data handling pipelines:
combinations of operations that transform a dataset into another dataset.
I also highlight some important properties of these operations.

I show how these operations can be combined to create complex data handling pipelines by
using them to solve the issues presented in \cref{sub:messy}.

\begin{mainbox}{Chapter remarks}

  \boxsubtitle{Contents}

  \startcontents[chapters]
  \printcontents[chapters]{}{1}{}
  \vspace{1em}

  \boxsubtitle{Context}

  \begin{itemize}
    \item \dots
  \end{itemize}

  \boxsubtitle{Objectives}

  \begin{itemize}
    \item \dots
  \end{itemize}

  \boxsubtitle{Takeways}

  \begin{itemize}
    \item \dots
  \end{itemize}
\end{mainbox}

{}
\clearpage

\section{Formal structured data}

\newcommand{\domainof}[1]{\mathcal{D}\!\left(#1\right)}
\newcommand{\missing}{\text{?}}
\newcommand{\rowcard}[1][k_1, \dots, k_k]{\operatorname{card}\!\left(#1\right)}

In this section, I present a formal definition of structured data.  This definition is
compatible with the relational model and tidy data presented in \cref{chap:data}.
My definition takes into account the index\footnote{Also called grouping variables.} of
the table, which is a key concept in data handling.  We also consider that values can be
missing.  Repeated values are represented by allowing cells to contain sets of values.

\begin{defbox}{Indexed table}{itable}
An indexed table $T$ is a tuple $(K, H, c)$, where $K = \left\{K_i : i = 1, \dots,
k\right\}$ is the set of index columns, $H$ is the set of (non-index) columns, and $c :
\domainof{K_1} \times \dots \times \domainof{K_k} \times H \to \mathcal{V}$ is the cell function.
Here, $\mathcal{V}$ represents the space of all possible tuples of values, which
may include missing values $\missing$.  Values have arbitrary types, such as integers,
real numbers, strings, etc.
Each index column $K_i$ has a domain $\domainof{K_i}$, which is an enumerable set of
values.
\end{defbox}

A possible row $r$ of the table is thus indexed by a tuple $r = (k_1, \dots,
k_k)$, where $k_i \in \domainof{K_i}$.  Each row has a cardinality $\rowcard[r]$, which
represents how many times the entity represented by the row is present in the table.
A row $r$ with $\rowcard[r] = 0$ is considered to be missing.

A cell is then represented by a row $r$ and a column $h \in H$.  The value of the cell,
$\vec{v} = c(r, h)$ is a tuple of values such that $|\vec{v}| = \rowcard[r]$.
The order of the elements in the tuple $\vec{v}$ is arbitrary but fixed.  A \emph{nested
row} consists of a tuple of values that associates different columns with the same
repetition of the entity, i.e. \[
  \Big[ v(h)_i : h \in H \Big]\text{,}
\]
where $c(r, h) = [v(h)_i : i = 1, \dots, \rowcard[r]]$, assuming an arbitrary fixed order
of the columns $h$.

We can stack nested rows to form a matrix of values.  This matrix is called the value
matrix of the row $r$.

\begin{defbox}{Value matrix}{vmatrix}
The value matrix $V = (v_{i, j})$ of the row $r$ is \[
  \Big[ c(r, h) : h \in H \Big]\text{,}
\] with dimensions $\rowcard[r] \times |H|$.
\end{defbox}

We assume that value matrices --- and consequently row cardinalities --- are minimal. This
means that there are no nested row $v_{i, 1}, \dots, v_{i, |H|}$ in the value matrices
such that $v_{i, j} = \missing$ for all $j$.

From these concepts, we can define the basic operations and properties that can be applied
to tables.

\subsection{Splitting and binding}

Split and bind are very basic operations that can be applied to tables.  They are
inverses of each other and are used to divide and combine tables, respectively.
They are important in the data science process because they play a key role in
data semantics and validation of solutions.

\begin{defbox}{Split operation}{split}
Given an indicator function $s : \domainof{K_1} \times \dots \times \domainof{K_k} \to
\left\{0, 1\right\}$, the split operation creates two tables, $T_0$ and $T_1$, that
contains only the rows for which
$s(r) = 0$ and $s(r) = 1$, respectively.

Mathematically, the split operation is defined as \[
  \operatorname{split}(T, s) = \left(T_0, T_1\right)\text{,}
\] where $T = (K, H, c)$, $T_i = (K, H, c_i)$, and \[
  c_i(r, h) = \begin{dcases}
    c(r, h) & \text{if } s(r) = i \\
    () & \text{otherwise.}
  \end{dcases}
\]
\end{defbox}

\emph{Note that, by definition, the split operation never ``breaks'' a row.  So, the
indices define the indivisible entities of the table.}  The resulting tables are
disjoint:

\begin{defbox}{Disjoint tables}{disjoint-tables}
  Two tables $T_0 = (K, H, c_0)$ and $T_1 = (K, H, c_1)$ are said to be disjoint if
  $\rowcard[r; c_0] = 0$ if $\rowcard[r; c_1] > 0$ for any row $r$, and vice-versa.
\end{defbox}

The binding operation is the inverse of the split operation.  Given two disjoint tables
$T_0 = (K, H, c_0)$ and $T_1 = (K, H, c_1)$, the binding operation creates a new table $T$
that contains all the rows of $T_0$ and $T_1$.

\begin{defbox}{Bind operation}{bind}
  Mathematically, the binding operation is defined as \[
    \operatorname{bind}(T_0, T_1) = (K, H, c)\text{,}
  \] where $T_i = (K, H, c_i)$ and \[ c(r, h) = c_0(r, h) + c_1(r, h)\text{.} \]
  The operator $+$ stands for the tuple concatenation operator%
  \footnote{The order of the concatenation here is not an issue since we guarantee
  that at least one of the operands is empty.}.
\end{defbox}

\emph{Thus, a requirement for the binding operation is that the tables are disjoint in
terms of the row entities they have.}

\paragraph{Premises in real-world applications}

One important aspect of these functions is that we assume that the entities represented by
the rows are indivisible, and that any binding operation will never occur for tables that
share the same entities.

In real-world applications, this is not always true.  Many times, we do not know the
process someone else has used to collect the data.  In these cases, we must be careful
about the guarantees we explain in this chapter.  On the other hand, one can consider the
premises we use as a guideline to design good data collection processes.

We can see data collection as the result of a splitting operation in the universe set of
all possible entities.  This is a good way to think about data collection, as we can try
to ensure that we collect all possible information about the entities we are interested
in.

This, of course, depends on what we define as the index columns of the table.  Consider
the example of collecting information about grades of students.  If we define as the index
columns the student's name and year, we must ensure that we collect all the grades of all
subjects a student has taken in a year.  We do not need, though, to collect information
from all students or all years.  On the other hand, if we define as the index column only
the student's name, we must collect all the grades of all subjects a student has taken in
all years.

In summary, the less variables we define as index columns, the more information we must
collect about each entity.  However, in the next sections, we show that assuming many index columns
lead to restrictions in the operations we can perform on the table.

This conceptual trade-off is important to understand when structuring the problem we are
trying to solve.  Neglecting these issues can lead to strong statistical biases and
incorrect conclusions.

\subsection{Split-invariance}

One property we can study about data handling operations is whether they are distributive
over the bind operation.  This property is called \emph{split-invariance}.

From now on, we will denote \[
  T_0 + T_1 = \operatorname{bind}(T_0, T_1)\text{,}
\] for any tables $T_0$ and $T_1$ to simplify the notation.

\begin{defbox}{Split-invariance}{split-invariance}
An arbitrary data handling operation $f(T)$ is said to be split-invariant
if, for any table $T$ and split function $s$, the following equation holds \[
  f\!\left(T_0 + T_1\right) =
    f\!\left(T_0\right) + f\!\left(T_1\right)\text{,}
\] where $T_0, T_1 = \operatorname{split}\!\left(T; s\right)$.
\end{defbox}

Split-invariance is a desirable property for data handling operations during the data
tasks described in \cref{chap:project}: integration and tidying.  Even while exploring
data, we should take effort to use split-invariant operations.

The reason is that split-invariance ensures that the operation does not depend on the
split performed (usually unknown to us) to create the table we have in hand.  This
property is important to avoid \gls{leakage} or to bias the results of the analysis.

\subsection{Illustrative example}

\begin{tablebox}[label=tab:grades1]{Data table of student grades.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{cccc}
    \toprule
    \textbf{student} & \textbf{subject} & \textbf{year} & \textbf{grade} \\
    \midrule
    Alice & Chemistry & 2020 & 6 \\
    Alice & Math & 2019 & 8 \\
    Alice & Physics & 2019 & 7 \\
    Bob & Chemistry & 2018 & ? \\
    Bob & Chemistry & 2019 & 7 \\
    Bob & Math & 2019 & 9 \\
    Bob & Physics & 2019 & 4 \\
    Bob & Physics & 2020 & 8 \\
    Carol & Biology & 2020 & 8 \\
    Carol & Chemistry & 2020 & 3 \\
    Carol & Math & 2020 & 10 \\
    \bottomrule
  \end{tabular}
  \tcblower
  Data collected about student grades.  All information that is available is presented.
\end{tablebox}

Consider the example of data collected about student grades.  \Cref{tab:grades1}
exemplifies all information we can possibly have about the grades of students.  A missing
value in a cell of that table indicates that, for some reason, the information is not
retrievable.

The domain of the variables are:
\begin{itemize}
  \itemsep0em
  \item $\domainof{\text{student}} = \left\{\text{Alice}, \text{Bob}, \text{Carol}\right\}$;
  \item $\domainof{\text{subject}} = \left\{\text{Biology}, \text{Chemistry}, \text{Math},
    \text{Physics}\right\}$;
  \item $\domainof{\text{year}} = \mathbb{Z}$; and
  \item $\domainof{\text{grade}} = \left[0, 10\right] \cup \left\{\missing\right\}$.
\end{itemize}

Of course, in practice, we have no guarantee that the data we have is complete nor the
clear specification of the domain of the variables.  Instead, we must choose good
premises about the data we are working with.

Knowing that the data is complete, we can safely assume that:
\begin{enumerate}
  \itemsep0em
  \item Alice has never taken Biology;
  \item Bob passed Physics, although at the second attempt;
  \item Carol has only taken classes in 2020.
\end{enumerate}

\begin{tablebox}[label=tab:grades2]{Data table of student grades assuming student and subject as indices.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{ccccc}
    \toprule
    \textbf{s} & \textbf{student} & \textbf{subject} & \textbf{year} & \textbf{grade} \\
    \midrule
    0 & Alice & Chemistry & (2020) & (6) \\
    1 & Alice & Math & (2019) & (8) \\
    1 & Alice & Physics & (2019) & (7) \\
    0 & Bob & Chemistry & (2018, 2019) & (?, 7) \\
    0 & Bob & Math & (2019) & (9) \\
    1 & Bob & Physics & (2019, 2020) & (4, 8) \\
    0 & Carol & Biology & (2020) & (8) \\
    0 & Carol & Chemistry & (2020) & (3) \\
    1 & Carol & Math & (2020) & (10) \\
    \bottomrule
  \end{tabular}
  \tcblower
  Indexed table with data from \cref{tab:grades1} assuming student and
  subject as indices.  The column $s$ is the split indicator.
\end{tablebox}

Now consider an arbitrary collection mechanism that consider student and subject as the
indices of the table.  \Cref{tab:grades2} shows the table we have in hand.  The column $s$
is the split indicator.  Only rows with $s = 1$ are available to us.

Now, about the statements we made before:
\begin{enumerate}
  \itemsep0em
  \item There is no way we can know if Alice has taken Biology or not.  It could be that
    the data collection mechanism failed to collect this information or that the
    information simply does not exist.
  \item We can safely assume that Bob has passed Physics in the second try, once all
    information about (Bob, Physics) is assumed to be available.
  \item There is no guarantee that Carol has only taken classes in 2020.  It could be that
    some row (Carol, subject) with year different than 2020 is missing in the table.
\end{enumerate}

\begin{tablebox}[label=tab:grades3]{Data table of student grades assuming student as the index.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{ccp{2.6cm}p{1.8cm}>{\raggedright\arraybackslash}p{1.2cm}}
    \toprule
    \textbf{s} & \textbf{student} & \textbf{subject} & \textbf{year} & \textbf{grade} \\
    \midrule
    1 & Alice & (Chemistry, Math, Physics) & (2020, 2019, 2019) & (6, 8, 7) \\
    0 & Bob & (Chemistry, Chemistry, Math, Physics, Physics) & (2018, 2019, 2019, 2019, 2020) & (?, 7, 9, 4, 8) \\
    1 & Carol & (Biology, Chemistry, Math) & (2020, 2020, 2020) & (8, 3, 10) \\
    \bottomrule
  \end{tabular}
  \tcblower
  Indexed table with data from \cref{tab:grades1} assuming only student
  as the index.  The column $s$ is the split indicator.
\end{tablebox}

We can be even more restrictive and consider only the student as the index of the table.
Imposing this restriction would difficult the data collection process, but it would
guarantee that we have all information about each student.  \Cref{tab:grades3} shows the
table we have in hand.  As before, the column $s$ is the split indicator and only rows with
$s = 1$ are available to us.

Our conclusions may change again:
\begin{enumerate}
  \itemsep0em
  \item We can safely assume that Alice has never taken Biology, as $\text{Chemistry}
    \not\in c(\text{Alice}, \text{subject})$.
  \item We can assume nothing about Bob's grades, as all information about him is missing.
  \item We can safely assume that Carol has only taken classes in 2020, as $c(\text{Carol},
    \text{year})$ contains only values with 2020.
\end{enumerate}

It is straightforward to see that the less index columns we have, the more information we
have about the present entities.  Also, we can see how important it is the assumptions on
the index columns to the conclusions we can draw from the data.  Consequently,
split-invariant operations can preserve valid conclusions about the data even when
information is missing\footnote{Absence can be due to incomplete data collection or
artificial splitting for validation, consult \cref{chap:planning}.}.

\section{Data handling pipelines}

In the literature and in software documentation, you will find a variety of terms used to
describe data handling operations\footnote{%
  The terminology ``data handling'' itself is not universal.  Some authors and libraries
  call it ``data manipulation'', ``data wrangling'', ``data shaping'', or ``data
  engineering''.  I use the term ``data handling'' because it seems more generic.
  Also, it avoids confusion with the term ``data
  manipulation'' which has a negative connotation in some contexts.}. %
They often refer to the same or similar operations, but the terminology can be confusing.
In this section, I present a summary of these operations mostly based on
\textcite{Wickham2023} definitions\footnote{Which they call \emph{verbs}.}.


During the preparation of data for our project, we will need to perform a set of operations
on possibly multiple datasets.  These operations are organized in a pipeline, where the
outputs of one operation are the inputs of the next one.
Operations are extensively parametrized, for instance, most of them can use predicates to
define the groups, arrangements, or conditions under which they should be applied.

\begin{figurebox}[label=fig:pipeline]{Example of data handling pipeline.}
  \centering
  \begin{tikzpicture}[every node/.style={font=\small, inner sep=4pt}]
    \node (s1) [darkcircle] at (0, 0) {Source 1};
    \node (s2) [darkcircle] at (0, -2) {Source 2};
    \node (f1) [mediumblock] at (2, 0) {$f_1$};
    \node (f2) [mediumblock] at (4, 0) {$f_2$};
    \node (f3) [mediumblock] at (2, -2) {$f_3$};
    \node (f4) [mediumblock] at (4, -2) {$f_4$};
    \node (f5) [mediumblock] at (6, -1) {$f_5$};
    \node (data) [darkcircle, minimum width=15mm] (data) at (8, -1) {Data};

    \path [line] (s1) -- (f1);
    \path [line] (f1) -- (f2);
    \path [line] (f1.east) -- (f4);
    \path [line] (s2) -- (f3);
    \path [line] (f3) -- (f4);
    \path [line] (f2) -- (f5);
    \path [line] (f4) -- (f5);
    \path [line] (f5) -- (data);
  \end{tikzpicture}
  \tcblower
  A data handling pipeline is a set of operations that transform a dataset into
  another dataset.  We can have more than one source dataset and the output is a single
  dataset where each row represents a sample in the observational unit we are interested
  in.
\end{figurebox}

In \cref{fig:pipeline}, we show an example of a data handling pipeline.  The pipeline
starts with two source datasets, Source 1 and Source 2.  The datasets are processed by a
set of operations, $f_1, f_2, f_3, f_4, f_5$, and the output is a single dataset,
Data.  Our goal at the data tasks --- see \cref{sub:workflow} --- is to create a dataset
that is representative of the observational unit we are interested in.  Representative
here means that the dataset is tidy\footnote{Remember that our definition of tidiness
depend on the observational unit.  That means, in practice, that if the original data
sources are in a observational unit different from the one we are interested in, after
joining them, the connecting variables might be dropped to eliminate transitive
dependencies.  Consult \cref{sub:tidy-not-tidy,sub:change-unit}.} and that the priors,
i.e. the distribution of the data is faithful to the real distribution of the phenomenon.

A pipeline is more flexible than a chain of operations because it can handle more complex
structures, where different branches (forks) of processing occur simultaneously, and then
come together (merges) later in the workflow.  For instance, the output of $f_1$ is the
input of $f_2$ and $f_4$ (fork), and $f_5$ has as input the outputs of $f_2$ and $f_4$
(merge).

Pipelines are great conceptual tools to organize the data handling process.  They allow
for the separation of concerns, where each operation is responsible for a single task.
Also, declaring the whole pipeline at once allows for the optimization of the operations
and the use of parallel processing.  This is important when dealing with large datasets.
The declarative approach, opposed to the imperative one, makes it easier to reason about
and maintain the code\footnote{Tidyverse and Polars are examples of
libraries that use a declarative approach to data handling.}.

\section{Split-invariant operations}

We use the following terminology to refer to the data handling parameters:
\begin{itemize}
  \item \textbf{Predicate}: a function that returns a logical value, used to filter
    rows/columns or to define the groups of rows/columns to be processed;
  \item \textbf{Aggregation function}: a function that returns a single value given a vector
    of values (in which, the order of the values may be important);
  \item \textbf{Window function}: a function that returns a vector of values given a vector
    of values in which, the order of the values is important;
  \item \textbf{Expression}: a function that returns a vector of values element-wise, used to create new
    columns or to modify existing ones.
\end{itemize}

% \begin{slidebox}{Data handling pipelines}{}
%   \begin{itemize}
%     \item Data handling operations can be combined to create complex pipelines;
%     \item Operators may be reversible;
%     \item Operators are vectorized;
%     \item They can be parametrized with predicates, aggregation functions, and expressions;
%     \item They operate on datasets and return new datasets as output.
%     \item They are declarative.
%   \end{itemize}
% \end{slidebox}


\subsection{Filtering rows}

Filtering is the process of selecting a subset of rows from a dataset based on a
predicate.  If more than a single predicate is used, they are combined using a logical
operator, such as logical disjunction (or) or logical conjunction (and).

After filtering, the dataset will contain only the rows that satisfy the predicate.
Columns remain unchanged.  This operation is potentially irreversible, as the removed
rows are lost.

In the basic form, each row is treated independently.  For instance, the predicate
\code{age > 18} will select all rows where the value in the \code{age} column is
greater than 18.

However, if the predicate depends on an aggregation or window function, one must specify
the groups and/or the order of the rows.  For instance, the predicate \code{age >
mean(age) group by country} will select the rows where the value in the \code{age}
column is greater than the mean of the \code{age} for each \code{country}. Another
example is the predicate \code{cumsum(price) < 100 sort by date}, which selects the rows
that satisfy the condition that the cumulative sum of the \code{price} column is less
than 100 given the order of the rows defined by the \code{date} column.

The trivial group is the entire dataset, so it is usually not necessary to specify it
explicitly.  However, it is usually not sensible to not specify the order of the rows.

When dealing with real values, be aware of floating-point precision issues.  In other
words, do not use the equality operator to compare real numbers.  Most of libraries
provide operators to compare real numbers within a given tolerance.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use filtering to remove rows that are not relevant to your analysis;
    \item Use predicates to define the conditions under which rows should be removed;
    \item When aggregation functions are needed to define the predicate, specify the groups and
      the order of the rows;
    \item Be aware of floating-point precision issues when comparing real numbers.
  \end{itemize}
\end{hlbox}

\subsection{Selecting columns}

Selecting is the process of choosing a subset of columns from a dataset.  The remaining
columns are discarded.  This operation is not reversible, as the discarded columns are
lost.  Rows remain unchanged.

There are two main ways to select columns: by name or by predicate.  The former is the
most common and is used to select a fixed set of columns.  The latter is used to select
columns that satisfy a given condition, i.e., the values in the columns are used to
determine which columns should be selected.

When selecting columns by name, one can use a list of column names or a regular
expression\footnote{Regular expressions are very general and powerful, but they are also
complex and error-prone.  An alternative is to use some form of hierarchical naming,
such as \code{type.column} to express groups of columns.}.
The latter is useful when the column names follow a pattern that reflects the semantics of
the columns.  For instance,
one can use the regular expression \code{col[0-9]+} to select all columns whose names
start with \code{col} followed by one or more digits.

When selecting columns by predicate, one can use a function that returns a logical value
to define the condition under which a column should be selected.  For instance, one can
use the predicate \code{isnumeric} to select all columns that contain numeric values.
Notice, however, that the predicate is applied to each column independently and returns a
single logical value for each column.

Like filtering, selecting predicates might contain aggregation functions.  Although it is
theorically possible to consider the order of the values in the columns, it is not common
to do so.  (Especially because one would need to assume that the rows are previously
sorted by some criterion.) Groups, however, never make sense in this context, once the
predicate is applied to each column independently.

Depending on the context, it may be useful to ``drop'' columns instead of selecting them.
This is the same as selecting all columns except the ones specified.  This is useful when
the number of columns to be dropped is small compared to the total number of columns.
Strictly speaking, we just need to negate the predicate or the regular expression used to
select the columns.

Finally, it is very common to find libraries and framework in which the order of the
columns is important.  As a result, columns can be selected by position as well.
I find this practice error-prone and I recommend avoiding it whenever possible.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use selecting to remove columns that are not relevant to your analysis;
    \item Use column names or regular expressions (or hierarchical names) to select columns;
    \item Use predicates (many to one, with no aggregation functions) to define the conditions
      under which columns should be selected;
    \item Avoid depending on the order of the columns.
  \end{itemize}
\end{hlbox}

\subsection{Mutating columns}

Mutating is the process of creating new columns.  The operation is reversible, as the
original columns are kept.  The new columns are added to the dataset.

The values in the new column are determined by an expression.  The expression is a
function that returns a vector of values given the values in the other columns.  The
expression can be a simple function, such as \code{y = x + 1}, or a more complex
function, such as
\begin{center}
  \code{y = ifelse(x > 0, 1, 0)}.
\end{center}
Here, \code{x} and \code{y} are
the names of an existing and the new column, respectively.

One may also use an aggregation and window function in the expression. This is particularly
useful when performing mutation considering a group.  In this case, the returned value is
repeated (aggregation function) for each row of the same group.  Like in filtering, the
more explicit you can be about order and groups, the better.

For example, the expression
\begin{center}
  \code{y = cumsum(x) group by category sort by date}
\end{center}
will create a new column \code{y} with the cumulative sum of the \code{x} column for each
\code{category} given the order of the rows defined by the \code{date} column.

Sometimes, the same expression can be used to create multiple columns.  This is useful
when the new columns are related.  To do so, one first specifies the columns in the same way as
when selecting columns.  Then, one needs to specify a rule to name the new columns.
For instance, \code{x\_new = x + 1 across x matches \textasciicircum{}col[0-9]+\$}.

Practically speaking, mutation can overwrite existing columns.  This is useful when the
new column is a replacement for the old one.  Formally, overwriting is just a sequence of
mutation and selection operations.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use mutating to create new columns that are relevant to your analysis;
    \item Use expressions to define the values of the new columns;
    \item Use aggregation and window functions in the expression to create new columns based on
      groups and order;
    \item Use the same expression to create multiple columns when the new columns are related.
  \end{itemize}
\end{hlbox}

\subsection{Aggregating rows}

We can aggregate the rows of a dataset to create a new dataset with fewer rows.    The
operation is not reversible, as the discarded rows are lost.  The columns are also lost,
only the new aggregate columns remain.

The values in the new columns are determined by an aggregation function.  Like filtering
and mutation, the aggregation function can be parametrized by specifying a group and/or an
order.

The resulting dataset will contain one row for each group.  The values in the new columns
are determined by the aggregation function applied to the values in the other columns.
All columns that define the groups are usually kept in the resulting dataset.  In this
case, as expected, values of such columns are equal for all rows in the same group.

For instance, the aggregation function \code{mean(x) group by category} will create a
new dataset with one row for each different value of \code{category} and a new column
with the mean of the \code{x} column for each group.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use aggregation to summarize the data in a dataset;
    \item Use aggregation functions to define the values of the new columns;
    \item Other columns are lost;
    \item Use the group and order parameters to define the groups and the behavior of the
      aggregation function.
  \end{itemize}
\end{hlbox}

\subsection{Binding datasets}

One trivial, yet important, operation is to bind datasets.  This is the process of
combining two or more datasets into a single dataset.  The operation is reversible, as the
original datasets are kept.  The new dataset contains all the rows and columns of the
original datasets.

There are two ways to bind datasets: by rows or by columns.  The former is used to
combine datasets that have exactly the same columns but represent different parts of the
same dataset.  The latter is used to combine datasets that comprise the same observations
(rows) but captures different aspects of the same dataset.

When binding datasets by rows, the datasets must have the same columns\footnote{In
practice, it is usually required that they share the same order of the columns as well.
This is not a theoretical requirement, but a common limitation of most libraries.}.
The resulting dataset will contain all the rows of the original datasets.  The columns
remain unchanged.  It is a good practice to create a new column that represents the source
of each row.  For instance, if each table represents data collected in a different year,
one can create a new column \code{year} that contains the year of the data.

When binding datasets by columns, the datasets must have the same number of rows.  Each
matching row represent the same observation\footnote{Practically speaking, either the
order of the rows or a key column is used to match the rows of the datasets.  In both
situations, this is equivalent to a join operation by the row number or the key column;
assuming that both datasets contains the same observations.}. The resulting dataset will
contain all the columns of the original datasets.  The rows remain unchanged.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use binding to combine datasets that represent different parts of the same dataset;
    \item Use binding by rows to combine datasets that have the same columns --- in this
      case, create a new column that represents the source of each row;
    \item Use binding by columns to combine datasets that have the same number of rows.
  \end{itemize}
\end{hlbox}

{\color{red} Talk about splitting as the reverse function, and the reason why missing
columns may be a problem. Example of the unit of measurement.}

\subsection{Joining datasets}

Joining is the process of combining two datasets into a single dataset based on common
columns.  The operation may not be reversible, consult \cref{sec:normalization} for more
details.

The join of two tables is the operation that returns a new table with the columns of both
tables.  Let \code{U} be the common set of columns.  For each occurring value of
\code{U} in the first table, the operation will look for the same value in the second
table.  If it finds it, it will create a new row with the columns of both tables.  If it
does not find it, no row will be created.  This operation assumes that values in \code{U}
are unique in each table.

The variation described above is usually called natural or inner join.  Three other
variations are possible.
\begin{itemize}
  \item Left join: for each occurring value of \code{U} in the first table, the operation
    will look for the same value in the second table.  If it finds it, it will create a new
    row with the columns of both tables.  If it does not find it, it will create a new row
    with the columns of the first table and missing values for the columns of the second
    table.
  \item Right join: the same as the left join, but the roles of the tables are reversed.
  \item Outer join: for each different value of \code{U} in both tables, the operation
    will create a new row with the columns of both tables.  If a value is missing in one
    table, it will be filled with a missing value.
\end{itemize}

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use joining to integrate datasets;
    \item Be aware of the risks of joining datasets (\cref{sec:normalization}), for
      example, that some joins may create invalid rows;
    \item Use the appropriate variation of the join operation in applications.
  \end{itemize}
\end{hlbox}

\subsection{Pivoting and unpivoting}

Another important operation is to pivot and unpivot datasets.  These are the processes of
transforming a dataset from a long format to a wide format and vice versa.  The operations
are reversible and they are the inverse of each other.

Pivoting requires to specify a name column --- whose discrete and finite possible values
will become the names of the new columns --- and a value column --- whose values will be
spread across the rows.  All remaining columns are considered to be keys, uniquely
identifying each row of new the dataset.

Unpivoting\footnote{Which \citeauthor{Wickham2023} call pivot longer.} is the reverse
operation.  One must specify all the columns whose names are the values of the before
called name column.  The values of these columns will be gathered into a new column.
As before, all remaining columns are considered to be keys.

In practical applications, where not all remaining columns are keys, one must aggregate
rows beforehand.

\begin{tablebox}[label=tab:pivot]{Pivoting example.}
  \begin{minipage}{0.45\textwidth}
    \centering
    \rowcolors{2}{black!10!white}{}
    \begin{tabular}{ccc}
      \toprule
      \textbf{name} & \textbf{year} & \textbf{value} \\
      \midrule
      A & 2019 & 1 \\
      A & 2020 & 2 \\
      A & 2021 & 3 \\
      B & 2019 & 4 \\
      B & 2020 & 5 \\
      B & 2021 & 6 \\
      \bottomrule
    \end{tabular}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \centering
    \rowcolors{2}{black!10!white}{}
    \begin{tabular}{cccc}
      \toprule
      \code{name} & \code{2019} & \code{2020} & \code{2021} \\
      \midrule
      A & 1 & 2 & 3 \\
      B & 4 & 5 & 6 \\
      \bottomrule
    \end{tabular}
  \end{minipage}
  \tcblower
  The left table is in the long format and the right table is in the wide format.  The
  name column is \code{year} and the value column is \code{value}.
\end{tablebox}

\Cref{tab:pivot} shows an example of pivoting.  The left table is in the long format and
the right table is in the wide format.  The name column is \code{year}, the value column
is \code{value}, and the remaining column is \code{name} which is an unique identifier
of the rows in the wide format.

\begin{hlbox}{Practical tips}
  \begin{itemize}
    \item Use pivoting to transform datasets from a long format to a wide format;
    \item Use unpivoting to transform datasets from a wide format to a long format;
    \item Be aware of the need to aggregate rows before unpivoting.
  \end{itemize}
\end{hlbox}

\section{An algebra for statistical transformations}

In recent years, some researchers made an effort to create a formal algebra for
statistical transformations.  The idea is to create a set of operations that can be
combined to create complex statistical transformations.  This is similar to the idea of
relational algebra, which is a set of operations that can be combined to create complex
queries.

The difference between relational algebra and a formal algebra for statistical
transformations is that the latter is more complex.  This is because statistical
transformations are more complex than queries.  For instance, the concept of missing data
is not present in relational algebra, but it is in statistical transformations.

\textcite{Song2021}, for example, propose a formal paradigm for statistical data
transformation.  They present a data model, an algebra, and a formal language.  Their goal
is to create a standard for statistical data transformation that can be used by different
statistical software.

However, in my opinion, the major deficiency of their work is that they mostly try to
``reverse engineer'' the operations that are commonly used in statistical software.  This
is useful for the translation of code between different software, but it is not productive
to advance in the theoretical understanding of statistical transformations.

If one ought to tackle the challenge of formally expressing statistical transformations, I
think one should start from the basic operations.  Basic operations mean that they are
irreducible, i.e., they cannot be expressed as a sequence of other operations.

Some thoughts about it:
\begin{itemize}
  \item Binding columns can be expressed as a join operation, thus it is not a basic
    operation.
  \item Some software provide features that can be better expressed in other (often simpler) ways.  Row
    naming is an example.  It is useful to keep track of the origin of each row, but names
    can be just another column.  I argue for excluding row naming in a formal algebra.
  \item Some operations are very useful and recurring, even if they are not basic.  Such
    operations must be omitted from the formal algebra for the sake of simplicity.
    However, any software that implements a language for the formal algebra can provide
    syntax sugar for these operations.
  \item Not defining your algebra in terms of a specific programming language is a good
    practice.  This is because the algebra is a theoretical concept and should be
    independent of any implementation.  It also gives opportunities to rethink the
    things that commonly done in a specific way.  This can lead to new insights and
    correct error-prone practices.
  \item Pivoting seems to be ``different'' enough to the other operations to be considered
    in the set of basic operations.  However, it is not hard to see that they can be
    rewritten as combinations with the meta tables containing the possible values of the
    attributes (or some sort of aggregation function).
\end{itemize}

% vim: spell spelllang=en
