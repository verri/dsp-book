\chapter{Learning from data}
\label{chap:slt}
\glsresetall %TODO: do the same thing in all chapters

\chapterprecishere{%
  To  understand  God's  thoughts  we  must study statistics, for these are the measure of His purpose.
  \par\raggedleft--- \textup{Florence Nightingale}, her diary}

As we discussed before, in this book, I focus on the problem of inferring a solution for a
predictive task from data.  In this chapter, we introduce the basic concepts of the
\gls{slt}, a general framework for predictive learning tasks.

\begin{mainbox}{Chapter remarks}

  \boxsubtitle{Contents}

  \startcontents[chapters]
  \printcontents[chapters]{}{1}{}
  \vspace{1em}

  \boxsubtitle{Context}

  \begin{itemize}
    \itemsep0em
    \item \dots
  \end{itemize}

  \boxsubtitle{Objectives}

  \begin{itemize}
    \itemsep0em
    \item \dots
  \end{itemize}

  \boxsubtitle{Takeways}

  \begin{itemize}
    \itemsep0em
    \item \dots
  \end{itemize}
\end{mainbox}

{}
\clearpage

\section{Introduction} % TODO: I don't like this title

Several problems can be addressed by techniques that use data somehow.  Once we focus on
one particular problem --- inductive learning ---, we need to define the scope of the
tasks we are interested in.  Let us start from the broader fields to the more specific
ones.

\Gls{ai} is a very broad field, including not only the study of algorithms
that exhibit intelligent behavior, but also the study of the behavior of intelligent
systems.  For instance, it encompasses the study of optimization methods, bioinspired algorithms,
robotics, philosophy of mind, and many other topics.  We are interested in the subfield of
artificial intelligence that studies algorithms that exhibit some form of intelligent
behavior.

A more specific subfield of \gls{ai} is \gls{ml}, which studies algorithms that
enable computers to automatically learn and improve their performance on a task from
experience, without being explicitly programmed by a human being.

Programming a computer to play chess is a good example of the difference between
traditional \gls{ai} and \gls{ml}.  In traditional \gls{ai}, a human programmer
would write a program that contains the rules of chess and the strategies to play the game.
The algorithm might even ``search'' among the possible moves to find the best one.  In
\gls{ml}, the programmer would write a program that learns to play chess by playing
against itself, against other programs, or even from watching games played by humans.
The system would learn the rules of chess and the strategies to play the game by itself.

This field is particularly useful when the task is too complex to be solved by
traditional programming methods or when we do not know how to solve the task.
Among the many tasks that can be addressed by \gls{ml}, we can specialize even more.

Predictive learning is the \gls{ml} paradigm that focuses on making predictions about
outcomes (sometimes about the future) based on historical data.  Predictive tasks
involve predicting the value of a target variable based on the values of one or more
input variables\footnote{Descriptive learning, which is out of the scope of this book,
focuses on describing the relationships between variables in the data without the
need of a target variable.}.

Depending on the reasoning behind the learning algorithms, we can divide the learning
field into two main approaches: \emph{inductive learning} and \emph{transductive
learning}\footnote{Trasduction is the process obtaining specific knowledge from specific
observation, and it is not the focus of this book.}.

Inductive learning involves deriving general rules from specific observations.  The
general rules can make predictions about \emph{any} new instances.  Such an approach
is exactly what we want to apply the project methodology we described in
\cref{sec:our-approach}:  the solution is the general rule inferred from the data.

\begin{figurebox}[label=fig:learning]{Organizational chart of the learning field.}
  \centering
  \begin{tikzpicture}
    \draw[outline] (0,0) circle (30mm) node {};
    \node[below] at (0, 2.6) {artificial intelligence};
    \draw[outline] (0,-0.5) circle (25mm) node {};
    \node[below] at (0, 1.6) {machine learning};
    \draw[outline] (0,-1) circle (20mm) node {};
    \node[below] at (0, 0.5) {predictive learning};
    \draw[outline] (0,-1.5) circle (15mm) node {};
    \node[below] at (0, -1.0) {inductive learning};
  \end{tikzpicture}
  \tcblower
  Artificial intelligence studies algorithms that exhibit intelligent behavior and the
  behavior of intelligent systems.  Machine learning is a subfield of artificial
  intelligence that studies algorithms that enable computers to automatically learn from
  data.  Predictive learning which focuses on making predictions about outcomes given
  known input data.  Inductive learning is a yet more specific type of learning that
  involves deriving general rules from specific observations.
\end{figurebox}

\Cref{fig:learning} give us a hierarchical view of the learning field.  Alternatives ---
such as descriptive learning in opposition to predictive learning, or transductive
learning in opposition to inductive learning --- are out of the scope of this book.

Maybe the most general (and useful) framework for predictive learning is \gls{slt}.
In this chapter, we will introduce the basic concepts of this theory and discuss the
properties of the main \gls{ml} methods.

\section{The learning problem}

Consider the set
\begin{equation}
  \label{eq:training-set}
  \big\{(\vec{x}_i, y_i) : i = 1, \dots, n \big\}
\end{equation}
where each sample $i$ is associated with a feature vector $\vec{x}_i \in \mathcal{X}$ and a target variable
$y_i \in \mathcal{Y}$.  We assume that samples are random independent identically
distributed (i.i.d.) observations drawn according to $$\Prob(x, y) = \Prob(y \mid x) \Prob(x)\text{.}$$
Both distributions $\Prob(x)$ and $\Prob(y \mid x)$ are fixed but unknown.

This is equivalent to the original \gls{slt} setup stated by \textcite{Vapnik1999b}, where
a generator produce random vectors $\vec{x}$ according to a fixed but unknown
probability distribution $\Prob(x)$ and a supervisor returns an output value $y$ for every
input vector $x$ according to a conditional distribution function $\Prob(y \mid x)$, also fixed but
unknown.

Moreover, note that this setup is compatible with the idea of tidy data and 3NF (see
\cref{sub:bridge}). Of course, we assume $X, Y$ are only the measured variables (or
non-prime attributes).  In practice, it means that we left aside the keys in the learning
process.

In terms of the tables defined in \cref{sec:formal-structured-data}, any row $r$ in the
table $T = (K, H, c)$, in the desired observational unit, such that $\rowcard[r] > 0$, and
$h \in H$ the chosen target variable, we have a corresponding target $y = c(r, h)$ and a
feature vector $\vec{x}$ corresponds to the tuple $$\big(c(r, h') : h' \in H \setminus
\left\{ h \right\}\big)\text{.}$$  Similarly, the variables $K$ that describe each unit
are left aside, as it does not make sense to infer general rules from them.

From the statistical point-of-view, learning problems consist of answering questions about
the distribution of the data.

\subsection{Learning tasks}

In terms of predictive learning, given de before-mentioned scenario, we can refine our
goals by tackling specific tasks\footnote{I consider tasks as well-defined subproblems of
a higher-level problem.}.

Consider a \emph{learning machine} capable of generating a set of functions, or
\emph{models}, $f(x; \theta) \equiv f_\theta(x)$, for a set of parametrizations $\theta
\in \Theta$ and such that $f_\theta : \mathcal{X} \rightarrow \mathcal{Y}$.  In a learning
task, we must choose, among all possible $f_\theta$, the one that predicts the target
variable the best possible way.

In order to learn, we must first define the \emph{loss} (or discrepancy) $\mathcal{L}$
between the response $y$ to a given input $x$, drawn from $\Prob(x, y)$, and the
response provided by the learned function.

Then, given the \emph{risk function}
\begin{equation}
  \label{eq:risk}
  R(\theta) = \int \mathcal{L}(y, f_\theta(x))\, d\!\Prob(x, y)\text{,}
\end{equation}
the goal is to find the function $f_\theta$ that minimizes $R(\theta)$
where the only available information is the \emph{training set} given by \eqref{eq:training-set}.

This formulation encompasses many specific tasks. I focus on the two of them which I
believe are the most fundamental ones: \emph{binary data classification}\footnote{In
\gls{slt}, Vapnik calls it \emph{pattern recognition}.} and \emph{regression
estimation}\footnote{We are not talking about \emph{regression analysis}; regression
estimation is closer to the \emph{scoring} task definition by \fullcite{Zumel2019}.}.  (I
left aside the density estimation problem, once it is not addressed in the remaining of
the book.)

\subsubsection{Binary data classification task}

In this task, the output $y$ takes on
only two possible values, zero or one --- usually called the negative and the positive
class, respectively ---, and the functions $f_\theta$ are indicator
functions. Choosing the loss
\begin{equation*}
  \mathcal{L}(y, f_\theta(x)) = \begin{cases}
    0 & \text{if } y = f_\theta(x) \\
    1 & \text{if } y \neq f_\theta(x)\text{,}
  \end{cases}
\end{equation*}
the risk $\eqref{eq:risk}$ becomes the probability of
classification error.  The function $f_\theta$, in this case, is called \emph{classifier}
and $y$ is called the \emph{label}.

\subsubsection{Regression estimation task}

In this task, the output $y$ is a real value and the functions $f_\theta$ are real-valued
functions.  The loss function is the squared error
\[
  \mathcal{L}(y, f_\theta(x)) = \big(y - f_\theta(x)\big)^2\text{.}
\]
In \cref{sec:optimal-solution}, we show that the function that minimizes the risk with such
loss function is the so-called \emph{regression}.
The estimator $f_\theta$ of the regression, in this case, is called a \emph{regressor}.

\subsection{A few remarks}

These two tasks are pretty general and can be applied to a wide range of problems.  The
modeling of the task at hand and choice of the loss function is crucial to the success of
the learning process.

About these learning tasks, we can make a few remarks.

\paragraph{Supervised and semisupervised learning}
In both cases, classification and regression estimation, the learning task is to find the function
that maps the input data to the output data in the best possible way.  Although the
learning machine described generate models in a \emph{supervised} manner --- i.e. target
is known for all samples in the training set ---, there are
alternative ways to solve the inductive learning problem, such as the \emph{semisupervised}
approach, where the model can be trained with a small subset of labeled data and a large
subset of unlabeled data --- that is, data whose outputs $y$ are unknown.

\paragraph{Generative and discriminative models}
Any learning machine generates a model that describes the relationship between the input
and output data.  This model can be generative or discriminative.  Generative models
describe the joint probability distribution $\Prob(x, y)$ and can also be used to generate new
data.  Discriminative models, on the other hand, describe the conditional probability
distribution $\Prob(y \mid x)$ directly and can only be used to make predictions. Generative models are
usually much more complex than discriminative models\footnote{Since modeling $\Prob(x, y)$
indirectly models $\Prob(y \mid x)$ and $\Prob(x)$.}, but they hold more information about
the data.  If you only
need to solve the predictive problem, prefer a discriminative model.

\paragraph{Multiclass classification}
In the binary classification task, the output $y$ is
a binary variable.  However, it is possible to have a multiclass classification task,
where  $y$ can take on more than two possible values.  Although some learning methods can
address directly the multiclass classification task, it is possible to transform the
problem into a binary classification task.  The most common method is the
\emph{one-versus-all} method where we train $l$ binary classifiers, one for each class,
and the class with the highest score is the predicted class.  Another method is the
\emph{one-versus-one} method, where we train $l(l-1)/2$ binary classifiers, one for each
pair of classes, and the class with the most votes is the predicted class.
As one should expected, dealing with more than two classes is more complex than dealing
with only two classes.  If possible, prefer to deal with binary classification tasks first.

\paragraph{Number of inputs and outputs}
Note that the definition of the learning problem does not restrict the number of inputs
and outputs.  The input data can be a scalar, a vector, a matrix, or a tensor, and the
output as well.  The learning machine must be able to handle the input and output data
according to the problem.

% TODO: when discussing bias
% \paragraph{Parametric vs nonparametric models}
% The learning machine generates a set of functions $f_\theta$ where $|\theta|$ can be fixed
% or not.  If $|\theta|$ is always fixed, the model is called \emph{parametric}.  If
% $|\theta|$ is not fixed beforehand, the model is called \emph{nonparametric}.  Parametric
% models are usually simpler and faster, but they are less flexible.  In other words, it is
% up to the researcher to choose the best model ``size'' for the problem.  If the model is
% too small, it will not be able to capture the complexity of the data.  If the model is too
% large, it will be too complex, too slow to train and might overfit to the data.
% Nonparametric models are more flexible, but they usually require more data to be trained.

\section{Optimal solutions}
\label{sec:optimal-solution}

In this section, I show that the optimal solutions for the tasks of binary data
classification and regression estimation depend only on $\Prob(y \mid x)$ (i.e.
discriminative models).  This is useful
to understand how good a solution can possibly be and to derive practical solutions in the
next sections.

\subsection{Bayes classifier}

The optimal solution for the binary data classification task is the \emph{Bayes
classifier}, which minimizes the probability of classification error.  The Bayes
classifier is defined as
\begin{equation*}
  f_\text{Bayes}(x) = \argmax_{y \in \mathcal{Y}} \Prob(y \mid x)\text{.}
\end{equation*}

We can easily see that the Bayes classifier is the optimal solution for the binary data
classification task.  The probability of classification error for an arbitrary classifier
$f$ is
\begin{equation*}
  R(f) = \int \mathbb{1}_{f(x) \neq y}\, d\!\Prob(x, y) =
    \iint \mathbb{1}_{f(x) \neq y}\, d\!\Prob(y | x)\, d\!\Prob(x)\text{,}
\end{equation*}
where $\mathbb{1}_{\cdot}$ is the indicator function that returns one if the condition is
true and zero otherwise.  Let $b(x) = \Prob(y = 1 \mid x)$, we have that
\[
  \int \mathbb{1}_{f(x) \neq y}\, d\!\Prob(y | x) =
    b(x) \mathbb{1}_{f(x) = 0} + (1 - b(x)) \mathbb{1}_{f(x) = 1}\text{,}
\]
which only of the terms is nonzero for each $x$.  Thus, the risk is minimized by choosing
a classifier that $f(x) = 1$ if $b(x) > 1 - b(x)$ and $f(x) = 0$ otherwise.  This is the
Bayes classifier.

Consequently, the \emph{Bayes error rate}, or irreducible error, is the lowest possible loss for any
classifier in a given problem.  The Bayes error rate sums the errors of the
Bayes classifier for each class:
\begin{equation*}
  R_\text{Bayes} = \int \left[
    b(x) \mathbb{1}_{f_\text{Bayes}(x) = 0} +
    \left(1 - b(x)\right) \mathbb{1}_{f_\text{Bayes}(x) = 1}
  \right] d\!\Prob(x)\text{.}
\end{equation*}

We know that $f_\text{Bayes}(x) = 1$ if $b(x) > 0.5$ and $f_\text{Bayes}(x) = 0$ otherwise.
Thus, the Bayes error rate can be rewritten as
\begin{equation*}
  R_\text{Bayes} = \int \min\left\{ b(x), 1 - b(x) \right\}\, d\!\Prob(x)\text{.}
\end{equation*}

\begin{figurebox}[label=fig:bayes-classifier]{Bayes classifier illustration.}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        ticks=none,
        axis x line=bottom,
        axis y line=left,
        xlabel={$x$},
        ymax=0.4,
        xmin=-2.2, xmax=1.4,
      ]
      % P(x|y=0)
      \addplot+[fill=gray, draw=black, opacity=0.2, smooth, mark=none] coordinates {
        (-2, 0.1) (-1.5, 0.2) (-1, 0.35) (-0.5, 0.2) (0, 0.1)
      };
      \node at (axis cs:-1, 0.2) {$\Prob(x \mid y = 0)$};
      % P(x|y=1)
      \addplot+[fill=gray, draw=black, opacity=0.4, smooth, mark=none] coordinates {
        (-0.8, 0.1) (-0.3, 0.2) (0.2, 0.35) (0.7, 0.2) (1.2, 0.1)
      };
      \node at (axis cs:0.2, 0.2) {$\Prob(x \mid y = 1)$};
      % Bayes
      \draw[dashed, gray] (axis cs:-0.3, 0) -- (axis cs:-0.3, 0.4);
    \end{axis}
  \end{tikzpicture}
  \tcblower
  The Bayes classifier is the line that separates the two classes.  The Bayes error is a
  result of the darker area in which the distributions of the classes intersect.
\end{figurebox}

\Cref{fig:bayes-classifier} illustrates the Bayes classifier and its error rate.
The vertical line represents the Bayes classifier that separates the classes the best way
possible in the space of the feature vectors $x$.  Since the distributions $\Prob(x \mid y
= 0)$ and $\Prob(x \mid y = 1)$ may intersect, there is a region where the Bayes
classifier cannot always predict the class correctly.

\subsection{Regression function}

In the regression estimation task, the goal is to approximate the optimal solution, called
\emph{regression function},
\begin{equation}
  \label{eq:regression-function}
  r(x) = \int y\, d\!\Prob(y \mid x)\text{,}
\end{equation}
that is the expected value of the target variable $y$ given the input $x$.

It is easy to show that the regression function minimizes the risk \eqref{eq:risk} with
loss
\[
  \mathcal{L}(y, r(x)) = \big(y - r(x)\big)^2\text{.}
\]
The risk functional for an arbitrary function $f$ is
\begin{multline*}
  R(f) =
    \int \big(y - f(x)\big)^2\, d\!\Prob(x, y) =\\
    \int y^2\, d\!\Prob(y) -
    2 \int f(x) \left[ \int y\, d\!\Prob(y \mid x) \right] d\!\Prob(x) +
    \int f(x)^2\, d\!\Prob(x)\text{,}
\end{multline*}
however we can substitute $r(x)$ for the inner integral and obtain
\begin{multline*}
  R(f) =
    \int y^2\, d\!\Prob(y) - 2 \int f(x) r(x)\, d\!\Prob(x) + \int f(x)^2\, d\!\Prob(x) = \\
    \int y^2\, d\!\Prob(y) + \int \left[ f(x)^2 - 2 f(x) r(x) \right]\, d\!\Prob(x)\text{.}
\end{multline*}
Once the first term is a constant, the risk is minimized by minimizing
\[
  f(x)^2 - 2 f(x) r(x)\text{.}
\]

Deriving the last expression with respect to $f(x)$ and setting it to zero, we obtain
\begin{equation*}
  \frac{d}{d f(x)} \Big[ f(x)^2 - 2 f(x) r(x) \Big] = 2 f(x) - 2 r(x) = 0 \Rightarrow
  f(x) = r(x)\text{.}
\end{equation*}

Like for the Bayes classifier, the stochastic nature of the data leads to an irreducible
error in the regression estimation task.  We have that
\begin{equation*}
  R(r) = \int \big(y - r(x)\big)^2\, d\!\Prob(x, y) =
    \int y^2\, d\!\Prob(y) - \int r(x)^2\, d\!\Prob(x)\text{,}
\end{equation*}
where the first term is
\[
  \E\!\left[y^2\right] = \Var(y) + \E\!\left[y\right]^2
\]
and the second term is
\[
  \E\left[\E\!\left[y \mid x\right]^2\right] =
    \Var\!\left(\E\!\left[y \mid x\right]\right) + \E\!\left[\E\!\left[y \mid x\right]\right]^2 =
    \Var\!\left(\E\!\left[y \mid x\right]\right) + \E\!\left[y\right]^2\text{.}
\]
Thus, the irreducible error is
\[
  R(r) = \Var(y) - \Var\!\left(\E\!\left[y \mid x\right]\right) \text{.}
\]

The interpretation of the irreducible error comes from the law of the total variance:
\begin{equation*}
  \Var(y) = \E\!\left[ \Var(y \mid x) \right] + \Var\!\left(\E\!\left[y \mid x\right]\right)\text{,}
\end{equation*}
where the first term is known as the unexplained variance and the second term, as the
explained variance.  The equality $R(r) = \E\!\left[ \Var(y \mid x) \right]$
captures the idea that this variance is the intrinsic uncertainty that cannot be further
reduced.

\begin{figurebox}[label=fig:explained-unexplained-variance]{Unexplained variance is the
  error of the regression.}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        axis lines=middle,
        xlabel={$x$},
        ylabel={$y$},
        xmin=-0.2, xmax=1.5,
        ymin=-0.5, ymax=1.8,
        xtick={0, 0.5, 1},
        ytick={0, 0.5, 1},
        domain=0:1]

      \addplot[thick] {x} node[right] {$r(x) = x$};

      \addplot [draw=none, fill=gray, opacity=0.2] {x + 1} \closedcycle;
      \addplot [draw=none, fill=gray, opacity=0.2] {x - 1} \closedcycle;
      \draw[Stealth-Stealth, gray] (axis cs:0.5,0.5) -- (axis cs:0.5, 1.5) node[midway, right] {$\sigma = 1$};
      \node at (axis cs:0.5,1.3) [anchor=west] {Unexplained variance};

     \addplot[samples=2, style={dashed}] {0.5} node[midway, below, anchor=north west] {Explained variance};
    \end{axis}
  \end{tikzpicture}
  \tcblower
  Expected error of the regression function for data generated by
  $\Prob(y \mid x) \Prob(x)$ such that $\Prob(y \mid x) = \mathcal{N}(x, 1)$ and
  $\Prob(x) = \mathcal{U}(0, 1)$.  The regression function is $r(x) = x$.
\end{figurebox}

\Cref{fig:explained-unexplained-variance} illustrates the irreducible error of the
regression function for arbitrary distributions $\Prob(y \mid x)$ and $\Prob(x)$.
In this case $\E\!\left[ \Var(y \mid x) \right] = 1$, which means that points drawn
by $\Prob(x, y)$ are distributed around the regression function $r(x)$ with a standard
deviation of one.  The explained variance is the spread of the regression across the
x-domain.

\section{ERM inductive principle}

It is very interesting to study the optimal solution for the learning tasks, but in the
real-world, we do not have access to the distributions $\Prob(x)$ and $\Prob(y \mid x)$.
We must rely on the training data $(x_1, y_1), \dots, (x_n, y_n)$ to infer a solution.

In the following sections, for the sake of simplicity, let $z$ describe the pair $(x, y)$
and $L(z, \theta)$, a generic loss function for the model $f_\theta$.  Note that the
training dataset is thus a set of $n$ i.i.d. samples $z_1, \dots, z_n$.

Since the distribution $\Prob(z)$ is unknown, the risk functional $R(\theta)$ is replaced by
the \emph{empirical risk functional}
\begin{equation}
  \label{eq:empirical-risk}
  R_n(\theta) = \frac{1}{n} \sum_{i=1}^n L(z_i, \theta)\text{.}
\end{equation}

Approximating $R(\theta)$ by the empirical risk functional $R_n(\theta)$ is the so called
\gls{erm} inductive principle.  The ERM principle is the basis of the \gls{slt}.

Traditional methods, such as least-squares, maximum likelihood, and maximum a posteriori are
all realizations of the ERM principle for specific loss functions and hypothesis spaces.

\subsection{Consistency of the learning process}

One important question about the ERM principle is the consistency of the learning process.
Consistency means that, given a sufficient number of samples, the empirical risk
functional $R_n(\theta)$ converges to the true risk functional $R(\theta)$ over the
hypothesis space $\Theta$.

The consistency of the ERM principle is guaranteed by the uniform (two-sided)
convergence\footnote{ Actually, only a weaker one-sided uniform is needed; consult a
detailed explanation in chapter 2 of \fullcite{Vapnik1999b}.  The equivalence is a
consequence of the key theorem of learning proved by Vapnik and Chervonenkis in 1989 and
later translated to English in \fullcite{Vapnik1991}.} of the empirical risk functional
$R_n(\theta)$ to the true risk functional $R(\theta)$ over the hypothesis space $\Theta$.
The uniform convergence is defined as
\[
  \lim_{n \to \infty} \Prob\!\left(
    \sup_{\theta \in \Theta} \Big| R_n(\theta) - R(\theta_{n_0}) \Big| > \epsilon
  \right) = 0\text{.}
\]

\subsection{Rate of convergence}

Beyond consistency, it is also useful to understand the rate at which $R_n(\theta)$
converges to $R(\theta)$ as the sample size $n$ increases.  It is possible for a learning
machine be consistent but have a slow convergence rate, which means that a large number of
samples is needed to achieve a good solution.

The asymptotic rate of convergence of the empirical risk functional $R_n(\theta)$ is
fast if, for any $n > n_0$, the exponential bound
\[
  \Prob\!\big(R(\theta_n) - R(\theta) > \epsilon\big) < \exp\!\left( - c n \epsilon^2 \right)
\]
holds true, where $c$ is a positive constant.

\subsection{VC entropy}

Let $L(z, \theta)$, $\theta \in \Theta$, be a set of bounded loss functions, i.e.
\[
  \left| L(z, \theta) \right| < M\text{,}
\]
for some constant $M$ and all $z$ and $\theta$.  One can construct $n$-dimensional vectors
\[
  l(z_1, \dots, z_n; \theta) = \big[ L(z_1, \theta), \dots, L(z_n, \theta) \big]\text{.}
\]
Once the loss functions are bounded, this set of vectors belongs to a $n$-dimensional
cube and has finite minimal $\epsilon$-net\footnote{An $\epsilon$-net is a set of points
that are $\epsilon$-close to any point in the set.}.

Consider the quantity $N(z_1, \dots, z_n; \Theta, \epsilon)$ that counts the number of
elements of the minimal $\epsilon$-net of that set of vectors.
Once the quantity $N$ is a random variable, we can define the VC entropy as
\[
  H(n; \Theta, \epsilon) = \E\!\left[ \ln N(z_1, \dots, z_n; \Theta, \epsilon) \right]\text{,}
\]
where $z_i$ are i.i.d. samples drawn from some $\Prob(z)$.

If $L(z, \theta)$, $\theta \in \Theta$ is a set of indicator functions (binary
classification task),  we measure the diversity of this set using the quantity $N(z_1,
\dots, z_n; \Theta)$ that counts the number of different separations of the given sample
can be made by the functions.  In this case, the minimal $\epsilon$-net for $\epsilon$ < 1
does not depend on $\epsilon$ and is a subset of the vertices of the unit cube.

A necessary and sufficient condition for the uniform convergence is
\begin{equation}
  \label{eq:uniform-convergence}
  \lim_{n \to \infty} \frac{H(n; \Theta, \epsilon)}{n} = 0\text{,}
\end{equation}
for all $\epsilon > 0$.

The VC entropy measures the complexity of the hypothesis space $\Theta$.  The intuition
behind the need of a decreasing VC entropy with increasing numbers of observations is
related to the nonfalsifiability of the learning machine.  For instance, the set of
functions that can always separate the training data perfectly (contains all the vertices
of the cube) is nonfalsifiable because
it implies that the minimum of the empirical risk is zero independently of the value of
the true risk.

\subsection{Growing function and VC dimension}

It turns out that we can guarantee both the uniform convergence and the fast rate of
convergence independently of $\Prob(z)$.  Actually,
\[
  \lim_{n \to \infty} \frac{G(n; \Theta)}{n} = 0
\]
is the necessary and sufficient condition, where
\[
  G(n; \Theta) = \ln \sup_{z_1, \dots, z_n} N(z_1, \dots, z_n; \Theta)\text{,}
\]
is the \emph{growth function} of the hypothesis space $\Theta$.

\citeauthor{Vapnik1968}\footfullcite{Vapnik1968} showed that the growth function either
satisfies
\[
  G(n; \Theta) = n \ln 2
\]
or is bounded by
\[
  G(n; \Theta) \leq h \left( \ln \frac{l}{n} + 1 \right)\text{,}
\]
where $h$ is an integer.  Thus, the growth function is either linear or logarithmic in
$n$.  In the first case, we say that the \emph{VC dimension} of the hypothesis space is
infinite, and in the second case, the VC dimension is $h$.

A finite VC dimension is enough to imply both consistency and a fast rate of convergence.

\subsubsection{Intuitions about the VC dimension}

% A figure with lines separating separating points in a plane
\begin{figurebox}[label=fig:vc-dimension]{VC dimension of a set of lines in the plane.}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        axis lines=middle,
        xmin=-1, xmax=1,
        ymin=-1, ymax=1,
        xtick={0},
        ytick={0},
        domain=-1:1]

      \addplot[only marks, mark=*, mark size=2pt] coordinates {
        (-0.5, 0.3) (-0.3, -0.5) (0.7, -0.5)
      };

      % diagonal lines separating points
      \addplot[dashed] {x} node[right] {$\theta_1$};
      \addplot[dashed] {-x} node[right] {$\theta_2$};
      \addplot[dashed] {-0.3} node[right] {$\theta_3$};

    \end{axis}
  \end{tikzpicture}
  \tcblower
  The VC dimension of the lines in the plane is equal to 3, since a line can shatter 3
  points in all 8 possible ways, but not four points.
\end{figurebox}

For a set of indicator functions, the VC dimension is the maximum number of vectors that
can be \emph{shattered} by the functions.  If, for any $n$, there is a set of $n$
vectors that can be shattered by the functions, the VC dimension is infinite.
We say that $h$ vectors can be shattered if they can be separated into two classes in all
$2^h$ possible ways.  \Cref{fig:vc-dimension} illustrates the VC dimension of a set of
lines in the plane.

\begin{figurebox}[label=fig:vc-sin]{High-frequency sine wave functions have infinite VC dimension.}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        axis lines=middle,
        xlabel={$x$},
        ylabel={$y$},
        xmin=-1.2, xmax=1.2,
        ymin=-1.2, ymax=1.2,
        xtick={0},
        ytick={0},
        domain=-1:1,
        samples=100]

      \addplot[smooth] {sin(deg(24 * x))};
      \addplot[dashed, smooth] {sin(deg(4 * x))};

      \addplot[only marks, mark=*, mark size=2pt] coordinates {
        (-0.942, 0.586) (-0.562, -0.780) (-0.337, -0.976)
        (0.313, 0.950) (0.562, 0.780) (0.942, -0.586)
      };
    \end{axis}
  \end{tikzpicture}
  \tcblower
  High-frequency sine wave approximate well any set of points even though they may come
  from a low-frequency sine wave or any other function.
\end{figurebox}

One misconception about the VC dimension is that it is related to the number of parameters
of the model.  The VC dimension is related to the complexity of the hypothesis space, not
to the number of parameters.  For instance, the VC dimension of functions
\[
  f(z; \theta) = \mathbb{1}_{\sin \theta x > 0}
\]
is infinite, even though the parameter $\theta$ is a scalar.  See \cref{fig:vc-sin}.
By increasing the frequency $\theta$ of the sine wave, the function can approximate any set
of points.

This opens remarkable opportunities to find good solutions containing a huge number of
parameters\footnote{Sometimes, like in a linear model, the number of parameters is
proportional to the number of dimensions of the feature vector.} but with a finite VC
dimension.  % TODO: discuss non-parametric here?

\section{Structural risk minimization}

The \gls{erm} principle is a powerful tool to study the generalization ability of the
learning process.  However, it relies on the hypothesis that the number of samples
tends to infinity.

% TODO what about finite n?

\section{Intuitions about learning bias}

\emph{Learning bias}, or \emph{inductive bias}, is the set of assumptions that the
learning machine uses to generate the set of functions $\left\{ f_\theta : \theta \in
\Theta \right\}$ --- see \cref{fig:learning-bias}.  Vapnik shows that the learning bias is the key to the generalization
ability of the learning process: the smaller the learning bias, the better the
generalization ability \parencite{Vapnik1999b}.  In an illustrative thought experiment,
one can see that ``no bias means no learning,'' since the learning machine would generate
all possible functions, which is impossible, including the function that perfectly fits
the training data but fails to generalize.

\begin{figurebox}[label=fig:learning-bias]{Learning bias illustration.}
  % A figure with points in the sapce \Theta and arrows between possible solutions
  \centering
  \begin{tikzpicture}
    \draw[outline] (0,0) circle (25mm) node {};
    \node at (0, 3) {$\Theta$};

    \node (a) at (0, 1) {$\theta_1$};
    \node (b) at (1, -1) {$\theta_2$};
    \node (c) at (-1, -1) {$\theta_3$};

    \draw[->] (a) -- (b);
    \draw[->] (b) -- (c);
  \end{tikzpicture}
  \tcblower
  A learning machine searches for the best parameter $\theta$ in the space $\Theta$.
  The learning bias is the set of assumptions that the learning machine uses to control
  how and where to search for the best parameter.
\end{figurebox}

It is important to understand the learning bias of the main machine-learning methods.  Let's
consider the binary classification task, which is more intuitive.  Some common learning
methods are the perceptron, the multi-layer perceptron, the decision tree, and the
$k$-nearest neighbors.

\begin{tablebox}[label=tab:paradigm]{Learning machines paradigms and characteristics.}
  \begin{tabularx}{\textwidth}{lX@{}X@{}}
    \toprule
    \textbf{Method} & \textbf{Paradigm} & \textbf{Characteristics} \\
    \midrule
    Perceptron & Functional (parametric) & The perceptron is a linear classifier that generates a hyperplane that separates the classes. \\
    MLP & Functional (parametric) & The multi-layer perceptron is a non-linear classifier that generates a set of hyperplanes that separates the classes. \\
    DT & Symbolic (nonparametric) & The decision tree is a classifier that comprises a set of rules that separate the classes. \\
    $k$-NN & Instance-based (nonparametric) & The $k$-nearest neighbors is a classifier that classifies the data based on the majority of the $k$-nearest neighbors. \\
    \bottomrule
  \end{tabularx}
\end{tablebox}

For the examples in the following subsections, we consider the datasets for the AND
and the XOR problem --- see \cref{tab:and-xor}.

\begin{tablebox}[label=tab:and-xor]{AND and XOR datasets.}
  \centering
  \begin{minipage}{0.45\textwidth}
    \centering
    \rowcolors{2}{black!10!white}{}
    \begin{tabular}{ccc}
      \toprule
      $x_1$ & $x_2$ & $y = x_1 \land x_2$ \\
      \midrule
      0 & 0 & 0 \\
      0 & 1 & 0 \\
      1 & 0 & 0 \\
      1 & 1 & 1 \\
      \bottomrule
    \end{tabular}
  \end{minipage}
  \begin{minipage}{0.45\textwidth}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{ccc}
    \toprule
    $x_1$ & $x_2$ & $y = x_1 \oplus x_2$ \\
    \midrule
    0 & 0 & 0 \\
    0 & 1 & 1 \\
    1 & 0 & 1 \\
    1 & 1 & 0 \\
    \bottomrule
  \end{tabular}
  \end{minipage}
  \tcblower
  The AND and XOR datasets are binary classification datasets where the output $y$ is the
  ``logical AND'' and the ``exclusive OR'' of the inputs $x_1$ and $x_2$, i.e.
  $y = x_1 \land x_2$ and $y = x_1 \oplus x_2$.
\end{tablebox}

\subsection{Perceptron learning bias}

The perceptron is a linear classifier that generates a hyperplane that separates the
classes.  The model for our example is
\begin{equation*}
  f(x_1, x_2; \theta = \vec{w} = \left[w_0, w_1, w_2\right]) =  \begin{cases}
    1 & \text{if } w_0 + w_1 x_1 + w_2 x_2 > 0 \\
    0 & \text{otherwise.}
  \end{cases}
\end{equation*}
As one can see, the perceptron learning bias is the assumption that the classes are
linearly separable. The equation $\vec{w} \cdot \vec{x} = 0$, where $\vec{x} = [1, x_1,
x_2]$, is the equation of a hyperplane.

\begin{figurebox}[label=fig:perceptron]{Perceptron learning bias.}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        axis x line=bottom,
        axis y line=left,
        xlabel={$x_1$},
        ylabel={$x_2$},
        width=0.6\textwidth,
        height=0.6\textwidth,
        xtick={0, 1},
        ytick={0, 1},
        grid=both,
        xmin=-0.5, xmax=1.5,
        ymin=-0.5, ymax=1.5,
      ]
      \addplot+[only marks, mark=+, mark size=3pt] coordinates {
        (0, 1) (1, 0)
      };
      \addplot+[only marks, mark=*, mark size=3pt] coordinates {
        (0, 0) (1, 1)
      };
      \addplot+[domain=0:1.5, mark=none, black] {-0.5 + x};
    \end{axis}
  \end{tikzpicture}
  \tcblower
  The perceptron learning bias is the assumption that the classes are linearly separable.
  The hyperplane that separates the classes is the learning machine model.
  In this case, $w_0 = -0.5$, $w_1 = 1$, and $w_2 = -1$.
\end{figurebox}

In \cref{fig:perceptron}, we show the hyperplane that the model $\vec{w} = [-0.5, 1, -1]$
generates for the XOR dataset.  As one can see, the classes are not linearly separable,
and the perceptron model fails to classify the dataset correctly, see \cref{tab:xor-perceptron}.

\begin{tablebox}[label=tab:xor-perceptron]{Truth table for the predictions of the perceptron.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{ccc|cc}
    \toprule
    $x_1$ & $x_2$ & $y$ & $-0.5 + x_1 - x_2$ & $\hat{y}$ \\
    \midrule
    0 & 0 & 0 & -0.5 & 0 \\
    0 & 1 & 1 & -1.5 & 0 \\
    1 & 0 & 1 & 0.5 & 1 \\
    1 & 1 & 0 & -0.5 & 0 \\
    \bottomrule
  \end{tabular}
  \tcblower
  The perceptron model with parameters $w_0 = -0.5$, $w_1 = 1$, and $w_2 = -1$
  fails to classify the XOR dataset correctly --- as any other perceptron would do.
\end{tablebox}

\begin{hlbox}{Think about\dots}
  What happens if we remove $w_0$ from the model?
\end{hlbox}

\subsection{Multi-layer perceptron learning bias}

The multi-layer perceptron (MLP) is a non-linear classifier that generates a set of hyperplanes
that separates the classes.  In order to simplify the understanding, consider the
that the activation function of the hidden layer is the discrete step function
\begin{equation*}
  \sigma(x) = \begin{cases}
    1 & \text{if } x > 0 \\
    0 & \text{otherwise.}
  \end{cases}
\end{equation*}
A model with two neurons in the hidden layer (effectively the combination of three
perceptrons) is
\begin{multline*}
  f(x_1, x_2; \theta = \left\{ \vec{w}^{(1)}, \vec{w}^{(2)}, \vec{w}^{(3)} \right\}) = \\
  \sigma\left(
    \vec{w}^{(3)} \cdot \left[1, \sigma(\vec{w}^{(1)} \cdot \vec{x}), \sigma(\vec{w}^{(2)} \cdot \vec{x})\right]
  \right)\text{.}
\end{multline*}

The parameters $\vec{w}^{(1)}$ and $\vec{w}^{(2)}$ represent the hyperplanes that separate
the classes in the hidden layer, and $\vec{w}^{(3)}$ represents how the hyperplanes are
combined to generate the output.  If we set $\vec{w}^{(1)} = [-0.5, 1, -1]$ (like the
perceptron in the previous example) and $\vec{w}^{(2)} = [-0.5, -1, 1]$, we use the third neuron
to combine the results of the first two neurons.  This way, solution for the XOR problem is
setting $\vec{w}^{(3)} = [0, 1, 1]$.

\begin{figurebox}[label=fig:mlp]{MLP learning bias.}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        axis x line=bottom,
        axis y line=left,
        xlabel={$x_1$},
        ylabel={$x_2$},
        width=0.6\textwidth,
        height=0.6\textwidth,
        xtick={0, 1},
        ytick={0, 1},
        grid=both,
        xmin=-0.5, xmax=1.5,
        ymin=-0.5, ymax=1.5,
      ]
      \addplot+[only marks, mark=+, mark size=3pt] coordinates {
        (0, 1) (1, 0)
      };
      \addplot+[only marks, mark=*, mark size=3pt] coordinates {
        (0, 0) (1, 1)
      };
      \addplot+[domain=0:1.5, mark=none, black] {-0.5 + x};
      \addplot+[domain=-0.5:1.5, mark=none, black] {0.5 + x};
    \end{axis}
  \end{tikzpicture}
  \tcblower
  \dots
\end{figurebox}

\begin{tablebox}[label=tab:xor-mlp]{Truth table for the predictions of the MLP.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{ccc|cccc}
    \toprule
    $x_1$ & $x_2$ & $y$ & \nth{1} neuron & \nth{2} neuron & $\hat{y}$ \\
    \midrule
    0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 1 & 1 & 0 & 1 & 1 \\
    1 & 0 & 1 & 1 & 0 & 1 \\
    1 & 1 & 0 & 0 & 0 & 0 \\
    \bottomrule
  \end{tabular}
  \tcblower
  \dots
\end{tablebox}

\begin{hlbox}{Think about\dots}
  Note that there are many possible solutions for the XOR problem using the MLP.
\end{hlbox}

\subsection{Decision tree learning bias}

The decision tree is a non-linear classifier that generates a set of hyperplanes that
are orthogonal to the axes.  Consider the decision tree in \cref{fig:tree-and}.

\begin{figurebox}[label=fig:tree-and]{Decision tree representation.}
  \centering
  \begin{tikzpicture}
    \node[decision] (x1) at (0, 0) {$x_1$};
    \node[block] (n1) at (-2, -1.5) {$\hat{y} = 0$};
    \node[decision] (x2) at (2, -1.5) {$x_2$};
    \node[block] (n2) at (0, -3) {$\hat{y} = 0$};
    \node[block] (p) at (4, -3) {$\hat{y} = 1$};

    \draw (x1) -| (n1) node [midway, above] {$\leq 0.5$};
    \draw (x1) -| (x2) node [midway, above] {$>0.5$};
    \draw (x2) -| (n2) node [midway, above] {$\leq 0.5$};
    \draw (x2) -| (p) node [midway, above] {$>0.5$};
  \end{tikzpicture}
  \tcblower
  The decision tree that separates the AND dataset.
\end{figurebox}

\begin{figurebox}[label=fig:tree-bias]{Decision tree learning bias.}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        axis x line=bottom,
        axis y line=left,
        xlabel={$x_1$},
        ylabel={$x_2$},
        width=0.6\textwidth,
        height=0.6\textwidth,
        xtick={0, 1},
        ytick={0, 1},
        grid=both,
        xmin=-0.5, xmax=1.5,
        ymin=-0.5, ymax=1.5,
      ]
      \addplot+[only marks, mark=+, mark size=3pt] coordinates {
        (1, 1)
      };
      \addplot+[only marks, mark=*, mark size=3pt] coordinates {
        (0, 0) (0, 1) (1, 0)
      };
      \addplot+[domain=0.5:1.5, mark=none, black, thick] {0.5};
      \addplot+[mark=none, black, thick] coordinates {(0.5, -0.5) (0.5, 1.5)};
    \end{axis}
  \end{tikzpicture}
  \tcblower
  The decision tree learning bias is the assumption that the classes can be separated with
  hyperplanes orthogonal to the axes.
\end{figurebox}

\begin{hlbox}{Think about\dots}
  Decision tree are nonparametric models, one can easily increase the depth of the tree to
  fit the data.
\end{hlbox}

\subsection{$k$-nearest neighbors learning bias}

The $k$-nearest neighbors ($k$-NN) is a non-linear nonparametric classifier that
generate arbitrarily complex decision boundaries by ``memoring'' the training data.
The behavior of the boundaries depends on the value of $k$ and the distance metric one
uses to find the nearest neighbors of a point.

\begin{figurebox}[label=fig:1nn-bias]{1-NN learning bias.}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        axis x line=bottom,
        axis y line=left,
        xlabel={$x_1$},
        ylabel={$x_2$},
        width=0.6\textwidth,
        height=0.6\textwidth,
        xtick={0, 1},
        ytick={0, 1},
        grid=both,
        xmin=-0.5, xmax=1.5,
        ymin=-0.5, ymax=1.5,
      ]
      \addplot+[only marks, mark=+, mark size=3pt] coordinates {
        (1, 1)
      };
      \addplot+[only marks, mark=*, mark size=3pt] coordinates {
        (0, 0) (0, 1) (1, 0)
      };
      \addplot+[domain=0.5:1.5, mark=none, black, thick] {0.5};
      \addplot+[mark=none, black, thick] coordinates {(0.5, 0.5) (0.5, 1.5)};
    \end{axis}
  \end{tikzpicture}
  \tcblower
  In this particular case, the 1-NN boundaries match the decision tree boundaries.
\end{figurebox}

As $k$ increases the boundaries become smoother:
\href{https://images.squarespace-cdn.com/content/v1/5d782753c70af105c29a9b14/1580261947016-XODPUVKWPGGMJJMAXSNF/Screen+Shot+2020-01-28+at+8.38.55+PM.png}{example}.

See illustration: \href{https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html}{here}.

% vim: spell spelllang=en
