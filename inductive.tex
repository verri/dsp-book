\chapter{Learning from data}
\label{chap:slt}

\chapterprecishere{%
  To  understand  God's  thoughts  we  must study statistics, for these are the measure of His purpose.
  \par\raggedleft--- \textup{Florence Nightingale}, her diary}

As we discussed before, in this book, I focus on the problem of inferring a solution for a
predictive task from data.  In this chapter, we introduce the basic concepts of the
\gls{slt}, a general framework for predictive learning tasks.

\begin{mainbox}{Chapter remarks}

  \boxsubtitle{Contents}

  \startcontents[chapters]
  \printcontents[chapters]{}{1}{}
  \vspace{1em}

  \boxsubtitle{Context}

  \begin{itemize}
    \itemsep0em
    \item \dots
  \end{itemize}

  \boxsubtitle{Objectives}

  \begin{itemize}
    \itemsep0em
    \item \dots
  \end{itemize}

  \boxsubtitle{Takeways}

  \begin{itemize}
    \itemsep0em
    \item \dots
  \end{itemize}
\end{mainbox}

{}
\clearpage

\section{Introduction} % TODO: I don't like this title

Several problems can be addressed by techniques that use data somehow.  Once we focus on
one particular problem --- inductive learning ---, we need to define the scope of the
tasks we are interested in.  Let us start from the broader fields to the more specific
ones.

\Gls{ai} is a very broad field, including not only the study of algorithms
that exhibit intelligent behavior, but also the study of the behavior of intelligent
systems.  For instance, it encompasses the study of optimization methods, bioinspired algorithms,
robotics, philosophy of mind, and many other topics.  We are interested in the subfield of
artificial intelligence that studies algorithms that exhibit some form of intelligent
behavior.

A more specific subfield of \gls{ai} is \gls{ml}, which studies algorithms that
enable computers to automatically learn and improve their performance on a task from
experience, without being explicitly programmed by a human being.

Programming a computer to play chess is a good example of the difference between
traditional \gls{ai} and \gls{ml}.  In traditional \gls{ai}, a human programmer
would write a program that contains the rules of chess and the strategies to play the game.
The algorithm might even ``search'' among the possible moves to find the best one.  In
\gls{ml}, the programmer would write a program that learns to play chess by playing
against itself, against other programs, or even from watching games played by humans.
The system would learn the rules of chess and the strategies to play the game by itself.

This field is particularly useful when the task is too complex to be solved by
traditional programming methods or when we do not know how to solve the task.
Among the many tasks that can be addressed by \gls{ml}, we can specialize even more.

Predictive learning is the \gls{ml} paradigm that focuses on making predictions about
outcomes (sometimes about the future) based on historical data.  Predictive tasks
involve predicting the value of a target variable based on the values of one or more
input variables\footnote{Descriptive learning, which is out of the scope of this book,
focuses on describing the relationships between variables in the data without the
need of a target variable.}.

Depending on the reasoning behind the learning algorithms, we can divide the learning
field into two main approaches: \emph{inductive learning} and \emph{transductive
learning}\footnote{Trasduction is the process obtaining specific knowledge from specific
observation, and it is not the focus of this book.}.

Inductive learning involves deriving general rules from specific observations.  The
general rules can make predictions about \emph{any} new instances.  Such an approach
is exactly what we want to apply the project methodology we described in
\cref{sec:our-approach}:  the solution is the general rule inferred from the data.

\begin{figurebox}[label=fig:learning]{Organizational chart of the learning field.}
  \centering
  \begin{tikzpicture}
    \draw[outline] (0,0) circle (30mm) node {};
    \node[below] at (0, 2.6) {artificial intelligence};
    \draw[outline] (0,-0.5) circle (25mm) node {};
    \node[below] at (0, 1.6) {machine learning};
    \draw[outline] (0,-1) circle (20mm) node {};
    \node[below] at (0, 0.5) {predictive learning};
    \draw[outline] (0,-1.5) circle (15mm) node {};
    \node[below] at (0, -1.0) {inductive learning};
  \end{tikzpicture}
  \tcblower
  Artificial intelligence studies algorithms that exhibit intelligent behavior and the
  behavior of intelligent systems.  Machine learning is a subfield of artificial
  intelligence that studies algorithms that enable computers to automatically learn from
  data.  Predictive learning which focuses on making predictions about outcomes given
  known input data.  Inductive learning is a yet more specific type of learning that
  involves deriving general rules from specific observations.
\end{figurebox}

\Cref{fig:learning} give us a hierarchical view of the learning field.  Alternatives ---
such as descriptive learning in opposition to predictive learning, or transductive
learning in opposition to inductive learning --- are out of the scope of this book.

Maybe the most general (and useful) framework for predictive learning is \gls{slt}.
In this chapter, we will introduce the basic concepts of this theory and discuss the
properties of the main \gls{ml} methods.

\section{Hypotheses and general setup}

Consider the set
\begin{equation}
  \label{eq:training-set}
  \big\{(\vec{x}_i, y_i) : i = 1, \dots, n \big\}
\end{equation}
where each sample $i$ is associated with a feature vector $\vec{x}_i \in \mathcal{X}$ and a target variable
$y_i \in \mathcal{Y}$.  We assume that samples are random independent identically
distributed (i.i.d.) observations drawn according to $$\Prob(x, y) = \Prob(y \mid x) \Prob(x)\text{.}$$
Both distributions $\Prob(x)$ and $\Prob(y \mid x)$ are fixed but unknown.

This is equivalent to the original setup stated by \textcite{Vapnik1999b}, where
a generator produce random vectors $\vec{x}$ according to a fixed but unknown
probability distribution $\Prob(x)$ and a supervisor returns an output value $y$ for every
input vector $x$ according to a conditional distribution function $\Prob(y \mid x)$, also fixed but
unknown.

Moreover, note that this setup is compatible with the idea of tidy data and 3NF (see
\cref{sub:bridge}). Of course, we assume $X, Y$ are only the measured variables (or
non-prime attributes).  In practice, it means that we left aside the keys in the learning
process.

In terms of the tables defined in \cref{sec:formal-structured-data}, any row $r$ in the
table $T = (K, H, c)$, in the desired observational unit, such that $\rowcard[r] > 0$, and
$h \in H$ the chosen target variable, we have a corresponding target $y = c(r, h)$ and a
feature vector $\vec{x}$ corresponds to the tuple $$\big(c(r, h') : h' \in H \setminus
\left\{ h \right\}\big)\text{.}$$  Similarly, the variables $K$ that describe each unit
are left aside, as it does not make sense to infer general rules from them.

\section{The learning problem}

Consider a \emph{learning machine} capable of generating a set of functions $f(x;
\theta) \equiv f_\theta(x)$, $\theta \in \Theta$ and $f_\theta : \mathcal{X} \rightarrow \mathcal{Y}$.
The problem of learning is that of choosing, among all possible $f_\theta$, the one that
predicts the target variable the best possible way.

In order to learn, we must first define the \emph{loss} (or discrepancy) $\mathcal{L}$
between the response $y$ to a given input $x$, drawn from $\Prob(x, y)$, and the
response provided by the learned function.

Then, given the \emph{risk function}
\begin{equation}
  \label{eq:risk}
  R(\theta) = \int \mathcal{L}(y, f_\theta(x))\, d\!\Prob(x, y)\text{,}
\end{equation}
the goal is to find the function $f_\theta$ that minimizes $R(\theta)$
where the only available information is the \emph{training set} given by \eqref{eq:training-set}.
This is the \gls{erm} problem.

This formulation encompasses many specific problems. I focus on the two of them which I
believe are the most fundamental ones: \emph{binary data classification}\footnote{Vapnik
calls it \emph{pattern recognition}.} and \emph{regression estimation}\footnote{We are not
talking about \emph{regression analysis}, it is closer to the \emph{scoring} task
definition by \textcite{Zumel2019}.}.  I left aside the density estimation problem, once
it is not
addressed in the remaining of the book.

\paragraph{Binary data classification task}  In this task, the output $y$ takes on
only two possible values, zero or one --- usually called the negative and the positive
class, respectively ---, and the functions $f_\theta$ are indicator
functions. Choosing the loss
\begin{equation*}
  \mathcal{L}(y, f_\theta(x)) = \begin{cases}
    0 & \text{if } y = f_\theta(x) \\
    1 & \text{if } y \neq f_\theta(x)\text{,}
  \end{cases}
\end{equation*}
the risk $\eqref{eq:risk}$ becomes the probability of
classification error.  The function $f_\theta$, in this case, is called \emph{classifier}
and $y$ is called the \emph{label}.

\paragraph{Regression estimation task} Let the outcome $y$ be a real value and
the \emph{regression} $r$ be, by definition, $$r(x) = \int y\, d\!\Prob(y|x) \text{.}$$

The regression is the function that minimizes the risk \eqref{eq:risk} with loss
\begin{equation*}
  \mathcal{L}(y, r(x)) = \big(y - r(x)\big)^2\text{.}
\end{equation*}

If $r \not\in \left\{ f_\theta : \theta\in\Theta \right\}$ --- i.e. if the regression function
cannot be generated by the learning machine ---, the function $f_{\theta'}$
that minimizes the risk function is the closest to the regression function in the
metric $l_2$.  In other words, we look for $\theta'$ such that
\begin{equation*}
  \theta' = \argmin_{\theta \in \Theta} \sqrt{
    \int \big(r(x) - f_\theta(x)\big)^2\, d\!\Prob(x)
  }\text{.}
\end{equation*}
A function $f_\theta$, in this case, is called a \emph{regressor}.

\subsection{A few remarks}

These two tasks are pretty general and can be applied to a wide range of problems.  The
modeling of the task at hand and choice of the loss function is crucial to the success of
the learning process.

About these learning tasks, we can make a few remarks.

\paragraph{Supervised and semisupervised learning}
In both cases, classification and regression estimation, the learning task is to find the function
that maps the input data to the output data in the best possible way.  Although the
learning machine described generate models in a \emph{supervised} manner --- i.e. target
is known for all samples in the training set ---, there are
alternative ways to solve the inductive learning problem, such as the \emph{semisupervised}
approach, where the model can be trained with a small subset of labeled data and a large
subset of unlabeled data --- that is, data whose outputs $y$ are unknown.

\paragraph{Generative and discriminative models}
Any learning machine generates a model that describes the relationship between the input
and output data.  This model can be generative or discriminative.  Generative models
describe the joint probability distribution $\Prob(x, y)$ and can also be used to generate new
data.  Discriminative models, on the other hand, describe the conditional probability
distribution $\Prob(y \mid x)$ directly and can only be used to make predictions. Generative models are
usually much more complex than discriminative models\footnote{Since modeling $\Prob(x, y)$
indirectly models $\Prob(y \mid x)$ and $\Prob(x)$.}, but they hold more information about
the data.  If you only
need to solve the predictive problem, prefer a discriminative model.

\paragraph{Multiclass classification}
In the binary classification task, the output $y$ is
a binary variable.  However, it is possible to have a multiclass classification task,
where  $y$ can take on more than two possible values.  Although some learning methods can
address directly the multiclass classification task, it is possible to transform the
problem into a binary classification task.  The most common method is the
\emph{one-versus-all} method where we train $l$ binary classifiers, one for each class,
and the class with the highest score is the predicted class.  Another method is the
\emph{one-versus-one} method, where we train $l(l-1)/2$ binary classifiers, one for each
pair of classes, and the class with the most votes is the predicted class.
As one should expected, dealing with more than two classes is more complex than dealing
with only two classes.  If possible, prefer to deal with binary classification tasks first.

\paragraph{Number of inputs and outputs}
Note that the definition of the learning problem does not restrict the number of inputs
and outputs.  The input data can be a scalar, a vector, a matrix, or a tensor, and the
output as well.  The learning machine must be able to handle the input and output data
according to the problem.

% TODO: when discussing bias
% \paragraph{Parametric vs nonparametric models}
% The learning machine generates a set of functions $f_\theta$ where $|\theta|$ can be fixed
% or not.  If $|\theta|$ is always fixed, the model is called \emph{parametric}.  If
% $|\theta|$ is not fixed beforehand, the model is called \emph{nonparametric}.  Parametric
% models are usually simpler and faster, but they are less flexible.  In other words, it is
% up to the researcher to choose the best model ``size'' for the problem.  If the model is
% too small, it will not be able to capture the complexity of the data.  If the model is too
% large, it will be too complex, too slow to train and might overfit to the data.
% Nonparametric models are more flexible, but they usually require more data to be trained.

\section{ERM inductive principle}

In the following sections, $z$ describes the pair $(x, y)$ and $L(z, \theta)$ a generic loss
function.  The training dataset is thus a set of $n$ i.i.d. samples $z_1, \dots, z_n$.

Since the distribution $\Prob(z)$ is unknown, the risk functional $R(\theta)$ is replaced by
the \emph{empirical risk functional}
\begin{equation}
  \label{eq:empirical-risk}
  R_n(\theta) = \frac{1}{n} \sum_{i=1}^n L(z_i, \theta)\text{.}
\end{equation}

Approximating $R(\theta)$ by the empirical risk functional $R_n(\theta)$ is the so called
ERM inductive principle.  The ERM principle is the basis of the statistical learning
theory.

Classical methods, such as least-squares, maximum likelihood, and maximum a posteriori are
all realizations of the ERM principle for specific loss functions and hypothesis spaces.

In the following sections, we address the four main questions of learning theory.  We
summarize them in \cref{tab:learning-questions}.

\begin{tablebox}[label=tab:learning-questions]{The four main questions of learning theory.}
  \begin{tabularx}{\textwidth}{@{}lX@{}}
    \toprule
     & \textbf{Question} \\
    \midrule
    \textbf{Consistency} &
      What are the necessary and sufficient conditions for consistency of a learning process? \\
    \textbf{Rate of convergence} &
      How fast is the rate of convergence of the learning process? \\
    \textbf{Generalization} &
      How can one controle the generalization ability of the learning process? \\
    \textbf{Construction} &
      How can one construct a learning machine that satisfies the conditions of consistency and generalization? \\
    \bottomrule
  \end{tabularx}
\end{tablebox}

\section{Consistency of learning processes}

Addressing consistency of a learning process means that we are interested in the
convergence of the empirical risk functional $R_n(\theta)$ to the risk functional
$R(\theta)$ as $n \to \infty$.  In other words, it is an asymptotic theory about the
behavior of the empirical risk functional as the sample size $n$ goes to infinity.

The necessary and sufficient conditions for consistency give us guarantees that the learning
process is general and cannot be improved given our premises.  The most important topic in
this section is the Vapnik-Chervonenkis (VC) entropy.

\subsection{Definition of consistency}

An ERM method is consistent if it produces a sequence of functions $f_{\theta_n}$, for
$n = 1, 2, \dots$, for which both the expected risk and the empirical risk converge to the
their minimum values.

\begin{defbox}{Consistency of a learning process}{}
  Let $\theta_n$ be the solution of
  \begin{equation*}
    \theta_n = \argmin_{\theta \in \Theta} R_n(\theta)\text{.}
  \end{equation*}

  An ERM method is consistent for the set of functions $\left\{ L(z, \theta) : \theta \in
  \Theta \right\}$ and the probability distribution $\Prob(z)$ if
  \begin{align*}
    \lim_{n \to \infty} R(\theta_n) &= \inf_{\theta \in \Theta} R(\theta)\text{,} \\
    \lim_{n \to \infty} R_n(\theta_n) &= \inf_{\theta \in \Theta} R(\theta)\text{.}
  \end{align*}
\end{defbox}

This definition means that one can estimate the risk functional $R(\theta)$ by the
empirical risk functional $R_n(\theta)$, while the values of achieved risks converge to
the minimum value of the risk functional.  See \cref{fig:consistency}.

\begin{figurebox}[label=fig:consistency]{Convergence of the empirical and expected risk functionals.}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        ticks=none,
        axis x line=bottom,
        axis y line=left,
        ymin=0,
        ymax=11,
        xlabel={$n$},
        legend pos=south east,
        legend style={draw=none, fill=none},
        width=0.9\textwidth,
        height=0.6\textwidth,
      ]
      \addplot+[smooth, mark=none, black] coordinates {
        (1, 10) (2, 6) (3, 5) (5, 4.1)
      };
      \addlegendentry{$R(\theta_n)$}
      \addplot+[smooth, mark=none, dashed, black] coordinates {
        (1, 0) (2, 0.1) (3, 3) (5, 3.9)
      };
      \addlegendentry{$R_n(\theta_n)$}
      \draw[dotted, gray] (axis cs:6, 4) -- (axis cs:-1, 4);
    \end{axis}
    \node[gray] at (1, 1.8) {$\inf_{\theta \in \Theta} R(\theta)$};
  \end{tikzpicture}
\end{figurebox}

However, since this definition of consistency includes cases of trivial consistency, there
is no way to obtain such conditions.

Consider the following example.  Suppose we have found a set of functions $\left\{ f_\theta
: \theta \in \Theta \right\}$ such that the ERM method is not consistent.  Let's add
one more function $\phi(z)$ to the set, such that
\begin{equation*}
  \inf_{\theta \in \Theta} L(z, \theta) > \phi(z),~\forall z\text{.}
\end{equation*}
It is straightforward to see that the ERM method is consistent for the new set of
functions $\left\{ L(z, \theta) : \theta \in \Theta \right\} \cup \left\{ \phi \right\}$
and the probability distribution $\Prob(z)$.  In this case, the function $\phi(z)$ gives both
the minimum value of the risk functional and the empirical risk functional.  This is
illustrated in \cref{fig:trivial-consistency}.

% A tikz picture showing the senoidal functions L(z, theta) and phi(z) and the infimum of
% the risk functionals.
\begin{figurebox}[label=fig:trivial-consistency]{An illustrative case of trivial consistency.}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        ticks=none,
        axis x line=bottom,
        axis y line=left,
        ymin=-4,
        ymax=11,
        xlabel={$z$},
        legend pos=south east,
        legend style={draw=none, fill=none},
        width=0.9\textwidth,
        height=0.6\textwidth,
      ]
      \addplot+[smooth, mark=none, black] coordinates {
        (0, 10) (1, 6) (2, 5) (3, 4.1) (4, 5) (5, 6) (6, 8) (8, 7) (10, 10)
      };
      \addplot+[smooth, mark=none, black] coordinates {
        (0, 4) (1, 4.1) (2, 7) (3, 7.9) (4, 7) (5, 4.1) (10, 4)
      };
      \addplot+[smooth, mark=none, black] coordinates {
        (0, 4) (1, 3.1) (2, 6) (3, 6.9) (4, 6) (5, 3.1) (10, 5)
      };
      \addplot+[smooth, mark=none, black] coordinates {
        (0, 7) (3, 5.1) (5, 5) (7, 5.9) (8, 5) (9, 7.1) (10, 5)
      };
      \addlegendentry{$L(z, \theta)$, $\theta \in \Theta$}
      \addplot+[smooth, mark=none, black, dashed] coordinates {
        (0, 3) (5, 1) (10, 3.5)
      };
      \addlegendentry{$\phi(z)$}
    \end{axis}
  \end{tikzpicture}
\end{figurebox}

\subsection{Nontrivial consistency}


\section{Rate of convergence of learning processes}

\section{Generalization ability of learning processes}

\section{Construction of learning machines}

\subsection{Data classification methods}

\subsection{Regression estimation methods}

\newpage
\section{Learning bias}

\emph{Learning bias}, or \emph{inductive bias}, is the set of assumptions that the
learning machine uses to generate the set of functions $\left\{ f_\theta : \theta \in
\Theta \right\}$ --- see \cref{fig:learning-bias}.  Vapnik shows that the learning bias is the key to the generalization
ability of the learning process: the smaller the learning bias, the better the
generalization ability \parencite{Vapnik1999b}.  In an illustrative thought experiment,
one can see that ``no bias means no learning,'' since the learning machine would generate
all possible functions, which is impossible, including the function that perfectly fits
the training data but fails to generalize.

\begin{figurebox}[label=fig:learning-bias]{Learning bias illustration.}
  % A figure with points in the sapce \Theta and arrows between possible solutions
  \centering
  \begin{tikzpicture}
    \draw[outline] (0,0) circle (25mm) node {};
    \node at (0, 3) {$\Theta$};

    \node (a) at (0, 1) {$\theta_1$};
    \node (b) at (1, -1) {$\theta_2$};
    \node (c) at (-1, -1) {$\theta_3$};

    \draw[->] (a) -- (b);
    \draw[->] (b) -- (c);
  \end{tikzpicture}
  \tcblower
  A learning machine searches for the best parameter $\theta$ in the space $\Theta$.
  The learning bias is the set of assumptions that the learning machine uses to control
  how and where to search for the best parameter.
\end{figurebox}

It is important to understand the learning bias of the main machine-learning methods.  Let's
consider the binary classification task, which is more intuitive.  Some common learning
methods are the perceptron, the multi-layer perceptron, the decision tree, and the
$k$-nearest neighbors.

\begin{tablebox}[label=tab:paradigm]{Learning machines paradigms and characteristics.}
  \begin{tabularx}{\textwidth}{lX@{}X@{}}
    \toprule
    \textbf{Method} & \textbf{Paradigm} & \textbf{Characteristics} \\
    \midrule
    Perceptron & Functional (parametric) & The perceptron is a linear classifier that generates a hyperplane that separates the classes. \\
    MLP & Functional (parametric) & The multi-layer perceptron is a non-linear classifier that generates a set of hyperplanes that separates the classes. \\
    DT & Symbolic (nonparametric) & The decision tree is a classifier that comprises a set of rules that separate the classes. \\
    $k$-NN & Instance-based (nonparametric) & The $k$-nearest neighbors is a classifier that classifies the data based on the majority of the $k$-nearest neighbors. \\
    \bottomrule
  \end{tabularx}
\end{tablebox}

For the examples in the following subsections, we consider the datasets for the AND
and the XOR problem --- see \cref{tab:and-xor}.

\begin{tablebox}[label=tab:and-xor]{AND and XOR datasets.}
  \centering
  \begin{minipage}{0.45\textwidth}
    \centering
    \rowcolors{2}{black!10!white}{}
    \begin{tabular}{ccc}
      \toprule
      $x_1$ & $x_2$ & $y = x_1 \land x_2$ \\
      \midrule
      0 & 0 & 0 \\
      0 & 1 & 0 \\
      1 & 0 & 0 \\
      1 & 1 & 1 \\
      \bottomrule
    \end{tabular}
  \end{minipage}
  \begin{minipage}{0.45\textwidth}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{ccc}
    \toprule
    $x_1$ & $x_2$ & $y = x_1 \oplus x_2$ \\
    \midrule
    0 & 0 & 0 \\
    0 & 1 & 1 \\
    1 & 0 & 1 \\
    1 & 1 & 0 \\
    \bottomrule
  \end{tabular}
  \end{minipage}
  \tcblower
  The AND and XOR datasets are binary classification datasets where the output $y$ is the
  ``logical AND'' and the ``exclusive OR'' of the inputs $x_1$ and $x_2$, i.e.
  $y = x_1 \land x_2$ and $y = x_1 \oplus x_2$.
\end{tablebox}

\subsection{Perceptron learning bias}

The perceptron is a linear classifier that generates a hyperplane that separates the
classes.  The model for our example is
\begin{equation*}
  f(x_1, x_2; \theta = \vec{w} = \left[w_0, w_1, w_2\right]) =  \begin{cases}
    1 & \text{if } w_0 + w_1 x_1 + w_2 x_2 > 0 \\
    0 & \text{otherwise.}
  \end{cases}
\end{equation*}
As one can see, the perceptron learning bias is the assumption that the classes are
linearly separable. The equation $\vec{w} \cdot \vec{x} = 0$, where $\vec{x} = [1, x_1,
x_2]$, is the equation of a hyperplane.

\begin{figurebox}[label=fig:perceptron]{Perceptron learning bias.}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        axis x line=bottom,
        axis y line=left,
        xlabel={$x_1$},
        ylabel={$x_2$},
        width=0.6\textwidth,
        height=0.6\textwidth,
        xtick={0, 1},
        ytick={0, 1},
        grid=both,
        xmin=-0.5, xmax=1.5,
        ymin=-0.5, ymax=1.5,
      ]
      \addplot+[only marks, mark=+, mark size=3pt] coordinates {
        (0, 1) (1, 0)
      };
      \addplot+[only marks, mark=*, mark size=3pt] coordinates {
        (0, 0) (1, 1)
      };
      \addplot+[domain=0:1.5, mark=none, black] {-0.5 + x};
    \end{axis}
  \end{tikzpicture}
  \tcblower
  The perceptron learning bias is the assumption that the classes are linearly separable.
  The hyperplane that separates the classes is the learning machine model.
  In this case, $w_0 = -0.5$, $w_1 = 1$, and $w_2 = -1$.
\end{figurebox}

In \cref{fig:perceptron}, we show the hyperplane that the model $\vec{w} = [-0.5, 1, -1]$
generates for the XOR dataset.  As one can see, the classes are not linearly separable,
and the perceptron model fails to classify the dataset correctly, see \cref{tab:xor-perceptron}.

\begin{tablebox}[label=tab:xor-perceptron]{Truth table for the predictions of the perceptron.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{ccc|cc}
    \toprule
    $x_1$ & $x_2$ & $y$ & $-0.5 + x_1 - x_2$ & $\hat{y}$ \\
    \midrule
    0 & 0 & 0 & -0.5 & 0 \\
    0 & 1 & 1 & -1.5 & 0 \\
    1 & 0 & 1 & 0.5 & 1 \\
    1 & 1 & 0 & -0.5 & 0 \\
    \bottomrule
  \end{tabular}
  \tcblower
  The perceptron model with parameters $w_0 = -0.5$, $w_1 = 1$, and $w_2 = -1$
  fails to classify the XOR dataset correctly --- as any other perceptron would do.
\end{tablebox}

\begin{hlbox}{Think about\dots}
  What happens if we remove $w_0$ from the model?
\end{hlbox}

\subsection{Multi-layer perceptron learning bias}

The multi-layer perceptron (MLP) is a non-linear classifier that generates a set of hyperplanes
that separates the classes.  In order to simplify the understanding, consider the
that the activation function of the hidden layer is the discrete step function
\begin{equation*}
  \sigma(x) = \begin{cases}
    1 & \text{if } x > 0 \\
    0 & \text{otherwise.}
  \end{cases}
\end{equation*}
A model with two neurons in the hidden layer (effectively the combination of three
perceptrons) is
\begin{multline*}
  f(x_1, x_2; \theta = \left\{ \vec{w}^{(1)}, \vec{w}^{(2)}, \vec{w}^{(3)} \right\}) = \\
  \sigma\left(
    \vec{w}^{(3)} \cdot \left[1, \sigma(\vec{w}^{(1)} \cdot \vec{x}), \sigma(\vec{w}^{(2)} \cdot \vec{x})\right]
  \right)\text{.}
\end{multline*}

The parameters $\vec{w}^{(1)}$ and $\vec{w}^{(2)}$ represent the hyperplanes that separate
the classes in the hidden layer, and $\vec{w}^{(3)}$ represents how the hyperplanes are
combined to generate the output.  If we set $\vec{w}^{(1)} = [-0.5, 1, -1]$ (like the
perceptron in the previous example) and $\vec{w}^{(2)} = [-0.5, -1, 1]$, we use the third neuron
to combine the results of the first two neurons.  This way, solution for the XOR problem is
setting $\vec{w}^{(3)} = [0, 1, 1]$.

\begin{figurebox}[label=fig:mlp]{MLP learning bias.}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        axis x line=bottom,
        axis y line=left,
        xlabel={$x_1$},
        ylabel={$x_2$},
        width=0.6\textwidth,
        height=0.6\textwidth,
        xtick={0, 1},
        ytick={0, 1},
        grid=both,
        xmin=-0.5, xmax=1.5,
        ymin=-0.5, ymax=1.5,
      ]
      \addplot+[only marks, mark=+, mark size=3pt] coordinates {
        (0, 1) (1, 0)
      };
      \addplot+[only marks, mark=*, mark size=3pt] coordinates {
        (0, 0) (1, 1)
      };
      \addplot+[domain=0:1.5, mark=none, black] {-0.5 + x};
      \addplot+[domain=-0.5:1.5, mark=none, black] {0.5 + x};
    \end{axis}
  \end{tikzpicture}
  \tcblower
  \dots
\end{figurebox}

\begin{tablebox}[label=tab:xor-mlp]{Truth table for the predictions of the MLP.}
  \centering
  \rowcolors{2}{black!10!white}{}
  \begin{tabular}{ccc|cccc}
    \toprule
    $x_1$ & $x_2$ & $y$ & \nth{1} neuron & \nth{2} neuron & $\hat{y}$ \\
    \midrule
    0 & 0 & 0 & 0 & 0 & 0 \\
    0 & 1 & 1 & 0 & 1 & 1 \\
    1 & 0 & 1 & 1 & 0 & 1 \\
    1 & 1 & 0 & 0 & 0 & 0 \\
    \bottomrule
  \end{tabular}
  \tcblower
  \dots
\end{tablebox}

\begin{hlbox}{Think about\dots}
  Note that there are many possible solutions for the XOR problem using the MLP.
\end{hlbox}

\subsection{Decision tree learning bias}

The decision tree is a non-linear classifier that generates a set of hyperplanes that
are orthogonal to the axes.  Consider the decision tree in \cref{fig:tree-and}.

\begin{figurebox}[label=fig:tree-and]{Decision tree representation.}
  \centering
  \begin{tikzpicture}
    \node[decision] (x1) at (0, 0) {$x_1$};
    \node[block] (n1) at (-2, -1.5) {$\hat{y} = 0$};
    \node[decision] (x2) at (2, -1.5) {$x_2$};
    \node[block] (n2) at (0, -3) {$\hat{y} = 0$};
    \node[block] (p) at (4, -3) {$\hat{y} = 1$};

    \draw (x1) -| (n1) node [midway, above] {$\leq 0.5$};
    \draw (x1) -| (x2) node [midway, above] {$>0.5$};
    \draw (x2) -| (n2) node [midway, above] {$\leq 0.5$};
    \draw (x2) -| (p) node [midway, above] {$>0.5$};
  \end{tikzpicture}
  \tcblower
  The decision tree that separates the AND dataset.
\end{figurebox}

\begin{figurebox}[label=fig:tree-bias]{Decision tree learning bias.}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        axis x line=bottom,
        axis y line=left,
        xlabel={$x_1$},
        ylabel={$x_2$},
        width=0.6\textwidth,
        height=0.6\textwidth,
        xtick={0, 1},
        ytick={0, 1},
        grid=both,
        xmin=-0.5, xmax=1.5,
        ymin=-0.5, ymax=1.5,
      ]
      \addplot+[only marks, mark=+, mark size=3pt] coordinates {
        (1, 1)
      };
      \addplot+[only marks, mark=*, mark size=3pt] coordinates {
        (0, 0) (0, 1) (1, 0)
      };
      \addplot+[domain=0.5:1.5, mark=none, black, thick] {0.5};
      \addplot+[mark=none, black, thick] coordinates {(0.5, -0.5) (0.5, 1.5)};
    \end{axis}
  \end{tikzpicture}
  \tcblower
  The decision tree learning bias is the assumption that the classes can be separated with
  hyperplanes orthogonal to the axes.
\end{figurebox}

\begin{hlbox}{Think about\dots}
  Decision tree are nonparametric models, one can easily increase the depth of the tree to
  fit the data.
\end{hlbox}

\subsection{$k$-nearest neighbors learning bias}

The $k$-nearest neighbors ($k$-NN) is a non-linear nonparametric classifier that
generate arbitrarily complex decision boundaries by ``memoring'' the training data.
The behavior of the boundaries depends on the value of $k$ and the distance metric one
uses to find the nearest neighbors of a point.

\begin{figurebox}[label=fig:1nn-bias]{1-NN learning bias.}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        axis x line=bottom,
        axis y line=left,
        xlabel={$x_1$},
        ylabel={$x_2$},
        width=0.6\textwidth,
        height=0.6\textwidth,
        xtick={0, 1},
        ytick={0, 1},
        grid=both,
        xmin=-0.5, xmax=1.5,
        ymin=-0.5, ymax=1.5,
      ]
      \addplot+[only marks, mark=+, mark size=3pt] coordinates {
        (1, 1)
      };
      \addplot+[only marks, mark=*, mark size=3pt] coordinates {
        (0, 0) (0, 1) (1, 0)
      };
      \addplot+[domain=0.5:1.5, mark=none, black, thick] {0.5};
      \addplot+[mark=none, black, thick] coordinates {(0.5, 0.5) (0.5, 1.5)};
    \end{axis}
  \end{tikzpicture}
  \tcblower
  In this particular case, the 1-NN boundaries match the decision tree boundaries.
\end{figurebox}

As $k$ increases the boundaries become smoother:
\href{https://images.squarespace-cdn.com/content/v1/5d782753c70af105c29a9b14/1580261947016-XODPUVKWPGGMJJMAXSNF/Screen+Shot+2020-01-28+at+8.38.55+PM.png}{example}.

See illustration: \href{https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html}{here}.

% vim: spell spelllang=en
